[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How To Prove It With Lean",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\bigtriangleup}\n$$"
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "How To Prove It With Lean",
    "section": "About This Book",
    "text": "About This Book\nThis book is intended to accompany my book How To Prove It (henceforth called HTPI), which is published by Cambridge University Press. Although this book is self-contained, we will sometimes have occasion to refer to passages in HTPI, so this book will be easiest to understand if you have a copy of HTPI available to you.\nHTPI explains a systematic approach to constructing mathematical proofs. The purpose of this book is to show you how to use a computer software package called Lean to help you master the techniques presented in HTPI. Lean is free software that is available for Windows, MacOS, and Unix computers. To get the most out of this book, you will need to download and install Lean on your computer. We will explain how to do that below.\nThe chapters and sections of this book are numbered to match the sections of HTPI to which they correspond. The first two chapters of HTPI cover preliminary topics in elementary logic and set theory that are needed to understand the proof techniques presented in later chapters. We assume that you are already familiar with that material (if not, go read those chapters in HTPI!), so Chapters 1 and 2 of this book will just briefly summarize the most important points. Those chapters are followed by an introduction to Lean that explains the basics of using Lean to write proofs. The presentation of proof techniques in HTPI begins in earnest in Chapter 3, so that is where we will begin to discuss how Lean can be used to master those techniques.\nIf you are reading this book online, then you will find a search box in the left margin. You can use that box to search for any word or phrase anywhere in the book. Below the search box is a list of the chapters of the book. Click on any chapter to go to that chapter. Within each chapter, a table of contents in the right margin lists the sections in that chapter. Again, you can go to any section by clicking on it. At the end of each chapter there are links to take you to the next or previous chapter."
  },
  {
    "objectID": "index.html#about-lean",
    "href": "index.html#about-lean",
    "title": "How To Prove It With Lean",
    "section": "About Lean",
    "text": "About Lean\nLean is a kind of software package called a proof assistant. What that means is that Lean can help you to write proofs. As we will see over the course of this book, there are several ways in which Lean can be helpful. First of all, if you type a proof into Lean, then Lean can check the correctness of the proof and point out errors. As you are typing a proof into Lean, it will keep track of what has been accomplished so far in the proof and what remains to be done to finish the proof, and it will display that information for you. That can keep you moving in the right direction as you are figuring out a proof. And sometimes Lean can fill in small details of the proof for you.\nOf course, to make this possible you must type your proof in a format that Lean understands. Much of this book will be taken up with explaining how to write a proof so that Lean will understand it."
  },
  {
    "objectID": "index.html#installing-lean",
    "href": "index.html#installing-lean",
    "title": "How To Prove It With Lean",
    "section": "Installing Lean",
    "text": "Installing Lean\nWe will be using Visual Studio Code to run Lean, so you will need to install VS Code first. VS Code is free and can be downloaded here.\nYou will also need the Lean package that accompanies this book, which can be downloaded from https://github.com/djvelleman/HTPILeanPackage. After following the link, click on the green “Code” button and, in the pop-up menu, select “Download ZIP”. Open the downloaded zip file to create a folder containing the HTPI Lean package. You can put this folder wherever you want on your computer.\nNow open VS Code. You should see a window that looks something like this:\n\nClick on the Extensions icon on the left side of the window, which is circled in red in the image above. That will bring up a list of available extensions:\n\nIn the Search Extensions in Marketplace field, type “lean4”. VS Code should find the Lean 4 extension and display it:\n\nClick on “Install” to install the Lean 4 extension.\nNext, in VS Code, select “Open Folder …” from the File menu and open the folder containing the HTPI Lean package that you downloaded earlier. Under the heading “Explorer” on the left side of the window, you should see a list of the files in the package. (If you don’t see the list, try clicking on the Explorer icon, circled in red below.)\n\nClick on the file “Blank.lean” in the file list. You should see a warning that VS Code failed to start the ‘lean’ language server:\n\n\n\n\n\nClick on the “Install Lean using Elan” button, and the Lean server should be installed. This may take a while, and there may be messages asking you to do things. If anything goes wrong, try quiting VS Code and restarting. Eventually your window should look like this:\n\nIf you don’t see the Infoview pane on the right side of the window, click on the icon circled in red in the image above, and the Infoview pane should appear.\nYour installation is now complete."
  },
  {
    "objectID": "Chap1.html",
    "href": "Chap1.html",
    "title": "1  Sentential Logic",
    "section": "",
    "text": "Chapter 1 of How To Prove It introduces the following symbols of logic:\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(\\neg\\)\nnot\n\n\n\\(\\wedge\\)\nand\n\n\n\\(\\vee\\)\nor\n\n\n\\(\\to\\)\nif … then\n\n\n\\(\\leftrightarrow\\)\niff (that is, if and only if)\n\n\n\n\nAs we will see, Lean uses the same symbols, with the same meanings. A statement of the form \\(P \\wedge Q\\) is called a conjunction, a statement of the form \\(P \\vee Q\\) is called a disjunction, a statement of the form \\(P \\to Q\\) is an implication or a conditional statement (with antecedent \\(P\\) and consequent \\(Q\\)), and a statement of the form \\(P \\leftrightarrow Q\\) is a biconditional statement. The statement \\(\\neg P\\) is the negation of \\(P\\).\nThis chapter also establishes a number of logical equivalences that will be useful to us later:\n\n\n\n\n\n\n\n\n\n\nName\n\nEquivalence\n\n\n\n\n\nDe Morgan’s Laws\n\\(\\neg (P \\wedge Q)\\)\nis equivalent to\n\\(\\neg P \\vee \\neg Q\\)\n\n\n\n\\(\\neg (P \\vee Q)\\)\nis equivalent to\n\\(\\neg P \\wedge \\neg Q\\)\n\n\nDouble Negation Law\n\\(\\neg\\neg P\\)\nis equivalent to\n\\(P\\)\n\n\nConditional Laws\n\\(P \\to Q\\)\nis equivalent to\n\\(\\neg P \\vee Q\\)\n\n\n\n\\(P \\to Q\\)\nis equivalent to\n\\(\\neg(P \\wedge \\neg Q)\\)\n\n\nContrapositive Law\n\\(P \\to Q\\)\nis equivalent to\n\\(\\neg Q \\to \\neg P\\)\n\n\n\n\nFinally, Chapter 1 of HTPI introduces some concepts from set theory. A set is a collection of objects; the objects in the collection are called elements of the set. If \\(P(x)\\) is a statement about \\(x\\), then \\(\\{x \\mid P(x)\\}\\) denotes the set whose elements are the objects \\(x\\) for which \\(P(x)\\) is true. The notation \\(x \\in A\\) means that \\(x\\) is an element of \\(A\\). Two sets \\(A\\) and \\(B\\) are equal if they have exactly the same elements. We say that \\(A\\) is a subset of \\(B\\), denoted \\(A \\subseteq B\\), if every element of \\(A\\) is an element of \\(B\\). And we have the following operations on sets:\n\n\\(A \\cap B = \\{x \\mid x \\in A \\wedge x \\in B\\} = {}\\) the intersection of \\(A\\) and \\(B\\),\n\\(A \\cup B = \\{x \\mid x \\in A \\vee x \\in B\\} = {}\\) the union of \\(A\\) and \\(B\\),\n\\(A \\setmin B = \\{x \\mid x \\in A \\wedge x \\notin B\\} = {}\\) the difference of \\(A\\) and \\(B\\),\n\\(A \\symmdiff B = (A \\setmin B) \\cup (B \\setmin A) = {}\\) the symmetric difference of \\(A\\) and \\(B\\)."
  },
  {
    "objectID": "Chap2.html",
    "href": "Chap2.html",
    "title": "2  Quantificational Logic",
    "section": "",
    "text": "Chapter 2 of How To Prove It introduces two more symbols of logic, the quantifiers \\(\\forall\\) and \\(\\exists\\). If \\(P(x)\\) is a statement about an object \\(x\\), then\n\n\\(\\forall x\\,P(x)\\) means “for all \\(x\\), \\(P(x)\\),”\n\nand\n\n\\(\\exists x\\,P(x)\\) means “there exists some \\(x\\) such that \\(P(x)\\).”\n\nLean also uses these symbols, although we will see that quantified statements are written slightly differently in Lean from the way they are written in HTPI. In the statement \\(P(x)\\), the variable \\(x\\) is called a free variable. But in \\(\\forall x\\,P(x)\\) or \\(\\exists x\\,P(x)\\), it is a bound variable; we say that the quantifiers \\(\\forall\\) and \\(\\exists\\) bind the variable.\nOnce again, there are logical equivalences involving these symbols that will be useful to us later:\n\n\n\n\n\nQuantifier Negation Laws\n\n\n\n\n\n\\(\\neg \\exists x\\,P(x)\\)\nis equivalent to\n\\(\\forall x\\,\\neg P(x)\\)\n\n\n\\(\\neg \\forall x\\,P(x)\\)\nis equivalent to\n\\(\\exists x\\,\\neg P(x)\\)\n\n\n\n\nChapter 2 of HTPI also introduces some more advanced set theory operations. For any set \\(A\\),\n\n\\(\\mathscr{P}(A) = \\{X \\mid X \\subseteq A\\} = {}\\) the power set of \\(A\\).\n\nAlso, if \\(\\mathcal{F}\\) is a family of sets—that is, a set whose elements are sets—then\n\n\\(\\bigcap \\mathcal{F} = \\{x \\mid \\forall A(A \\in \\mathcal{F} \\to x \\in A)\\} = {}\\) the intersection of the family \\(\\mathcal{F}\\),\n\\(\\bigcup \\mathcal{F} = \\{x \\mid \\exists A(A \\in \\mathcal{F} \\wedge x \\in A)\\} = {}\\) the union of the family \\(\\mathcal{F}\\).\n\nFinally, Chapter 2 introduces the notation \\(\\exists ! x\\,P(x)\\) to mean “there is exactly one \\(x\\) such that \\(P(x).\\)” This can be thought of as an abbreviation for \\(\\exists x(P(x) \\wedge \\neg\\exists y(P(y) \\wedge y \\ne x))\\). By the quantifier negation, De Morgan, and conditional laws, this is equivalent to \\(\\exists x(P(x) \\wedge \\forall y(P(y) \\to y = x))\\)."
  },
  {
    "objectID": "IntroLean.html",
    "href": "IntroLean.html",
    "title": "Introduction to Lean",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\bigtriangleup}\n$$\nIf you are reading this book in conjunction with How To Prove It, you should complete Section 3.2 of HTPI before reading this chapter. Once you have reached that point in HTPI, you are ready to start learning about Lean. In this chapter we’ll explain the basics of writing proofs in Lean and getting feedback from Lean."
  },
  {
    "objectID": "IntroLean.html#a-first-example",
    "href": "IntroLean.html#a-first-example",
    "title": "Introduction to Lean",
    "section": "A First Example",
    "text": "A First Example\nWe’ll start with Example 3.2.4 in How To Prove It. Here is how the theorem and proof in that example appear in HTPI (consult HTPI if you want to see how this proof was constructed):\n\nSuppose \\(P \\to (Q \\to R)\\). Then \\(\\neg R \\to (P \\to \\neg Q)\\).\n\n\nProof. Suppose \\(\\neg R\\). Suppose \\(P\\). Since \\(P\\) and \\(P \\to (Q \\to R)\\), it follows that \\(Q \\to R\\). But then, since \\(\\neg R\\), we can conclude \\(\\neg Q\\). Thus, \\(P \\to \\neg Q\\). Therefore \\(\\neg R \\to (P \\to \\neg Q)\\).  □\n\nAnd here is how we would write the proof in Lean. (If you are reading this book online, then Lean examples like the one below will appear in gray boxes. You can copy the example to your clipboard by clicking in the upper-right corner of the box, and then you can paste it into a file in VS Code to try it out.)\ntheorem Example_3_2_4\n    (P Q R : Prop) (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  have h4 : Q → R := h h3\n  contrapos at h4            --Now h4 : ¬R → ¬Q\n  show ¬Q from h4 h2\nLet’s go through this Lean proof line-by-line and see what it means. The first line tells Lean that we are going to prove a theorem, and it gives the theorem a name, Example_3_2_4. The next line states the theorem. In the theorem as stated in HTPI, the letters \\(P\\), \\(Q\\), and \\(R\\) are used to stand for statements that are either true or false. In logic, such statements are often called propositions. The expression (P Q R : Prop) on the second line tells Lean that P, Q, and R will be used in this theorem to stand for propositions. The next parenthetical expression, (h : P → (Q → R)), states the hypothesis of the theorem and gives it the name h; the technical term that Lean uses is that h is an identifier for the hypothesis. Assigning an identifier to the hypothesis gives us a way to refer to it when it is used later in the proof. Almost any string of characters that doesn’t begin with a digit can be used as an identifier, but it is traditional to use identifiers beginning with the letter h for hypotheses. After the statement of the hypothesis there is a colon followed by the conclusion of the theorem, ¬R → (P → ¬Q). Finally, at the end of the second line, the expression := by signals the beginning of the proof.\nEach of the remaining lines is a step in the proof. The first line of the proof introduces the assumption ¬R and gives it the identifier h2. Of course, this corresponds precisely to the first sentence of the proof in HTPI. Similarly, the second line, corresponding to the second sentence of the HTPI proof, assigns the identifier h3 to the assumption P. The next line makes the inference Q → R, giving it the identifier h4. The inference is justified by combining statements h and h3—that is, the statements P → (Q → R) and P—exactly as in the third sentence of the proof in HTPI.\nThe next step of the proof in HTPI combines the statements \\(Q \\to R\\) and \\(\\neg R\\) to draw the inference \\(\\neg Q\\). This reasoning is justified by the contrapositive law, which says that \\(Q \\to R\\) is equivalent to its contrapositive, \\(\\neg R \\to \\neg Q\\). In the Lean proof, this inference is broken up into two steps. In the fourth line of the proof, we ask Lean to rewrite statement h4—that is, Q → R—using the contrapositive law. Two hyphens in a row tell Lean that the rest of the line is a comment. Lean ignores comments and displays them in green. The comment on line four serves as a reminder that h4 now stands for the statement ¬R → ¬Q. Finally, in the last line of the proof, we combine the new h4 with h2 to infer ¬Q. There is no need to give this statement an identifier, because it completes the proof. In the proof in HTPI, there are a couple of final sentences explaining why this completes the proof, but Lean doesn’t require this explanation."
  },
  {
    "objectID": "IntroLean.html#term-mode",
    "href": "IntroLean.html#term-mode",
    "title": "Introduction to Lean",
    "section": "Term Mode",
    "text": "Term Mode\nNow that you have seen an example of a proof in Lean, it is time for you to write your first proof. Lean has two modes for writing proofs, called term mode and tactic mode. The example above was written in tactic mode, and that is the mode we will use for most proofs in this book. But before we study the construction of proofs in tactic mode, it will be helpful to learn a bit about term mode. Term mode is best for simple proofs, so we begin with a few very short proofs.\nIf you have not yet installed Lean on your computer, go back and follow the instructions for installing it now. Then in VS Code, open the folder containing the HTPI Lean Package that you downloaded, and click on the file Blank.lean. The file starts with the line import HTPIDefs. Click on the blank line at the end of the file; this is where you will be typing your first proofs.\nNow type in the following theorem and proof:\ntheorem extremely_easy (P : Prop) (h : P) : P := h\nThis theorem and proof are so short we have put everything on one line. In this theorem, the letter P is used to stand for a proposition. The theorem has one hypothesis, P, which has been given the identifier h, and the conclusion of the theorem is also P. The notation := indicates that what follows will be a proof in term mode.\nOf course, the proof of the theorem is extremely easy: to prove P, we just have to point out that it is given as the hypothesis h. And so the proof in Lean consists of just one letter: h.\nEven though this example is a triviality, there are some things to be learned from it. First of all, although we have been describing the letter h as an identifier for the hypothesis P, this example illustrates that Lean also considers h to be a proof of P. In general, when we see h : P in a Lean proof, where P is a proposition, we can think of it as meaning, not just that h is an identifier for the statement P, but also that h is a proof of P.\nWe can learn something else from this example by changing it slightly. If you change the final h to a different letter—say, f—you will see that Lean puts a red squiggly line under the f, like this:\ntheorem extremely_easy (P : Prop) (h : P) : P := **f::\nThis indicates that Lean has detected an error in the proof. Lean always indicates errors by putting a red squiggle under the offending text. Lean also puts a message in the Lean Infoview pane explaining what the error is. (If you don’t see the Infoview pane, choose “Command Palette …” in the “View” menu, and then type “Lean” in the text box that appears. You will see a list of commands that start with “Lean”. Click on “Lean 4: Infoview: Toggle” to make the Infoview pane appear.) In this case, the message is unknown identifier 'f'. The message is introduced by a heading, in red, that identifies the file, the line number, and the character position on that line where the error appears. If you change f back to h, the red squiggle and error message go away.\nLet’s try a slightly less trivial example. You can type the next theorem below the previous one, leaving a blank line between them to keep them visually separate. To type the → symbol in the next example, type \\to and then hit either the space bar or the tab key; when you type either space or tab, the \\to will change to →. Alternatively, you can type \\r (short for “right arrow”) or \\imp (short for “implies”), again followed by either space or tab. Or, you can type ->, and Lean will interpret it as →.\ntheorem very_easy\n    (P Q : Prop) (h1 : P → Q) (h2 : P) : Q := h1 h2\nIndenting the second line is not necessary, but it is traditional. When stating a theorem, we will generally indent all lines after the first with two tabs in VS Code. Once you indent a line, VS Code will maintain that same indenting in subsequent lines until you delete tabs at the beginning of a line to reduce or eliminate indenting.\nThis time there are two hypotheses, h1 : P → Q and h2 : P. As explained in Section 3.2 of HTPI, the conclusion Q follows from these hypotheses by the logical rule modus ponens. To use modus ponens to complete this proof in term mode, we simply write the identifiers of the two hypotheses—which, as we have just seen, can also be thought of as proofs of the two hypotheses—one after the other, with a space between them. It is important to write the proof of the conditional hypothesis first, so the proof is written h1 h2; if you try writing this proof as h2 h1, you will get a red squiggle. In general, if a is a proof of any conditional statement X → Y, and b is a proof of the antecedent X, then a b is a proof of the consequent Y. The proofs a and b need not be simply identifiers; any proofs of a conditional statement and its antecedent can be combined in this way.\nWe’ll try one more proof in term mode:\ntheorem easy (P Q R : Prop) (h1 : P → Q)\n    (h2 : Q → R) (h3 : P) : R :=\nNote that in the statement of the theorem, you can break the lines however you please; this time we have put the declaration of P, Q, and R and the first hypothesis on the first line and the other two hypotheses on the second line. How can we prove the conclusion R? Well, we have h2 : Q → R, so if we could prove Q then we could use modus ponens to reach the desired conclusion. In other words, h2 _ will be a proof of R, if we can fill in the blank with a proof of Q. Can we prove Q? Yes, Q follows from P → Q and P by modus ponens, so h1 h3 is a proof of Q. Filling in the blank, we conclude that h2 (h1 h3) is a proof of R. Type it in, and you’ll see that Lean will accept it. Note that the parentheses are important; if you write h2 h1 h3 then Lean will interpret it as (h2 h1) h3, which doesn’t make sense, and you’ll get an error."
  },
  {
    "objectID": "IntroLean.html#tactic-mode",
    "href": "IntroLean.html#tactic-mode",
    "title": "Introduction to Lean",
    "section": "Tactic Mode",
    "text": "Tactic Mode\nFor more complicated proofs, it is easier to use tactic mode. Type the following theorem into Lean; to type the symbol ¬, type \\not, followed again by either space or tab. Alternatively, if you type Not P, Lean will interpret it as meaning ¬P.\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P :=\nLean is now waiting for you to type a proof in term mode. To switch to tactic mode, type by after :=. Although it is not necessary, we find it helpful to set off a tactic proof from the surrounding text by indenting it with one tab, and also by marking where the proof ends. To do this, leave a blank line after the statement of the theorem, adjust the indenting to one tab, and type done. You will type your proof between the statement of the theorem and the line containing done, so click on the blank line between them to position the cursor there.\nOne of the advantages of tactic mode is that Lean displays, in the Lean Infoview pane, information about the status of the proof as your write it. As soon as you position your cursor on the blank line, Lean displays what it calls the “tactic state” in the Infoview pane. Your screen should look like this:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\n⊢ R → ¬P\n\n\nThe red squiggle under done indicates that Lean knows that the proof isn’t done. The tactic state in the Infoview pane is very similar to the lists of givens and goals that are used in HTPI. The hypotheses h1 : P → Q and h2 : Q → ¬R are examples of what are called givens in HTPI. The tactic state above says that P, Q, and R stand for propositions, and then it lists the two givens h1 and h2. The symbol ⊢ in the last line labels the goal, R → ¬P. The tactic state is a valuable tool for guiding you as you are figuring out a proof; whenever you are trying to decide on the next step of a proof, you should look at the tactic state to see what givens you have to work with and what goal you need to prove.\nFrom the givens h1 and h2 it shouldn’t be hard to prove P → ¬R, but the goal is R → ¬P. This suggests that we should prove the contrapositive of the goal. Type contrapos (indented by one tab) to tell Lean that you want to replace the goal with its contrapositive. As soon as you type contrapos, Lean will update the tactic state to reflect the change in the goal. You should now see this:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\n⊢ P → ¬R\n\n\nIf you want to make your proof a little more readable, you could add a comment saying that the goal has been changed to P → ¬R. To prove the new goal, we will assume P and prove ¬R. So type assume h3 : P on a new line (after contrapos, but before done). Once again, the tactic state is immediately updated. Lean adds h3 : P as a new given, and it knows, without having to be told, that the goal should now be ¬R:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos           --Goal is now P → ¬R\n  assume h3 : P\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\nh3 : P\n⊢ ¬R\n\n\nWe can now use modus ponens to infer Q from h1 : P → Q and h3 : P. As we saw earlier, this means that h1 h3 is a term-mode proof of Q. So on the next line, type have h4 : Q := h1 h3. To make an inference, you need to provide a justification, so := here is followed by the term-mode proof of Q. Usually we will use have to make easy inferences for which we can give simple term-mode proofs. (We’ll see later that it is also possible to use have to make an inference justified by a tactic-mode proof.) Of course, Lean updates the tactic state by adding the new given h4 : Q:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos           --Goal is now P → ¬R\n  assume h3 : P\n  have h4 : Q := h1 h3\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\nh3 : P\nh4 : Q\n⊢ ¬R\n\n\nFinally, to complete the proof, we can infer the goal ¬R from h2 : Q → ¬R and h4 : Q, using the term-mode proof h2 h4. Type show ¬R from h2 h4 to complete the proof. You’ll notice two changes in the display: the red squiggle will disappear from the word done, and the tactic state will say “Goals accomplished”:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos           --Goal is now P → ¬R\n  assume h3 : P\n  have h4 : Q := h1 h3\n  show ¬R from h2 h4\n  done\n\n\n>>Goals accomplished 🎉\n\n\nCongratulations! You’ve written your first proof in tactic mode. If you move your cursor around in the proof, you will see that Lean always displays in the Infoview the tactic state at the point in the proof where the cursor is located. Try clicking on different lines of the proof to see how the tactic state changes over the course of the proof. If you want to try another example, you could try typing in the first example in this chapter. You will learn the most from this book if you continue to type the examples into Lean and see for yourself how the tactic state gets updated as the proof is written.\nWe have now seen four tactics: contrapos, assume, have, and show. If the goal is a conditional statement, the contrapos tactic replaces it with its contrapositive. If h is a given that is a conditional statement, then contrapos at h will replace h with its contrapositive. If the goal is a conditional statement P → Q, you can use the assume tactic to assume the antecedent P, and Lean will set the goal to be the consequent Q. You can use the have tactic to make an inference from your givens, as long as you can justify the inference with a proof. The show tactic is similar, but it is used to infer the goal, thus completing the proof. And we have learned how to use one rule of inference in term mode: modus ponens. In the rest of this book we will learn about other tactics and other term-mode rules.\nBefore continuing, it might be useful to summarize how you type statements into Lean. We have already told you how to type the symbols → and ¬, but you will want to know how to type all of the logical connectives. In each case, the command to produce the symbol must be followed by space or tab, but there is also a plain text alternative:\n\n\n\n\nSymbol\nHow To Type It\nPlain Text Alternative\n\n\n\n\n¬\n\\not or \\n\nNot\n\n\n∧\n\\and\n/\\\n\n\n∨\n\\or or \\v\n\\/\n\n\n→\n\\to or \\r or \\imp\n->\n\n\n↔︎\n\\iff or \\lr\n<->\n\n\n\n\nLean has conventions that it follows to interpret a logical statement when there are not enough parentheses to indicate how terms are grouped in the statement. For our purposes, the most important of these conventions is that P → Q → R is interpreted as P → (Q → R), not (P → Q) → R. The reason for this is simply that statements of the form P → (Q → R) come up much more often in proofs than statements of the form (P → Q) → R. (Lean also follows this “grouping-to-the-right” convention for ∧ and ∨, although this makes less of a difference, since these connectives are associative.) Of course, when in doubt about how to type a statement, you can always put in extra parentheses to avoid confusion.\nWe will be using tactics to apply several logical equivalences. Here are tactics corresponding to all of the logical laws listed in Chapter 1, as well as one additional law:\n\n\n\n\n\n\n\n\n\n\n\nLogical Law\nTactic\n\nTransformation\n\n\n\n\n\nContrapositive Law\ncontrapos\nP → Q\nis changed to\n¬Q → ¬P\n\n\nDe Morgan’s Laws\ndemorgan\n¬(P ∧ Q)\nis changed to\n¬P ∨ ¬Q\n\n\n\n\n¬(P ∨ Q)\nis changed to\n¬P ∧ ¬Q\n\n\n\n\nP ∧ Q\nis changed to\n¬(¬P ∨ ¬Q)\n\n\n\n\nP ∨ Q\nis changed to\n¬(¬P ∧ ¬Q)\n\n\nConditional Laws\nconditional\nP → Q\nis changed to\n¬P ∨ Q\n\n\n\n\n¬(P → Q)\nis changed to\nP ∧ ¬Q\n\n\n\n\nP ∨ Q\nis changed to\n¬P → Q\n\n\n\n\nP ∧ Q\nis changed to\n¬(P → ¬Q)\n\n\nDouble Negation Law\ndouble_neg\n¬¬P\nis changed to\nP\n\n\nBiconditional Negation Law\nbicond_neg\n¬(P ↔︎ Q)\nis changed to\n¬P ↔︎ Q\n\n\n\n\nP ↔︎ Q\nis changed to\n¬(¬P ↔︎ Q)\n\n\n\n\nAll of these tactics work the same way as the contrapos tactic: by default, the transformation is applied to the goal; to apply it to a given h, add at h after the tactic name."
  },
  {
    "objectID": "IntroLean.html#types",
    "href": "IntroLean.html#types",
    "title": "Introduction to Lean",
    "section": "Types",
    "text": "Types\nAll of our examples so far have just used letters to stand for propositions. To prove theorems with mathematical content, we will need to introduce one more idea.\nThe underlying theory on which Lean is based is called type theory. We won’t go very deeply into type theory, but we will need to make use of the central idea of the theory: every variable in Lean must have a type. What this means is that, when you introduce a variable to stand for a mathematical object in a theorem or proof, you must specify what type of object the variable stands for. We have already seen this idea in action: in our first example, the expression (P Q R : Prop) told Lean that the variables P, Q, and R have type Prop, which means they stand for propositions. There are types for many kinds of mathematical objects. For example, Nat is the type of natural numbers, and Real is the type of real numbers. So if you want to state a theorem about real numbers x and y, the statement of your theorem might start with (x y : Real). You must include such a type declaration before you can use the variables x and y as free variables in the hypotheses or conclusion of your theorem.\nWhat about sets? If you want to prove a theorem about a set A, can you say that A has type Set? No, Lean is fussier than that. Lean wants to know, not only that A is a set, but also what the type of the elements of A is. So you can say that A has type Set Nat if A is a set whose elements are natural numbers, or Set Real if it is a set of real numbers, or even Set (Set Nat) if it is a set whose elements are sets of natural numbers. Here is an example of a simple theorem about sets; it is a simplified version of Example 3.2.5 in HTPI. To type the symbols ∈, ∉, and \\ in this theorem, type \\in, \\notin, and \\\\, respectively.\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n\n  **done::\n\n\nB C : Set ℕ\na : ℕ\nh1 : a ∈ B\nh2 : ¬a ∈ B \\ C\n⊢ a ∈ C\n\n\nThe second line of this theorem statement declares that the variables B and C stand for sets of natural numbers, and a stands for a natural number. The third line states the two hypotheses of the theorem, a ∈ B and a ∉ B \\ C, and the conclusion, a ∈ C.\nTo figure out this proof, we’ll imitate the reasoning in Example 3.2.5 in HTPI. We begin by writing out the meaning of the given h2. Fortunately, we have a tactic for that. The tactic define writes out the definition of the goal, and as usual we can add at to apply the tactic to a given rather than the goal. Here’s the situation after using the tactic define at h2:\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n  define at h2       --Now h2 : ¬(a ∈ B ∧ ¬a ∈ C)\n  **done::\n\n\nB C : Set ℕ\na : ℕ\nh1 : a ∈ B\nh2 : ¬(a ∈ B ∧ ¬a ∈ C)\n⊢ a ∈ C\n\n\nLooking at the tactic state, we see that Lean has written out the meaning of set difference in h2. And now we can see that, as in Example 3.2.5 in HTPI, we can put h2 into a more useful form by applying first one of De Morgan’s laws to rewrite it as ¬a ∈ B ∨ a ∈ C and then a conditional law to change it to a ∈ B → a ∈ C:\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n  define at h2       --Now h2 : ¬(a ∈ B ∧ ¬a ∈ C)\n  demorgan at h2     --Now h2 : ¬a ∈ B ∨ a ∈ C\n  conditional at h2  --Now h2 : a ∈ B → a ∈ C\n  **done::\n\n\nB C : Set ℕ\na : ℕ\nh1 : a ∈ B\nh2 : a ∈ B → a ∈ C\n⊢ a ∈ C\n\n\nOccasionally, you may feel that the application of two tactics one after the other should be thought of as a single step. To allow for this, Lean lets you put two tactics on the same line, separated by a semicolon. For example, in this proof you could write the use of De Morgan’s law and the conditional law as a single step by writing demorgan at h2; conditional at h2. Now the rest is easy: we can apply modus ponens to reach the goal:\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n  define at h2       --Now h2 : ¬(a ∈ B ∧ ¬a ∈ C)\n  demorgan at h2; conditional at h2\n                     --Now h2 : a ∈ B → a ∈ C\n  show a ∈ C from h2 h1\n  done\n\n\n>>Goals accomplished 🎉\n\n\nThere is one unfortunate feature of this theorem: We have stated it as a theorem about sets of natural numbers, but the proof has nothing to do with natural numbers. Exactly the same reasoning would prove a similar theorem about sets of real numbers, or sets of objects of any other type. Do we need to write a different theorem for each of these cases? No, fortunately there is a way to write one theorem that covers all the cases:\ntheorem Example_3_2_5_simple_general\n    (U : Type) (B C : Set U) (a : U)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\nIn this version of the theorem, we have introduced a new variable U, whose type is … Type! So U can stand for any type. You can think of the variable U as playing the role of the universe of discourse, an idea that was introduced in Section 1.3 of HTPI. The sets B and C contain elements from that universe of discourse, and a belongs to the universe. You can prove the new version of the theorem by using exactly the same sequence of tactics as before."
  },
  {
    "objectID": "Chap3.html",
    "href": "Chap3.html",
    "title": "3  Proofs",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\bigtriangleup}\n$$"
  },
  {
    "objectID": "Chap3.html#proofs-involving-negations-and-conditionals",
    "href": "Chap3.html#proofs-involving-negations-and-conditionals",
    "title": "3  Proofs",
    "section": "3.1 & 3.2. Proofs Involving Negations and Conditionals",
    "text": "3.1 & 3.2. Proofs Involving Negations and Conditionals\nSections 3.1 and 3.2 of How To Prove It present strategies for dealing with givens and goals involving negations and conditionals. We restate those strategies here, and explain how to use them with Lean.\nSection 3.1 gives two strategies for proving a goal of the form P → Q:\n\nTo prove a goal of the form P → Q:\n\nAssume P is true and prove Q.\nAssume Q is false and prove that P is false.\n\nWe’ve already seen how to carry out both of these strategies in Lean. For the first strategy, use the assume tactic to introduce the assumption P and assign an identifier to it; Lean will automaticall set Q as the goal. We can summarize the effect of using this strategy by showing how the tactic state changes if you use the tactic assume h : P:\n\n\n>> ⋮\n⊢ P → Q\n\n\n>> ⋮\nh : P\n⊢ Q\n\n\nThe second strategy is justified by the contrapositive law. In Lean, you can use the contrapos tactic to rewrite the goal as ¬Q → ¬P and then use the tactic assume h : ¬Q. The net effect of these two tactics is:\n\n\n>> ⋮\n⊢ P → Q\n\n\n>> ⋮\nh : ¬Q\n⊢ ¬P\n\n\nSection 3.2 gives two strategies for using givens of the form P → Q, with the second once again being a variation on the first based on the contrapositive law:\n\n\nTo use a given of the form P → Q:\n\nIf you are also given P, or you can prove that P is true, then you can use this given to conclude that Q is true.\nIf you are also given ¬Q, or you can prove that Q is false, then you can use this given to conclude that P is false.\n\nThe first strategy is the modus ponens rule of inference, and we saw in the last chapter that if you have h1 : P → Q and h2 : P, then h1 h2 is a (term-mode) proof of Q; often we use this rule with the have or show tactic. For the second strategy, if you have h1 : P → Q and h2 : ¬Q, then the contrapos at h1 tactic will change h1 to h1 : ¬Q → ¬P, and then h1 h2 will be a proof of ¬P.\nAll of the strategies listed above for working with conditional statements as givens or goals were illustrated in examples in the last chapter.\nSection 3.2 of HTPI offers two strategies for proving negative goals:\n\n\nTo prove a goal of the form ¬P:\n\nReexpress the goal in some other form.\nUse proof by contradiction: assume P is true and try to deduce a contradiction.\n\nFor the first strategy, the tactics demorgan, conditional, double_neg, and bicond_neg may be useful, and we saw how those tactics work in the last chapter. But how do you write a proof by contradiction in Lean? The answer is to use a tactic called by_contra. If the goal is ¬P, then the tactic by_contra h will introduce the assumption h : P and set the goal to be False, like this:\n\n\n>> ⋮\n⊢ ¬P\n\n\n>> ⋮\nh : P\n⊢ False\n\n\nIn Lean, False represents a statement that is always false—that is, a contradiction, as that term is defined in Section 1.2 of HTPI. The by_contra tactic can actually be used even if the goal is not a negative statement. If the goal is a statement P that is not a negative statement, then by_contra h will initiate a proof by contradiction by introducing the assumption h : ¬P and setting the goal to be False.\nYou will usually complete a proof by contradiction by deducing two contradictory statements—say, h1 : Q and h2 : ¬Q. But how do you convince Lean that the proof is over? You must be able to prove the goal False from the two givens h1 and h2. There are two ways to do this. The first is based on the fact that Lean treats a statement of the form ¬Q as meaning the same thing as Q → False. This makes sense, because these statements are logically equivalent, as shown by the following truth table:\n\n\n\n\nQ\n¬Q\n(Q\n→\nFalse)\n\n\n\n\nF\nT\nF\nT\n    F\n\n\nT\nF\nT\nF\n    F\n\n\n\n\nThinking of h2 : ¬Q as meaning h2 : Q → False, we can combine it with h1 : Q using modus ponens to deduce False. In other words, h2 h1 is a proof of False.\nBut there is a second way of completing the proof that it is worthwhile to know about. From contradictory statements h1 : Q and h2 : ¬Q you can validly deduce any statement. This follows from the definition of a valid argument in Section 1.1 of HTPI. According to that definition, you can validly infer a conclusion R from premises h1 : Q and h2 : ¬Q if the premises cannot both be true without the conclusion also being true. In this case, that standard is met, for the simple reason that the premises cannot both be true! (This gives part of the answer to exercise 18 in Section 1.2 of HTPI.) Thus, Lean has a rule that allows you to prove any statement from contradictory premises. If you have h1 : Q and h2 : ¬Q, then Lean will recognize absurd h1 h2 as a (term-mode) proof of any statement.\nTo summarize, if you have h1 : Q and h2 : ¬Q, then there are two ways to prove False. Lean will recognize h2 h1 as a proof of False, and it will recognize absurd h1 h2 as a proof of any statement, including False. Notice the difference in the order in which h1 and h2 are listed in these two proofs: In the first one, the negative statement h2 must come first, just as the conditional statement must come first in an application of modus ponens. But in a proof using absurd, the negative statement must come second.\nTo illustrate proof by contradiction in Lean, let’s redo our first example from the last Chapter in a different way. That example was based on Example 3.2.4 in HTPI. We’ll begin with the same first two steps, introducing two assumptions. (We won’t bother to include the done line in the displays below.)\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\n⊢ ¬Q\n\n\nNow the goal is a negative statement, so we use the tactic by_contra h4 to introduce the assumption h4 : Q and set the goal to be False:\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\n⊢ False\n\n\nUsing the givens h, h3, and h4 we can deduce first Q → R and then R by two applications of modus ponens:\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  have h5 : Q → R := h h3\n  have h6 : R := h5 h4\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\nh5 : Q → R\nh6 : R\n⊢ False\n\n\nNow we have a contradiction: h2 : ¬R and h6 : R. To complete the proof, we deduce False from these two givens. Either h2 h6 or absurd h6 h2 would be accepted by Lean as a proof of False:\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  have h5 : Q → R := h h3\n  have h6 : R := h5 h4\n  show False from h2 h6\n\n\n>>Goals accomplished 🎉\n\n\nFinally, we have two strategies for using a given that is a negative statement:\n\n\nTo use a given of the form ¬P:\n\nReexpress the given in some other form.\nIf you are doing a proof by contradiction, you can achieve a contradiction by proving P, since that would contradict the given ¬P.\n\nOf course, strategy 1 suggests the use of the demorgan, conditional, double_neg, and bicond_neg tactics, if they apply. For strategy 2, if you are doing a proof by contradiction and you have a given h : ¬P, then the tactic contradict h will set the goal to be P, which will complete the proof by contradicting h. In fact, this tactic can be used with any given; if you have a given h : P, where P is not a negative statement, then contradict h will set the goal to be ¬P. You can also follow the word contradict with a proof that is more complicated than a single identifier. For example, if you have givens h1 : P → ¬Q and h2 : P, then h1 h2 is a proof of ¬Q, so the tactic contradict h1 h2 will set the goal to be Q.\nIf you’re not doing a proof by contradiction, then the tactic contradict h with h' will first initiate a proof by contradiction by assuming the negation of the goal, giving that assumption the identifier h', and then it will set the goal to be the negation of the statement proven by h. In other words, contradict h with h' is shorthand for by_contra h'; contradict h.\nWe can illustrate this with yet another way to write the proof from Example 3.2.4. Our first three steps will be the same as last time:\n\n\ntheorem Example_3_2_4_v3 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\n⊢ False\n\n\nSince we are now doing a proof by contradiction and the given h2 : ¬R is a negative statement, a likely way to proceed is to try to prove R, which would contradict h2. So we use the tactic contradict h2:\n\n\ntheorem Example_3_2_4_v3 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  contradict h2\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\n⊢ R\n\n\nAs before, we can now prove R by combining h, h3, and h4. In fact, we could do it in one step: by modus ponens, h h3 is a proof of Q → R, and therefore, by another application of modus ponens, (h h3) h4 is a proof of R. The parentheses here are not necessary; Lean will interpret h h3 h4 as (h h3) h4, so we can complete the proof like this:\n\n\ntheorem Example_3_2_4_v3 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  contradict h2\n  show R from h h3 h4\n\n\n>>Goals accomplished 🎉\n\n\nYou could shorten this proof slightly by replacing the lines by_contra h4 and contradict h2 with the single line contradict h2 with h4.\nThere is one more idea that is introduced in Section 3.2 of HTPI. The last example in that section illustrates how you can sometimes use rules of inference to work backwards. Here’s a similar example in Lean:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n\n\nU : Type\nA B C : Set U\na : U\nh1 : a ∈ A\nh2 : ¬a ∈ A \\ B\nh3 : a ∈ B → a ∈ C\n⊢ a ∈ C\n\n\nThe goal is a ∈ C, and the only given that even mentions C is h3 : a ∈ B → a ∈ C. If only we could prove a ∈ B, then we could apply h3, using modus ponens, to reach our goal. So it would make sense to work toward the goal of proving a ∈ B.\nTo get Lean to use this proof strategy, we use the tactic apply h3 _. The underscore here represents a blank to be filled in by Lean. You might think of this tactic as asking Lean the question: If we want h3 _ to be a proof of the goal a ∈ C, what do we have to put in the blank? Lean is able to figure out that the answer is: a proof of a ∈ B. So it sets the goal to be a ∈ B, since a proof of that goal, when inserted into the blank in h3 _, would prove the original goal a ∈ C:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n  apply h3 _\n\n\nU : Type\nA B C: Set U\na : U\nh1 : a ∈ A\nh2 : ¬a ∈ A \\ B\nh3 : a ∈ B → a ∈ C\n⊢ a ∈ B\n\n\nIt may not be clear what to do next, but the given h2 is a negative statement, so perhaps reexpressing it will help. Writing out the definition of set difference, h2 means ¬(a ∈ A ∧ a ∉ B), and then one of De Morgan’s laws and a conditional law allow us to rewrite it first as a ∉ A ∨ a ∈ B and then as a ∈ A → a ∈ B. Of course, we have tactics to accomplish all of these reexpressions:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n  apply h3 _\n  define at h2\n  demorgan at h2; conditional at h2\n\n\nU : Type\nA B C : Set U\na : U\nh1 : a ∈ A\nh2 : a ∈ A → a ∈ B\nh3 : a ∈ B → a ∈ C\n⊢ a ∈ B\n\n\nAnd now it is easy to complete the proof by applying modus ponens, using h2 and h1:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n  apply h3 _\n  define at h2\n  demorgan at h2; conditional at h2\n  show a ∈ B from h2 h1\n\n\n>>Goals accomplished 🎉\n\n\nWe will see many more uses of the apply tactic later in this book.\nSections 3.1 and 3.2 of HTPI contain several proofs that involve algebraic reasoning. Although one can do such proofs in Lean, it requires ideas that we are not ready to introduce yet. So for the moment we will stick to proofs involving only logic and set theory.\n\n\nExercises\nFill in proofs of the following theorems. All exercises are taken from HTPI.\n\ntheorem Exercise_3_2_1a (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → R) : P → R := by\n  \n  **done::\n\n\ntheorem Exercise_3_2_1b (P Q R : Prop)\n    (h1 : ¬R → (P → ¬Q)) : P → (Q → R) := by\n  \n  **done::\n\n\ntheorem Exercise_3_2_2a (P Q R : Prop)\n    (h1 : P → Q) (h2 : R → ¬Q) : P → ¬R := by\n  \n  **done::\n\n\ntheorem Exercise_3_2_2b (P Q : Prop)\n    (h1 : P) : Q → ¬(Q → ¬P) := by\n  \n  **done::"
  },
  {
    "objectID": "Chap3.html#proofs-involving-quantifiers",
    "href": "Chap3.html#proofs-involving-quantifiers",
    "title": "3  Proofs",
    "section": "3.3. Proofs Involving Quantifiers",
    "text": "3.3. Proofs Involving Quantifiers\nIn the notation used in HTPI, if \\(P(x)\\) is a statement about \\(x\\), then \\(\\forall x\\, P(x)\\) means “for all \\(x\\), \\(P(x)\\),” and \\(\\exists x\\, P(x)\\) means “there exists at least one \\(x\\) such that \\(P(x)\\).” The letter \\(P\\) here does not stand for a proposition; it is only when it is applied to some object \\(x\\) that we get a proposition. We will say that \\(P\\) is a predicate, and when we apply \\(P\\) to an object \\(x\\) we get the proposition \\(P(x)\\). You might want to think of the predicate \\(P\\) as representing some property that an object might have, and the proposition \\(P(x)\\) asserts that \\(x\\) has that property.\nTo use a predicate in Lean, you must tell Lean the type of objects to which it applies. If U is a type, then Pred U is the type of predicates that apply to objects of type U. If P has type Pred U (that is, P is a predicate applying to objects of type U) and x has type U, then to apply P to x we just write P x (with a space but no parentheses). Thus, if we have P : Pred U and x : U, then P x is an expression of type Prop. That is, P x is a proposition, and its meaning is that x has the property represented by the predicate P.\nThere are a few differences between the way quantified statements are written in HTPI and the way they are written in Lean. First of all, when we apply a quantifier to a variable in Lean we will specify the type of the variable explicitly. Also, Lean requires that after specifying the variable and its type, you must put a comma before the proposition to which the quantifier is applied. Thus, if P has type Pred U, then to say that P holds for all objects of type U we would write ∀ (x : U), P x. Similarly, ∃ (x : U), P x is the proposition asserting that there exists at least one x of type U such that P x.\nAnd there is one more important difference between the way quantified statements are written in HTPI and Lean. In HTPI, a quantifier is interpreted as applying to as little as possible. Thus, \\(\\forall x\\, P(x) \\wedge Q(x)\\) is interpreted as \\((\\forall x\\, P(x)) \\wedge Q(x)\\); if you want the quantifier \\(\\forall x\\) to apply to the entire statement \\(P(x) \\wedge Q(x)\\) you must use parentheses and write \\(\\forall x(P(x) \\wedge Q(x))\\). The convention in Lean is exactly the opposite: a quantifier applies to as much as possible. Thus, Lean will interpret ∀ (x : U), P x ∧ Q x as meaning ∀ (x : U), (P x ∧ Q x). If you want the quantifier to apply to only P x, then you must use parentheses and write (∀ (x : U), P x) ∧ Q x.\nWith this preparation, we are ready to consider how to write proofs involving quantifiers in Lean. The most common way to prove a goal of the form ∀ (x : U), P x is to use the following strategy:\n\nTo prove a goal of the form ∀ (x : U), P x:\n\nLet x stand for an arbitrary object of type U and prove P x. If the letter x is already being used in the proof to stand for something, then you must choose an unused variable, say y, to stand for the arbitrary object, and prove P y.\n\nTo do this in Lean, you should use the tactic fix x : U, which tells Lean to treat x as standing for some fixed but arbitrary object of type U. This has the following effect on the tactic state:\n\n\n>> ⋮\n⊢ ∀ (x : U), P x\n\n\n>> ⋮\nx : U\n⊢ P x\n\n\nTo use a given of the form ∀ (x : U), P x, we usually apply a rule of inference called universal instantiation, which is described by the following proof strategy:\n\n\nTo use a given of the form ∀ (x : U), P x:\n\nYou may plug in any value of type U, say a, for x and use this given to conclude that P a is true.\n\nThis strategy says that if you have h : ∀ (x : U), P x and a : U, then you can infer P a. Indeed, in this situation Lean will recognize h a as a proof of P a. For example, you can write have h' : P a := h a in a Lean tactic-mode proof, and Lean will add h' : P a to the tactic state.\nLet’s try these strategies out in a Lean proof. In Lean, if you don’t want to give a theorem a name, you can simply call it an example rather than a theorem, and then there is no need to give it a name. In the following theorem, you can enter the symbol ∀ by typing \\forall or \\all, and you can enter ∃ by typing \\exists or \\ex.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\n⊢ ¬∃ x, P x\n\n\n(In the tactic state, why doesn’t Lean show the type of the variable x after the existential quantifier in the goal? I don’t know. Sometimes you can leave out the type of a quantified variable and Lean is able to figure it out on its own. But sometimes Lean is unable to figure out the type if it is not supplied, and you will get an error message if you leave it out. To avoid confusion, we will always include the type of the quantified variable when we enter a quantified statement into Lean, but you will notice that Lean generally leaves out the type when it displays existentially quantified statements in the tactic state.)\nTo use the givens h1 and h2, we will probably want to use universal instantiation. But to do that we would need an object of type U to plug in for x in h1 and h2, and there is no object of type U in the tactic state. So at this point, we can’t apply universal instantiation to h1 and h2. We should watch for an object of type U to come up in the course of the proof, and consider applying universal instantiation if one does. Until then, we turn our attention to the goal.\nThe goal is a negative statement, so we begin by reexpressing it as an equivalent positive statement, using a quantifier negation law. The tactic quant_neg applies a quantifier negation law to rewrite the goal. As with the other tactics for applying logical equivalences, you can write quant_neg at h if you want to apply a quantifier negation law to a given h. The effect of the tactic can be summarized as follows:\n\n\n\n\n\nquant_neg Tactic\n\n\n\n\n\n¬∀ (x : U), P x\nis changed to\n∃ (x : U), ¬P x\n\n\n¬∃ (x : U), P x\nis changed to\n∀ (x : U), ¬P x\n\n\n∀ (x : U), P x\nis changed to\n¬∃ (x : U), ¬P x\n\n\n∃ (x : U), P x\nis changed to\n¬∀ (x : U), ¬P x\n\n\n\n\nUsing the quant_neg tactic leads to the following result.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\n⊢ ∀ (x : U), ¬P x\n\n\nNow the goal starts with ∀, so we use the strategy above and introduce an arbitrary object of type U. Since the variable x occurs as a bound variable in several statements in this theorem, it might be best to use a different letter for the arbitrary object; this isn’t absolutely necessary, but it may help to avoid confusion. So our next tactic is fix y : U.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n  fix y : U\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\ny : U\n⊢ ¬P y\n\n\nNow we have an object of type U in the tactic state, namely, y. So let’s try applying universal instantiation to h1 and h2 and see if it helps.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n  fix y : U\n  have h3 : P y → ¬Q y := h1 y\n  have h4 : Q y := h2 y\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\ny : U\nh3 : P y → ¬Q y\nh4 : Q y\n⊢ ¬P y\n\n\nWe’re almost done, because the goal now follows easily from h3 and h4. If we use the contrapositive law to rewrite h3 as Q y → ¬P y, then we can apply modus ponens to the rewritten h3 and h4 to reach the goal:\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n  fix y : U\n  have h3 : P y → ¬Q y := h1 y\n  have h4 : Q y := h2 y\n  contrapos at h3  --Now h3 : Q y → ¬P y\n  show ¬P y from h3 h4\n\n\n>>Goals accomplished 🎉\n\n\nOur next example is a theorem of set theory. You already know how to type a few set theory symbols in Lean, but you’ll need a few more for our next example. Here’s a summary of the most important set theory symbols and how to type them in Lean.\n\n\n\n\nSymbol\nHow To Type It\n\n\n\n\n∈\n\\in\n\n\n∉\n\\notin or \\inn\n\n\n⊆\n\\sub\n\n\n⊈\n\\subn\n\n\n=\n=\n\n\n≠\n\\ne\n\n\n∪\n\\union or \\cup\n\n\n∩\n\\inter or \\cap\n\n\n\\\n\\\\\n\n\n△\n\\bigtriangleup\n\n\n∅\n\\emptyset\n\n\n𝒫\n\\powerset\n\n\n\n\nWith this preparation, we can turn to our next example.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∈ A → ¬x ∈ B\n⊢ A ⊆ C\n\n\nWe begin by using the define tactic to write out the definition of the goal.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal: ∀ {a : U}, a ∈ A → a ∈ C\n\n\nU : Type\nA B C: Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∈ A → ¬x ∈ B\n⊢ ∀ {a : U},\n>>  a ∈ A → a ∈ C\n\n\nNotice that Lean’s definition of the goal starts with ∀ {a : U}, not ∀ (a : U). Why did Lean use curly braces rather than parentheses? We’ll return to that question shortly. The difference doesn’t affect our next steps, which are to introduce an arbitrary object y of type U and assume y ∈ A.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal: ∀ {a : U}, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∈ A → ¬x ∈ B\ny : U\nh3 : y ∈ A\n⊢ y ∈ C\n\n\nNow we can combine h2 and h3 to conclude that ¬y ∈ B. Since we have y : U, by universal instantiation, h2 y is a proof of y ∈ A → ¬y ∈ B, and therefore by modus ponens, h2 y h3 is a proof of ¬y ∈ B.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal: ∀ {a : U}, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∈ A → ¬x ∈ B\ny : U\nh3 : y ∈ A\nh4 : ¬y ∈ B\n⊢ y ∈ C\n\n\nWe should be able to use similar reasoning to combine h1 and h3, if we first write out the definition of h1.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal: ∀ {a : U}, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ {a : U}, a ∈ U → a ∈ B ∪ C\n\n\nU : Type\nA B C : Set U\nh1 : ∀ {a : U},\n>>  a ∈ A → a ∈ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∈ A → ¬x ∈ B\ny : U\nh3 : y ∈ A\nh4 : ¬y ∈ B\n⊢ y ∈ C\n\n\nOnce again, Lean has used curly braces to define h1, and now we are ready to explain what they mean. If the definition had been h1 : ∀ (a : U), a ∈ A → a ∈ B ∪ C, then exactly as in the previous step, h1 y h3 would be a proof of y ∈ B ∪ C. The use of curly braces in the definition h1 : ∀ {a : U}, a ∈ A → a ∈ B ∪ C means that you don’t need to tell Lean that y is being plugged in for a in the universal instantiation step; Lean will figure that out on its own. Thus, you can just write h1 h3 as a proof of y ∈ B ∪ C. Indeed, if you write h1 y h3 then you will get an error message, because Lean expects not to be told what to plug in for a. You might think of the definition of h1 as meaning h1 : _ ∈ A → _ ∈ B ∪ C, where the blanks can be filled in with anything of type U (with the same thing being put in both blanks). When you ask Lean to apply modus ponens by combining this statement with h3 : y ∈ A, Lean figures out that in order for modus ponens to apply, the blanks must be filled in with y.\nIn this situation, the a in h1 is called an implicit argument. What this means is that, when h1 is applied to make an inference in a proof, the value to be assigned to a is not specified explicitly; rather, the value is implicit. We will see many more examples of implicit arguments later in this book.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal: ∀ {a : U}, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ {a : U}, a ∈ U → a ∈ B ∪ C\n  have h5 : y ∈ B ∪ C := h1 h3\n\n\nU : Type\nA B C : Set U\nh1 : ∀ {a : U},\n>>  a ∈ A → a ∈ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∈ A → ¬x ∈ B\ny : U\nh3 : y ∈ A\nh4 : ¬y ∈ B\nh5 : y ∈ B ∪ C\n⊢ y ∈ C\n\n\nIf Lean was able to figure out that y should be plugged in for a in h1 in this step, couldn’t it have figured out that y should be plugged in for x in h2 in the previous have step? The answer is yes. Of course, in h2, x was not an implicit argument, so Lean wouldn’t automatically figure out what to plug in for x. But we could have asked it to figure it out by writing the proof in the previous step as h2 _ h3 rather than h2 y h3. In a term-mode proof, an underscore represents a blank to be filled in by Lean. Try changing the earlier step of the proof to have h4 : y ∉ B := h2 _ h3 and you will see that Lean will accept it. Of course, in this case this doesn’t save us any typing, but in some situations it is useful to let Lean figure out some part of a proof.\nLean’s ability to fill in blanks in term-mode proofs is limited. For example, if you try changing the previous step to have h4 : y ∉ B := h2 y _, you’ll get a red squiggle under the blank, and the error message in the Infoview pane will say don't know how to synthesize placeholder. In other words, Lean was unable to figure out how to fill in the blank in this case. In future proofs you might try replacing some expressions with blanks to get a feel for what Lean can and cannot figure out for itself.\nContinuing with the proof, we see that we’re almost done, because we can combine h4 and h5 to reach our goal. To see how, we first write out the definition of h5.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal: ∀ {a : U}, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ {a : U}, a ∈ U → a ∈ B ∪ C\n  have h5 : y ∈ B ∪ C := h1 h3\n  define at h5  --h5 : y ∈ B ∨ y ∈ C\n\n\nU : Type\nA B C : Set U\nh1 : ∀ {a : U},\n>>  a ∈ A → a ∈ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∈ A → ¬x ∈ B\ny : U\nh3 : y ∈ A\nh4 : ¬y ∈ B\nh5 : y ∈ B ∨ y ∈ C\n⊢ y ∈ C\n\n\nA conditional law will convert h5 to ¬y ∈ B → y ∈ C, and then modus ponens with h4 will complete the proof.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal: ∀ {a : U}, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ {a : U}, a ∈ U → a ∈ B ∪ C\n  have h5 : y ∈ B ∪ C := h1 h3\n  define at h5  --h5 : y ∈ B ∨ y ∈ C\n  conditional at h5  --h5 : ¬y ∈ B → y ∈ C\n  show y ∈ C from h5 h4\n\n\n>>Goals accomplished 🎉\n\n\nNext we turn to strategies for working with existential quantifiers.\n\n\nTo prove a goal of the form ∃ (x : U), P x:\n\nFind a value of x, say a, for which you think P a is true, and prove P a.\n\nThis strategy is based on the fact that if you have a : U and h : P a, then you can infer ∃ (x : U), P x. Indeed, in this situation the expression Exists.intro a h is a Lean term-mode proof of ∃ (x : U), P x. The name Exists.intro indicates that this is a rule for introducing an existential quantifier.\nAs suggested by the strategy above, we will often want to use this rule in situations in which our goal is ∃ (x : U), P x and we have an object a of type U that we think makes P a true, but we don’t yet have a proof of P a. In that situation we can use the tactic apply Exists.intro a _. Recall that the apply tactic asks Lean to figure out what to put in the blank to turn Exists.intro a _ into a proof of the goal. Lean will figure out that what needs to go in the blank is a proof of P a, so it sets P a to be the goal. In other words, the tactic apply Exists.intro a _ has the following effect on the tactic state:\n\n\n>> ⋮\na : U\n⊢ ∃ (x : U), P x\n\n\n>> ⋮\na : U\n⊢ P a\n\n\nOur strategy for using an existential given is a rule that is called existential instantiation in HTPI:\n\n\nTo use a given of the form ∃ (x : U), P x:\n\nIntroduce a new variable, say a, into the proof to stand for an object of type U for which P a is true.\n\nSuppose that, in a Lean proof, you have h : ∃ (x : U), P x. To apply the existential instantiation rule, you would use the tactic obtain (a : U) (h' : P a) from h. This tactic introduces into the tactic state both a new variable a of type U and also the identifier h' for the new given P a. Note that h can be any proof of a statement of the form ∃ (x : U), P x; it need not be just a single identifier.\nOften, if your goal is an existential statement ∃ (x : U), P x, you won’t be able to use the strategy above for existential goals right away, because you won’t know what object a to use in the tactic apply Exists.intro a _. You may have to wait until a likely candidate for a pops up in the course of the proof. On the other hand, it is usually best to use the obtain tactic right away if you have an existential given. This is illustrated in our next example.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ y,\n>>  P x → ¬Q y\nh2 : ∃ x, ∀ (y : U),\n>>  P x → Q y\n⊢ ∃ x, ¬P x\n\n\nThe goal is the existential statement ∃ (x : U), ¬P x, and our strategy for existential goals says that we should try to find an object a of type U that we think would make the statement ¬P a true. But we don’t have any objects of type U in the tactic state, so it looks like we can’t use that strategy yet. Similarly, we can’t use the given h1 yet, since we have nothing to plug in for x in h1. However, h2 is an existential given, and we can use it right away.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ y,\n>>  P x → ¬Q y\nh2 : ∃ x, ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\n⊢ ∃ x, ¬P x\n\n\nNow that we have a : U, we can apply universal instantiation to h1, plugging in a for x.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ y,\n>>  P x → ¬Q y\nh2 : ∃ x, ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ y, P a → ¬Q y\n⊢ ∃ x, ¬P x\n\n\nBy the way, this is another case in which Lean could have figured out a part of the proof on its own. Try changing h1 a in the last step to h1 _, and you’ll see that Lean will be able to figure out how to fill in the blank.\nOur new given h4 is another existential statement, so again we use it right away to introduce another object of type U. Since this object might not be the same as a, we must give it a different name. (Indeed, if you try to use the name a again, Lean will give you an error message.)\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ y,\n>>  P x → ¬Q y\nh2 : ∃ x, ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ y, P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\n⊢ ∃ x, ¬P x\n\n\nWe have not yet used h3. We could plug in either a or b for y in h3, but a little thought should show you that plugging in b is more useful.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ y,\n>>  P x → ¬Q y\nh2 : ∃ x, ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ y, P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\nh6 : P a → Q b\n⊢ ∃ x, ¬P x\n\n\nNow look at h5 and h6. They show that P a leads to contradictory conclusions, ¬Q b and Q b. This means that P a must be false. We finally know what value of x to use to prove the goal.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n  apply Exists.intro a _\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ y,\n>>  P x → ¬Q y\nh2 : ∃ x, ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ y, P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\nh6 : P a → Q b\n⊢ ¬P a\n\n\nSince the goal is now a negative statement that cannot be reexpressed as a positive statement, we use proof by contradiction.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n  apply Exists.intro a _\n  by_contra h7\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ y,\n>>  P x → ¬Q y\nh2 : ∃ x, ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ y, P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\nh6 : P a → Q b\nh7 : P a\n⊢ False\n\n\nNow h5 h7 is a proof of ¬Q b and h6 h7 is a proof of Q b, so h5 h7 (h6 h7) is a proof of False.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n  apply Exists.intro a _\n  by_contra h7\n  show False from h5 h7 (h6 h7)\n\n\n>>Goals accomplished 🎉\n\n\nWe conclude this section with the theorem from Example 3.3.5 in HTPI. That theorem concerns a union of a family of sets. In HTPI, such a union is written using a large union symbol, \\(\\bigcup\\). Lean uses the symbol ∪₀, which is entered by typing \\U0 (that is, backslash–capital U–zero). For an intersection of a family of sets, Lean uses ∩₀, typed as \\I0.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n\n\nU : Type\nB : Set U\nF : Set (Set U)\n⊢ ⋃₀F ⊆ B → F ⊆ 𝒫 B\n\n\nNote that F has type Set (Set U), which means that it is a set whose elements are sets of objects of type U. Since the goal is a conditional statement, we assume the antecedent and set the consequent as our goal. We’ll also write out the definition of the new goal.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ⋃₀F ⊆ B\n⊢ ∀ {a : Set U},\n>>  a ∈ F → a ∈ 𝒫 B\n\n\nBased on the form of the goal, we introduce an arbitrary object x of type Set U and assume x ∈ F. The new goal will be x ∈ 𝒫 B. The define tactic works out that this means x ⊆ B, which can be further expanded to ∀ {a : U}, a ∈ x → a ∈ B.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ⋃₀F ⊆ B\nx : Set U\nh2 : x ∈ F\n⊢ ∀ {a : U},\n>>  a ∈ x → a ∈ B\n\n\nOnce again the form of the goal dictates our next steps: introduce an arbitrary y of type U and assume y ∈ x.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ⋃₀F ⊆ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ y ∈ B\n\n\nThe goal can be analyzed no further, so we turn to the givens. We haven’t used h1 yet. To see how to use it, we write out its definition.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ {a : U},\n>>  a ∈ ⋃₀F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ y ∈ B\n\n\nNow we see that we can try to use h1 to reach our goal. Indeed, h1 _ would be a proof of the goal if we could fill in the blank with a proof of y ∈ ∪₀F. So we use the apply h1 _ tactic.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ {a : U},\n>>  a ∈ ⋃₀F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ y ∈ ⋃₀F\n\n\nOnce again we have a goal that can be analyzed by using the define tactic.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n  define\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ {a : U},\n>>  a ∈ ⋃₀F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ ∃ a, a ∈ F ∧ y ∈ a\n\n\nOur goal is now an existential statement, so we look for a value of a that will make the statement a ∈ F ∧ y ∈ a true. The givens h2 and h3 tell us that x is such a value, so as described earlier our next tactic should be apply Exists.intro x _.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n  define\n  apply Exists.intro x _\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ {a : U},\n>>  a ∈ ⋃₀F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ x ∈ F ∧ y ∈ x\n\n\nClearly the goal now follows from h2 and h3, but how do we write the proof in Lean? Since we need to introduce the “and” symbol ∧, you shouldn’t be surprised to learn that the rule we need is called And.intro. Proof strategies for statements involving “and” will be the subject of the next section.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ∪₀F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n  define\n  apply Exists.intro x _\n  show x ∈ F ∧ y ∈ x from And.intro h2 h3\n\n\n>>Goals accomplished 🎉\n\n\nYou might want to compare the Lean proof above to the way the proof was written in HTPI. Here are the theorem and proof from HTPI:\n\nSuppose \\(B\\) is a set and \\(\\mathcal{F}\\) is a family of sets. If \\(\\bigcup\\mathcal{F} \\subseteq B\\) then \\(\\mathcal{F} \\subseteq \\mathscr{P}(B)\\).\n\n\nProof. Suppose \\(\\bigcup \\mathcal{F} \\subseteq B\\). Let \\(x\\) be an arbitrary element of \\(\\mathcal{F}\\). Let \\(y\\) be an arbitrary element of \\(x\\). Since \\(y \\in x\\) and \\(x \\in \\mathcal{F}\\), by the definition of \\(\\bigcup \\mathcal{F}\\), \\(y \\in \\bigcup \\mathcal{F}\\). But then since \\(\\bigcup \\mathcal{F} \\subseteq B\\), \\(y \\in B\\). Since \\(y\\) was an arbitrary element of \\(x\\), we can conclude that \\(x \\subseteq B\\), so \\(x \\in \\mathscr{P}(B)\\). But \\(x\\) was an arbitrary element of \\(\\mathcal{F}\\), so this shows that \\(\\mathcal{F} \\subseteq \\mathscr{P}(B)\\), as required.  □\n\n\n\nExercises\n\ntheorem Exercise_3_3_1\n    (U : Type) (P Q : Pred U) (h1 : ∃ (x : U), P x → Q x) :\n    (∀ (x : U), P x) → ∃ (x : U), Q x := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_8 (U : Type) (F : Set (Set U)) (A : Set U)\n    (h1 : A ∈ F) : A ⊆ ⋃₀F := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_9 (U : Type) (F : Set (Set U)) (A : Set U)\n    (h1 : A ∈ F) : ⋂₀F ⊆ A := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_10 (U : Type) (B : Set U) (F : Set (Set U))\n    (h1 : ∀ (A : Set U), A ∈ F → B ⊆ A) : B ⊆ ⋂₀F := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_13 (U : Type)\n    (F G : Set (Set U)) : F ⊆ G → ⋂₀G ⊆ ⋂₀F := by\n  \n  **done::"
  },
  {
    "objectID": "Chap3.html#proofs-involving-conjunctions-and-biconditionals",
    "href": "Chap3.html#proofs-involving-conjunctions-and-biconditionals",
    "title": "3  Proofs",
    "section": "3.4. Proofs Involving Conjunctions and Biconditionals",
    "text": "3.4. Proofs Involving Conjunctions and Biconditionals\nThe strategies in HTPI for working with conjunctions are very simple.\n\nTo prove a goal of the form P ∧ Q:\n\nProve P and Q separately.\n\nWe already saw an example, at the end of the last section, of the use of the rule And.intro to prove a conjunction. In general, if you have h1 : P and h2 : Q, then And.intro h1 h2 is a proof of P ∧ Q. It follows that if your goal is P ∧ Q but you don’t yet have proofs of P and Q, then you can use the tactic apply And.intro _ _. Lean will figure out that the blanks need to be filled in with proofs of P and Q, so it will ask you to prove P and Q separately, as suggested by the strategy above.\nIf you already have a proof of either P or Q, then you can fill in one of the blanks in the apply tactic. For example, if you have h : P, then you can write apply And.intro h _, and Lean will tell you that you just have to prove Q to complete the proof. Similarly, if you have h : Q, then apply And.intro _ h will lead to just the single goal P. There is also a shortcut you can use with the apply tactic: any blanks that come at the end of the tactic can be left out. So instead of apply And.intro _ _, you can just write apply And.intro, and instead of apply And.intro h _, you can write apply And.intro h. On the other hand, apply And.intro _ h can’t be shortened; it is only blanks at the end that can be left out.\nThe strategy for a given that is a conjunction is similar.\n\n\nTo use a given of the form P ∧ Q:\n\nTreat this as two separate givens: P, and Q.\n\nIf you have a given h : P ∧ Q, then Lean will recognize h.left as a proof of P, and h.right as a proof of Q.\nHere’s an example that illustrates these strategies. It is similar to Example 3.4.1 in HTPI\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n\n\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\n⊢ A ∩ C ⊆ B \\ D\n\n\nThe define tactic will rewrite the goal as ∀ {a : U}, a ∈ A ∩ C → a ∈ B \\ D, and then we can introduce an arbitrary x : U and assume x ∈ A ∩ C.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n\n\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∩ C\n⊢ x ∈ B \\ D\n\n\nNow let’s take a look at the definitions of h3 and the goal:\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n\n\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ x ∈ B ∧ ¬x ∈ D\n\n\nSince the goal is now a conjuction, we use the strategy above by using the tactic apply And.intro.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n\n\ncase left\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ x ∈ B\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ ¬x ∈ D\n\n\nLook carefully at the tactic state. Lean has listed two goals, one after the other, and it has helpfully labeled them “case left” and “case right,” indicating that the first goal is proving the left side of the conjunction and the second is proving the right. The given information in both cases is the same, but in the first case the goal is x ∈ B, and in the second it is ¬x ∈ D. As we continue with the proof, Lean will interpret our tactics as applying to the first goal, until we achieve that goal. Once we achieve it, Lean will move on to the second goal. To make the proof more readable, we will add comments indicating which steps give the proof of the first goal and which prove the second. (If, for some reason, you prefer to prove the second goal first, the tactic swap will switch the order of the two goals.)\nThe first goal is easy: We have h1 : A ⊆ B and, as explained above, h3.left : x ∈ A. As we have seen in several previous examples, the tactic define at h1 will rewrite h1 as ∀ {a : U}, a ∈ A → a ∈ B, and then h1 h3.left will be a proof of x ∈ B. And now we’ll let you in on a little secret: the define tactic isn’t really necessary. You may find the define tactic to be useful in many situations, because it helps you see what a statement means. But Lean doesn’t need to be told to work out what the statement means; it will do that automatically. So we can skip the define tactic and just give h1 h3.left as a proof of x ∈ B. In general, if you have h1 : A ⊆ B and h2 : x ∈ A, then Lean will recognize h1 h2 as a proof of x ∈ B.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  -- Proof that x ∈ B:\n  show x ∈ B from h1 h3.left\n\n\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ ¬x ∈ D\n\n\nNote that Lean has recognized that the first goal has been achieved, so it has moved on to the second goal. This goal is a negative statement, and the given h2 is also a negative statement. This suggests using proof by contradiction, and achieving the contradiction by contradicting h2.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  -- Proof that x ∈ B:\n  show x ∈ B from h1 h3.left\n  -- Proof that ¬x ∈ D:\n  contradict h2 with h4\n\n\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\nh4 : x ∈ D\n⊢ ∃ c, c ∈ C ∩ D\n\n\nThe goal is now an existential statement, and looking at h3 and h4 it is clear that the right value to plug in for c in the goal is x. The tactic apply Exists.intro x will change the goal to x ∈ C ∩ D (we have again left off the unnecessary blank at the end of the apply tactic).\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  -- Proof that x ∈ B:\n  show x ∈ B from h1 h3.left\n  -- Proof that ¬x ∈ D:\n  contradict h2 with h4\n  apply Exists.intro x\n\n\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ c, c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\nh4 : x ∈ D\n⊢ x ∈ C ∩ D\n\n\nThe define tactic would now rewrite the goal as x ∈ C ∧ x ∈ D, and we could prove this goal by combining h3.right and h4, using the And.intro rule. But since we know what the result of the define tactic will be, there is really no need to use it. We can just use And.intro right away to complete the proof.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  -- Proof that x ∈ B:\n  show x ∈ B from h1 h3.left\n  -- Proof that ¬x ∈ D:\n  contradict h2 with h4\n  apply Exists.intro x\n  show x ∈ C ∩ D from And.intro h3.right h4\n\n\n>>Goals accomplished 🎉\n\n\nSince P ↔︎ Q is shorthand for (P → Q) ∧ (Q → P), the strategies given above for conjunctions lead immediately to the following strategies for biconditionals:\n\n\nTo prove a goal of the form P ↔︎ Q:\n\nProve P → Q and Q → P separately.\n\n\n\nTo use a given of the form P ↔︎ Q:\n\nTreat this as two separate givens: P → Q, and Q → P.\n\nThe methods for using these strategies in Lean are similar to those we used above for conjunctions. If we have h1 : P → Q and h2 : Q → P, then Iff.intro h1 h2 is a proof of P ↔︎ Q. Thus, if the goal is P ↔︎ Q, then the tactic apply Iff.intro _ _ will convert this into two separate goals, P → Q and Q → P. Once again, you can fill in one of these blanks if you already have a proof of either P → Q or Q → P, and you can leave out any blanks at the end of the tactic. If you have a given h : P ↔︎ Q, then h.ltr is a proof of the left-to-right direction of the biconditional, P → Q, and h.rtl is a proof of the right-to-left direction, Q → P.\nLet’s try these strategies out in an example.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ x, P x) ↔ ∃ x, Q x\n\n\nThe goal is a biconditional statement, so we begin with the tactic apply Iff.intro.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ x, P x) → ∃ x, Q x\ncase mpr\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ x, Q x) → ∃ x, P x\n\n\nOnce again, we have two goals. (The case labels this time aren’t very intuitive; “mp” stands for “modus ponens” and “mpr” stands for “modus ponens reverse”.) As in HTPI, we’ll label the proofs of the two goals with (→) and (←), representing the two directions of the biconditional symbol ↔︎. (You can type ← in VS Code by typing \\l, short for “left”.) The first goal is a conditional statement, so we assume the antecedent. In the displays below, we’ll just show the tactic state for the first goal, since that’s all that’s relevant for figuring out our next steps.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  -- (→)\n  assume h2 : ∃ (x : U), P x\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ x, P x\n⊢ ∃ x, Q x\n\n\nAs usual, when we have an existential given, we use it right away.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  -- (→)\n  assume h2 : ∃ (x : U), P x\n  obtain (u : U) (h3 : P u) from h2\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ x, P x\nu : U\nh3 : P u\n⊢ ∃ x, Q x\n\n\nNow that we have an object of type U in the tactic state, we can use h1 by applying universal instantiation.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  -- (→)\n  assume h2 : ∃ (x : U), P x\n  obtain (u : U) (h3 : P u) from h2\n  have h4 : P u ↔ Q u := h1 u\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ x, P x\nu : U\nh3 : P u\nh4 : P u ↔ Q u\n⊢ ∃ x, Q x\n\n\nLooking at h3 and h4, we can now see that we will be able to complete the proof if we assign the value u to x in the goal. So our next step is the tactic apply Exists.intro u.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  -- (→)\n  assume h2 : ∃ (x : U), P x\n  obtain (u : U) (h3 : P u) from h2\n  have h4 : P u ↔ Q u := h1 u\n  apply Exists.intro u\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ x, P x\nu : U\nh3 : P u\nh4 : P u ↔ Q u\n⊢ Q u\n\n\nTo complete the proof, we use the left-to-right direction of h4. We have h4.ltr : P u → Q u and h3 : P u, so by modus ponens, h4.ltr h3 proves the goal Q u.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  -- (→)\n  assume h2 : ∃ (x : U), P x\n  obtain (u : U) (h3 : P u) from h2\n  have h4 : P u ↔ Q u := h1 u\n  apply Exists.intro u\n  show Q u from h4.ltr h3\n\n\ncase mpr\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ x, Q x) → ∃ x, P x\n\n\nThis completes the (→) half of the proof; the tactic state now lists only the goal for the (←) half. The second half of the proof is similar to the first. We begin by assuming h2 : ∃ (x : U), Q x, and then we use that assumption to obtain u : U and h3 : Q u.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  -- (→)\n  assume h2 : ∃ (x : U), P x\n  obtain (u : U) (h3 : P u) from h2\n  have h4 : P u ↔ Q u := h1 u\n  apply Exists.intro u\n  show Q u from h4.ltr h3\n  -- (←)\n  assume h2 : ∃ (x : U), Q x\n  obtain (u : U) (h3 : Q u) from h2\n\n\ncase mpr\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ x, Q x\nu : U\nh3 : Q u\n⊢ ∃ x, P x\n\n\nWe can actually shorten the proof by packing a lot into a single step. See if you can figure out the last line of the completed proof below; we’ll give an explanation after the proof.\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  -- (→)\n  assume h2 : ∃ (x : U), P x\n  obtain (u : U) (h3 : P u) from h2\n  have h4 : P u ↔ Q u := h1 u\n  apply Exists.intro u\n  show Q u from h4.ltr h3\n  -- (←)\n  assume h2 : ∃ (x : U), Q x\n  obtain (u : U) (h3 : Q u) from h2\n  show ∃ (x : U), P x from Exists.intro u ((h1 u).rtl h3)\nTo understand the last step, start with the fact that h1 u is a proof of P u ↔︎ Q u. Therefore (h1 u).rtl is a proof of Q u → P u, so by modus ponens, (h1 u).rtl h3 is a proof of P u. It follows that Exists.intro u ((h1 u).rtl h3) is a proof of ∃ (x : U), P x, which was the goal.\nThere is one more style of reasoning that is sometimes used in proofs of biconditional statements. It is illustrated in Example 3.4.5 of HTPI. Here is that theorem, as it is presented in HTPI.\n\nSuppose \\(A\\), \\(B\\), and \\(C\\) are sets. Then \\(A \\cap (B \\setmin C) = (A \\cap B) \\setmin C\\).\n\n\nProof. Let \\(x\\) be arbitrary. Then \\[\\begin{align*}\nx \\in A \\cap (B \\setmin C) &\\text{ iff } x \\in A \\wedge x \\in B \\setmin C\\\\\n&\\text{ iff } x \\in A \\wedge x \\in B \\wedge x \\notin C\\\\\n&\\text{ iff } x \\in (A \\cap B) \\wedge x \\notin C\\\\\n&\\text{ iff } x \\in (A \\cap B) \\setmin C.\n\\end{align*}\\] Thus, \\(\\forall x(x \\in A \\cap (B \\setmin C) \\leftrightarrow x \\in (A \\cap B) \\setmin C)\\), so \\(A \\cap (B \\setmin C) = (A \\cap B) \\setmin C\\).  □\n\nThis proof is based on a fundamental principle of set theory that says that if two sets have exactly the same elements, then they are equal. This principle is called the axiom of extensionality, and it is what justifies the inference, in the last sentence, from \\(\\forall x(x \\in A \\cap (B \\setmin C) \\leftrightarrow x \\in (A \\cap B) \\setmin C)\\) to \\(A \\cap (B \\setmin C) = (A \\cap B) \\setmin C\\).\nThe heart of the proof is a string of equivalences that, taken together, establish the biconditional statement \\(x \\in A \\cap (B \\setmin C) \\leftrightarrow x \\in (A \\cap B) \\setmin C\\). One can also use this technique to prove a biconditional statement in Lean. This time we’ll simply present the complete proof first, and then explain it afterwards.\ntheorem Example_3_4_5 (U : Type)\n    (A B C : Set U) : A ∩ (B \\ C) = (A ∩ B) \\ C := by\n  apply Set.ext\n  fix x : U\n  show x ∈ A ∩ (B \\ C) ↔ x ∈ (A ∩ B) \\ C from\n    calc\n      x ∈ A ∩ (B \\ C) ↔ x ∈ A ∧ (x ∈ B ∧ x ∉ C) := Iff.refl _\n                    _ ↔ (x ∈ A ∧ x ∈ B) ∧ x ∉ C := and_assoc.symm\n                    _ ↔ x ∈ (A ∩ B) \\ C := Iff.refl _\nThe name of the axiom of extensionality in Lean is Set.ext, and it is applied in the first step of the Lean proof. As usual, the apply tactic works backwards from the goal. In other words, after the first line of the proof, the goal is ∀ (x : U), x ∈ A ∩ (B \\ C) ↔︎ x ∈ (A ∩ B) \\ C, because by Set.ext, the conclusion of the theorem would follow from this statement. The rest of the proof then proves this goal by introducing an arbitrary x of type U and then proving the biconditional by stringing together several equivalences, exactly as in the HTPI proof.\nThe proof of the biconditional is called a calculational proof, and it is introduced by the keyword calc. The calculational proof consists of a string of biconditional statements, each of which is provided with a proof. You can think of the underscore on the left side of each biconditional after the first as standing for the right side of the previous biconditional.\nThe proofs of the individual biconditionals in the calculational proof require some explanation. Lean has a large library of theorems that it knows, and you can use those theorems in your proofs. In particular, Iff.refl and and_assoc are names of theorems in Lean’s library. You can find out what any theorem says by using the Lean command #check. (Commands that ask Lean for a response generally start with the character #.) If you type #check Iff.refl in a Lean file, you will see Lean’s response in the Infoview pane: Iff.refl : ∀ (a : Prop), a ↔︎ a. So Iff.refl is the name of the theorem ∀ (a : Prop), a ↔︎ a. (This theorem says that “iff” has a property called reflexivity; we’ll discuss reflexivity in Chapter 4.) Thus, by universal instantiation, for any proposition a, Iff.refl a is a proof of a ↔︎ a. This is used to justify the first biconditional in the calculational proof.\nBut wait! The first biconditional in the calculational proof is x ∈ A ∩ (B \\ C) ↔︎ x ∈ A ∧ (x ∈ B ∧ x ∉ C), which does not have the form a ↔︎ a. How can it be justified by the theorem Iff.refl? Recall that Lean doesn’t need to be told to write out definitions of mathematical notation; it does that automatically. When the definitions of the set theory notation are written out, the first line of the calculational proof becomes x ∈ A ∧ (x ∈ B ∧ x ∉ C) ↔︎ x ∈ A ∧ (x ∈ B ∧ x ∉ C), which does have the form a ↔︎ a, so it can be proven with the term-mode proof Iff.refl _. Note that we are using an underscore here to ask Lean to figure out what to plug in for a. This saves us the trouble of writing out the full term-mode proof, which would be Iff.refl (x ∈ A ∧ (x ∈ B ∧ x ∉ C)). The lesson of this example is that the theorem Iff.refl is more powerful than it looks. Not only can we use Iff.refl _ to prove statements of the form a ↔︎ a, we can also use it to prove statements of the form a ↔︎ a', if a and a' reduce to the same thing when definitions are filled in. We say in this case that a and a' are definitionally equal. This explains the third line of the calculational proof, which is also justified by the proof Iff.refl _.\nThe second line uses the theorem and_assoc. If you type #check and_assoc, you will get a response from Lean that looks something like this:\n\nand_assoc : (?m.4075 ∧ ?m.4076) ∧ ?m.4077 ↔ ?m.4075 ∧ ?m.4076 ∧ ?m.4077\n\nThe explanation for this cryptic response is that and_assoc has implicit arguments, and they are not displayed by default. To see the implicit arguments, you must type #check @and_assoc. Lean’s response is:\n\n@and_assoc : ∀ {a b c : Prop}, (a ∧ b) ∧ c ↔ a ∧ b ∧ c\n\nwhich is shorthand for\n\n@and_assoc : ∀ {a : Prop}, ∀ {b : Prop}, ∀ {c : Prop},\n              (a ∧ b) ∧ c ↔ a ∧ (b ∧ c)\n\n(Recall that Lean groups the logical connectives to the right, which means that it interprets a ∧ b ∧ c as a ∧ (b ∧ c).) This is the associative law for “and” (see Section 1.2 of HTPI). Since a, b, and c are implicit, Lean will recognize and_assoc as a proof of any statement of the form (a ∧ b) ∧ c ↔︎ a ∧ (b ∧ c), where a, b, and c can be replaced with any propositions. Lean doesn’t need to be told what propositions are being used as a, b, and c; it will figure that out for itself. Unfortunately, the second biconditional in the calculational proof is x ∈ A ∧ (x ∈ B ∧ x ∉ C) ↔︎ (x ∈ A ∧ x ∈ B) ∧ x ∉ C, which has the form a ∧ (b ∧ c) ↔︎ (a ∧ b) ∧ c, not (a ∧ b) ∧ c ↔︎ a ∧ (b ∧ c). (Notice that the first biconditional is the same as the second except that the left and right sides have been swapped.) To account for this discrepancy, we use the fact that if h is a proof of any biconditional P ↔︎ Q, then h.symm is a proof of Q ↔︎ P. Thus and_assoc.symm proves the second biconditional in the calculational proof. (By the way, the HTPI proof avoids any mention of the associativity of “and” by simply leaving out parentheses in the conjunction \\(x \\in A \\wedge x \\in B \\wedge x \\notin C\\). As explained in Section 1.2 of HTPI, this represents an implicit use of the associativity of “and.”)\nYou can get a better understanding of the first step of our last proof by typing #check @Set.ext. The result is\n\n@Set.ext : ∀ {α : Type u_1} {a b : Set α},\n            (∀ (x : α), x ∈ a ↔ x ∈ b) → a = b\n\nwhich is shorthand for\n\n@Set.ext : ∀ {α : Type u_1}, ∀ {a : Set α}, ∀ {b : Set α},\n            (∀ (x : α), x ∈ a ↔ x ∈ b) → a = b\n\nIgnoring the u_1, whose significance won’t be important to us, this means that Set.ext can be used to prove any statement of the form (∀ (x : α), x ∈ a ↔︎ x ∈ b) → a = b, where α can be replaced by any type and a and b can be replaced by any sets of objects of type α. Make sure you understand how this explains the effect of the tactic apply Set.ext in the first step of our last proof. Almost all of our proofs that two sets are equal will start with apply Set.ext.\nNotice that in Lean’s responses to both #check @and_assoc and #check @Set.ext, multiple universal quantifiers in a row were grouped together and written as a single universal quantifier followed by a list of variables (with types). Lean allows this notational shorthand for any sequence of consecutive quantifiers, as long as they are all of the same kind (all existential or all universal), and we will use this notation from now on.\n\n\nExercises\n\ntheorem Exercise_3_4_2 (U : Type) (A B C : Set U)\n    (h1 : A ⊆ B) (h2 : A ⊆ C) : A ⊆ B ∩ C := by\n  \n  **done::\n\n\ntheorem Exercise_3_4_4 (U : Type) (A B C : Set U)\n    (h1 : A ⊆ B) (h2 : A ⊈ C) : B ⊈ C := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_16 (U : Type) (B : Set U)\n    (F : Set (Set U)) : F ⊆ 𝒫 B → ⋃₀F ⊆ B := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_17 (U : Type) (F G : Set (Set U))\n    (h1 : ∀ (A : Set U), A ∈ F → ∀ (B : Set U), B ∈ G → A ⊆ B) :\n    ⋃₀F ⊆ ⋂₀G := by\n  \n  **done::\n\n\ntheorem Exercise_3_4_7 (U : Type) (A B : Set U) :\n    𝒫 (A ∩ B) = 𝒫 A ∩ 𝒫 B := by\n\n  **done::\n\n\ntheorem Exercise_3_4_17 (U : Type) (A : Set U) : A = ⋃₀(𝒫 A) := by\n\n  **done::\n\n\ntheorem Exercise_3_4_18a (U : Type) (F G : Set (Set U)) :\n    ⋃₀(F ∩ G) ⊆ (⋃₀F) ∩ (⋃₀G) := by\n  \n  **done::\n\n\ntheorem Exercise_3_4_19 (U : Type) (F G : Set (Set U)) :\n    (⋃₀F) ∩ (⋃₀G) ⊆ ⋃₀(F ∩ G) ↔\n      ∀ (A B : Set U), A ∈ F → B ∈ G → A ∩ B ⊆ ⋃₀(F ∩ G) := by\n  \n  **done::"
  },
  {
    "objectID": "Chap3.html#proofs-involving-disjunctions",
    "href": "Chap3.html#proofs-involving-disjunctions",
    "title": "3  Proofs",
    "section": "3.5. Proofs Involving Disjunctions",
    "text": "3.5. Proofs Involving Disjunctions\nA common proof method for dealing with givens or goals that are disjunctions is proof by cases. Here’s how it works.\n\nTo use a given of the form P ∨ Q:\n\nBreak your proof into cases. For case 1, assume that P is true and use this assumption to prove the goal. For case 2, assume that Q is true and prove the goal.\n\nIn Lean, you can break a proof into cases by using the by_cases tactic. If you have a given h : P ∨ Q, then the tactic by_cases on h will break your proof into two cases. For the first case, the given h will be changed to h : P, and for the second, it will be changed to h : Q; the goal for both cases will be the same as the original goal. Thus, the effect of the by_cases on h tactic is as follows:\n\n\n>> ⋮\nh : P ∨ Q\n⊢ goal\n\n\ncase Case_1\n>> ⋮\nh : P\n⊢ goal\ncase Case_2\n>> ⋮\nh : Q\n⊢ goal\n\n\nNotice that the original given h : P ∨ Q gets replaced by h : P in case 1 and h : Q in case 2. This is usually what is most convenient, but if you write by_cases on h with h1, then the original given h will be preserved, and new givens h1 : P and h1 : Q will be added to cases 1 and 2, respectively. If you want different names for the new givens in the two cases, then use by_cases on h with h1, h2 to add the new given h1 : P in case 1 and h2 : Q in case 2.\nYou can follow by_cases on with any proof of a disjunction, even if that proof is not just a single identifier. In that cases you will want to add with to specify the identifier or identifiers to be used for the new assumptions in the two cases. Another variant is that you can use the tactic by_cases h : P to break your proof into two cases, with the new assumptions being h : P in case 1 and h : ¬P in case 2. In other words, the effect of by_cases h : P is the same as adding the new given h : P ∨ ¬P (which, of course, is a tautology) and then using the tactic by_cases on h.\nThere are two introduction rules that you can use in Lean to prove a goal of the form P ∨ Q. If you have h : P, then Lean will accept Or.intro_left h as a proof of P ∨ Q, and if you have h : Q, then Or.intro_right h is a proof of P ∨ Q. There are shortened forms Or.inl and Or.inr for Or.intro_left and Or.intro_right, respectively.\nOften, when your goal has the form P ∨ Q, you will be unable to prove P, and also unable to prove Q. Proof by cases can help in that situation as well.\n\n\nTo prove a goal of the form P ∨ Q:\n\nBreak your proof into cases. In each case, either prove P or prove Q.\n\nExample 3.5.2 from HTPI illustrates these strategies:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n\n\nU : Type\nA B C : Set U\n⊢ A \\ (B \\ C) ⊆ A \\ B ∪ C\n\n\nThe define tactic would rewrite the goal as ∀ {a : U}, a ∈ A \\ (B \\ C) → a ∈ A \\ B ∪ C, which suggests that our next two tactics should be fix x : U and assume h1 : x ∈ A \\ (B \\ C). But as we have seen before, if you know what the result of the define tactic is going to be, then there is no need to use it. After introducing x as an arbitrary element of A \\ (B \\ C), we write out the definitions of our new given and goal to help guide our next strategy choice:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n\n\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ ¬x ∈ B \\ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nThe goal is now a disjunction, which suggests that proof by cases might be helpful. But what cases should we use? The key is to look at the meaning of the right half of the given h1. The meaning of ¬x ∈ B \\ C is ¬(x ∈ B ∧ x ∉ C), which, by one of the De Morgan laws, is equivalent to x ∉ B ∨ x ∈ C.\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : ¬x ∈ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --Now h2 : ¬x ∈ B ∨ x ∈ C\n\n\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ ¬x ∈ B \\ C\nh2 : ¬x ∈ B ∨ x ∈ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nThe new given h2 is now a disjunction, which suggests what cases we should use:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : ¬x ∈ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --Now h2 : ¬x ∈ B ∨ x ∈ C\n  by_cases on h2\n\n\ncase Case_1\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ ¬x ∈ B \\ C\nh2 : ¬x ∈ B\n⊢ x ∈ A \\ B ∨ x ∈ C\ncase Case_2\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ ¬x ∈ B \\ C\nh2 : x ∈ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nLooking at the givens h1 and h2 in both cases, it is not hard to see that we should be able to prove x ∈ A \\ B in case 1 and x ∈ C in case 2. Thus, in case 1 we will be able to give a proof of the goal that has the form Or.inl _, where the blank will be filled in with a proof of x ∈ A \\ B, and in case 2 we can use Or.inr _, filling in the blank with a proof of x ∈ C. This suggests that we should use the tactics apply Or.inl in case 1 and apply Or.inr in case 2. Focusing first on case 1, we get:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : ¬x ∈ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --Now h2 : ¬x ∈ B ∨ x ∈ C\n  by_cases on h2\n  -- Case 1\n  apply Or.inl\n\n\ncase Case_1.h\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ ¬x ∈ B \\ C\nh2 : ¬x ∈ B\n⊢ x ∈ A \\ B\n\n\nNotice that the tactic apply Or.inl has changed the goal for case 1 to the left half of the original goal, x ∈ A \\ B. Since this means x ∈ A ∧ x ∉ B, we can complete case 1 by combining h1.left with h2.\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : ¬x ∈ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --Now h2 : ¬x ∈ B ∨ x ∈ C\n  by_cases on h2\n  -- Case 1\n  apply Or.inl\n  show x ∈ A \\ B from And.intro h1.left h2\n\n\ncase Case_2\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ ¬x ∈ B \\ C\nh2 : x ∈ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nCase 2 is similar, using Or.inr and h2\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : ¬x ∈ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --Now h2 : ¬x ∈ B ∨ x ∈ C\n  by_cases on h2\n  -- Case 1\n  apply Or.inl\n  show x ∈ A \\ B from And.intro h1.left h2\n  -- Case 2\n  apply Or.inr\n  show x ∈ C from h2\n\n\n>>Goals accomplished 🎉\n\n\nThere is a second strategy that is often useful to prove a goal of the form P ∨ Q. It is motivated by the fact that P ∨ Q is equivalent to both ¬P → Q and ¬Q → P.\n\n\nTo prove a goal of the form P ∨ Q:\n\nAssume that P is false and prove Q, or assume that Q is false and prove P.\n\nIf your goal is P ∨ Q, then the Lean tactic or_left with h will add the new given h : ¬Q to the tactic state and set the goal to be P, and or_right with h will add h : ¬P to the tactic state and set the goal to be Q. For example, here is the effect of the tactic or_left with h:\n\n\n>> ⋮\n⊢ P ∨ Q\n\n\n>> ⋮\nh : ¬Q\n⊢ P\n\n\nNotice that or_left and or_right have the same effect as apply Or.inl and apply Or.inr, except that each adds a new given to the tactic state. Sometimes you can tell in advance that you won’t need the extra given, and in that case the tactics apply Or.inl and apply Or.inr can be useful. For example, that was the case in the example above. But if you think the extra given might be useful, you are better off using or_left or or_right. Here’s an example illustrating this.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n\n\nU : Type\nA B C : Set U\nh1 : A \\ B ⊆ C\n⊢ A ⊆ B ∪ C\n\n\nOf course, we begin by letting x be an arbitrary element of A. Writing out the meaning of the new goal shows that it is a disjunction.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n  fix x : U\n  assume h2 : x ∈ A\n  define\n\n\nU : Type\nA B C : Set U\nh1 : A \\ B ⊆ C\nx : U\nh2 : x ∈ A\n⊢ x ∈ B ∨ x ∈ C\n\n\nLooking at the givens h1 and h2, we see that if we assume x ∉ B, then we should be able to prove x ∈ C. This suggests that we should use the or_right tactic.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n  fix x : U\n  assume h2 : x ∈ A\n  define\n  or_right with h3\n\n\nU : Type\nA B C : Set U\nh1 : A \\ B ⊆ C\nx : U\nh2 : x ∈ A\nh3 : ¬x ∈ B\n⊢ x ∈ C\n\n\nWe can now complete the proof. h1 _ will be a proof of the goal x ∈ C, if we can fill in the blank with a proof of x ∈ A \\ B. Since x ∈ A \\ B means x ∈ A ∧ ¬x ∈ B, we can prove it with the expression And.intro h2 h3.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n  fix x : U\n  assume h2 : x ∈ A\n  define\n  or_right with h3\n  show x ∈ C from h1 (And.intro h2 h3)\n\n\n>>Goals accomplished 🎉\n\n\nThe fact that P ∨ Q is equivalent to both ¬P → Q and ¬Q → P also suggests another strategy for using a given that is a disjunction.\n\n\nTo use a given of the form P ∨ Q:\n\nIf you are also given ¬P, or you can prove that P is false, then you can use this given to conclude that Q is true. Similarly, if you are given ¬Q or can prove that Q is false, then you can conclude that P is true.\n\nThis strategy is a rule of inference called disjunctive syllogism, and the tactic for using this strategy in Lean is called disj_syll. If you have h1 : P ∨ Q and h2 : ¬P, then the tactic disj_syll h1 h2 will change h1 to h1 : Q; if instead you have h2 : ¬Q, then disj_syll h1 h2 will change h1 to h1 : P. Notice that, as with the by_cases tactic, the given h1 gets replaced with the conclusion of the rule. The tactic disj_syll h1 h2 with h3 will preserve the original h1 and introduce the conclusion as a new given with the identifier h3. Also, as with the by_cases tactic, either h1 or h2 can be a complex proof rather than simply an identifier (although in that case it must be enclosed in parentheses, so that Lean can tell where h1 ends and h2 begins). The only requirement is that h1 must be a proof of a disjunction, and h2 must be a proof of the negation of one side of the disjunction. If h1 is not simply an identifier, then you will want to use with to specify the identifier to be used for the conclusion of the rule.\nHere’s an example illustrating the use of the disjunctive syllogism rule.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ x, x ∈ A ∩ B\n⊢ A ⊆ C\n\n\nOf course, we begin by introducing an arbitrary element of A. We also rewrite h2 as an equivalent positive statement.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  quant_neg at h2\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  ¬x ∈ A ∩ B\na : U\nh3 : a ∈ A\n⊢ a ∈ C\n\n\nWe can now make two inferences by combining h1 with h3 and by applying h2 to a. To see how to use the inferred statements, we write out their definitions, and since one of them is a negative statement, we reexpress it as an equivalent positive statement.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  quant_neg at h2\n  have h4 : a ∈ B ∪ C := h1 h3\n  have h5 : a ∉ A ∩ B := h2 a\n  define at h4\n  define at h5; demorgan at h5\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  ¬x ∈ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\nh5 : ¬a ∈ A ∨ ¬a ∈ B\n⊢ a ∈ C\n\n\nBoth h4 and h5 are disjunctions, and looking at h3 we see that the disjunctive syllogism rule can be applied. From h3 and h5 we can draw the conclusion ¬a ∈ B, and then combining that conclusion with h4 we can infer a ∈ C. Since that is the goal, we are done.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  quant_neg at h2\n  have h4 : a ∈ B ∪ C := h1 h3\n  have h5 : a ∉ A ∩ B := h2 a\n  define at h4\n  define at h5; demorgan at h5\n  disj_syll h5 h3  --Now h5 : ¬a ∈ B\n  disj_syll h4 h5  --Now h4 : a ∈ C\n  show a ∈ C from h4\n\n\n>>Goals accomplished 🎉\n\n\nWe’re going to redo the last example, to illustrate another useful technique in Lean. We start with some of the same steps as before.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ x, x ∈ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\n⊢ a ∈ C\n\n\nAt this point, you might see a possible route to the goal: from h2 and h3 we should be able to prove that a ∉ B, and then, combining that with h4 by the disjunctive syllogism rule, we should be able to deduce the goal a ∈ C. Let’s try writing the proof that way.\n\n\n??example::\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := sorry\n  disj_syll h4 h5  --Now h4 : a ∈ C\n  show a ∈ C from h4\n\n\n>>Goals accomplished 🎉\n\n\nWe have introduced a new idea in this proof. The justification we have given for introducing h5 : a ∉ B is sorry. You might think of this as meaning “Sorry, I’m not going to give a justification for this statement, but please accept it anyway.” Of course, this is cheating; in a complete proof, every step must be justified. Lean accepts sorry as a proof of any statement, but it displays it in red to warn you that you’re cheating. It also puts a brown squiggle under the keyword example and it puts the message declaration uses 'sorry' in the Infoview, to warn you that, although the proof has reached the goal, it is not fully justified.\nAlthough writing the proof this way is cheating, it is a convenient way to see that our plan of attack for this proof is reasonable. Lean has accepted the proof, except for the warning that we have used sorry. So now we know that if we go back and replace sorry with a proof of a ∉ B, then we will have a complete proof.\nThe proof of a ∉ B is hard enough that it is easier to do it in tactic mode rather than term mode. So we will begin the proof as we always do for tactic-mode proofs: we replace sorry with by, leave a blank line, and then put done, indented further than the surrounding text. When we put the cursor on the blank line before done, we see the tactic state for our “proof within a proof.”\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := by\n\n    **done::\n  disj_syll h4 h5  --Now h4 : a ∈ C\n  show a ∈ C from h4\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ x, x ∈ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\n⊢ ¬a ∈ B\n\n\nNote that h5 : a ∉ B is not a given in the tactic state, because we have not yet justified it; in fact, ¬a ∈ B is the goal. This goal is a negative statement, and h2 is also negative. This suggests that we could try using proof by contradiction, achieving the contradiction by contradicting h2. So we use the tactic contradict h2 with h6.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := by\n    contradict h2 with h6\n    **done::\n  disj_syll h4 h5  --Now h4 : a ∈ C\n  show a ∈ C from h4\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ x, x ∈ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\nh6 : a ∈ B\n⊢ ∃ x, x ∈ A ∩ B\n\n\nLooking at h3 and h6, we see that the right value to plug in for x in the goal is a. In fact, Exists.intro a _ will prove the goal, if we can fill in the blank with a proof of a ∈ A ∩ B. Since this means a ∈ A ∧ a ∈ B, we can prove it with And.intro h3 h6. Thus, we can complete the proof in one more step:\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := by\n    contradict h2 with h6\n    show ∃ (x : U), x ∈ A ∩ B from\n      Exists.intro a (And.intro h3 h6)\n    done\n  disj_syll h4 h5  --Now h4 : a ∈ C\n  show a ∈ C from h4\n\n\n>>Goals accomplished 🎉\n\n\nThe red squiggle has disappeared from the word done, indicating that the proof is complete.\nIt was not really necessary for us to use sorry when writing this proof. We could have simply written the steps in order, exactly as they appear above. Any time you use the have tactic with a conclusion that is difficult to justify, you have a choice. You can establish the have with sorry, complete the proof, and then return and fill in a justification for the have, as we did in the example above. Or, you can justify the have right away by typing by after := and then plunging into the “proof within in a proof.” Once you complete the inner proof, you can continue with the original proof.\nAnd in case you were wondering: yes, if the inner proof uses the have tactic with a statement that is hard to justify, then you can write a “proof within a proof within a proof”!\n\n\nExercises\nIn each case, replace sorry with a proof.\n\ntheorem Exercise_3_5_2 (U : Type) (A B C : Set U) :\n    (A ∪ B) \\ C ⊆ A ∪ (B \\ C) := sorry\n\n\ntheorem Exercise_3_5_5 (U : Type) (A B C : Set U)\n    (h1 : A ∩ C ⊆ B ∩ C) (h2 : A ∪ C ⊆ B ∪ C) : A ⊆ B := sorry\n\n\ntheorem Exercise_3_5_7 (U : Type) (A B C : Set U) :\n    A ∪ C ⊆ B ∪ C ↔ A \\ C ⊆ B \\ C := sorry\n\n\ntheorem Exercise_3_5_8 (U : Type) (A B : Set U) :\n    𝒫 A ∪ 𝒫 B ⊆ 𝒫 (A ∪ B) := sorry\n\n\ntheorem Exercise_3_5_17b (U : Type) (F : Set (Set U)) (B : Set U) :\n    B ∪ (⋂₀ F) =\n      ⋂₀ { X : Set U | ∃ (A : Set U), A ∈ F ∧ X = B ∪ A} := sorry\n\n\ntheorem Exercise_3_5_18 (U : Type) (F G H : Set (Set U))\n    (h1 : ∀ (A : Set U), A ∈ F → ∀ (B : Set U), B ∈ G → A ∪ B ∈ H) :\n    ⋂₀H ⊆ (⋂₀F) ∪ (⋂₀G) := sorry\n\n\ntheorem Exercise_3_5_24a (U : Type) (A B C : Set U) :\n    (A ∪ B) △ C ⊆ (A △ C) ∪ (B △ C) := sorry"
  },
  {
    "objectID": "Chap3.html#existence-and-uniqueness-proofs",
    "href": "Chap3.html#existence-and-uniqueness-proofs",
    "title": "3  Proofs",
    "section": "3.6. Existence and Uniqueness Proofs",
    "text": "3.6. Existence and Uniqueness Proofs\nRecall that ∃! (x : U), P x means that there is exactly one x of type U such that P x is true. One way to deal with a given or goal of this form is to use the define tactic to rewrite it as the equivalent statement ∃ (x : U), P x ∧ ∀ (x_1 : U), P x_1 → x_1 = x. You can then apply techniques discussed previously in this chapter. However, there are also proof techniques, and corresponding Lean tactics, for working directly with givens and goals of this form.\nOften a goal of the form ∃! (x : U), P x is proven by using the following strategy. This is a slight rephrasing of the strategy presented in HTPI. The rephrasing is based on the fact that for any propositions A, B, and C, A ∧ B → C is equivalent to A → B → C (you can check this equivalence by making a truth table). The second of these statements is usually easier to work with in Lean than the first one, so we will often rephrase statements that have the form A ∧ B → C as A → B → C. That’s what we have done here, as you will see if you compare the strategy stated below to the one in HTPI.\n\nTo prove a goal of the form ∃! (x : U), P x:\n\nProve ∃ (x : U), P x and ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x_2. The first of these goals says that there exists an x such that P x is true, and the second says that it is unique. The two parts of the proof are therefore sometimes labeled existence and uniqueness.\n\nTo apply this strategy in a Lean proof, we use the tactic exists_unique. We’ll illustrate this with the theorem from Example 3.6.2 in HTPI. Here’s how that theorem and its proof are presented in HTPI:\n\nThere is a unique set \\(A\\) such that for every set \\(B\\), \\(A \\cup B = B\\).\n\n\nProof. Existence: Clearly \\(\\forall B(\\varnothing \\cup B = B)\\), so \\(\\varnothing\\) has the required property.\nUniqueness: Suppose \\(\\forall B(C \\cup B = B)\\) and \\(\\forall B(D \\cup B = B)\\). Applying the first of these assumptions to \\(D\\) we see that \\(C \\cup D = D\\), and applying the second to \\(C\\) we get \\(D \\cup C = C\\). But clearly \\(C \\cup D = D \\cup C\\), so \\(C = D\\).  □\n\nYou will notice that there are two statements in this proof that are described as “clearly” true. This brings up one of the difficulties with proving theorems in Lean: things that are clear to us are not necessarily clear to Lean! There are two ways to deal with such “clear” statements. The first is to see if the statement is in the library of theorems that Lean knows. The second is to prove the statement as a preliminary theorem that can then be used in the proof of our main theorem. We’ll take the second approach here, since proving these “clear” facts will give us more practice with Lean proofs, but later we’ll have more to say about searching for statements in Lean’s theorem library.\nThe first theorem we need says that for every set B, ∅ ∪ B = B, and it brings up a subtle issue: in Lean, the symbol ∅ is ambiguous! The reason for this is Lean’s strict typing rules. For each type U, there is an empty set of type Set U. There is, for example, the set of type Set Nat that contains no natural numbers, and also the set of type Set Real that contains no real numbers. To Lean, these are different sets, because they have different types. Which one does the symbol ∅ denote? The answer will be different in different contexts. Lean can sometimes figure out from context which empty set you have in mind, but if it can’t, then you have to tell it explicitly by writing (∅ : Set U) rather than ∅.\nWith that preparation, we are ready to prove our first preliminary theorem. To avoid problems with ambiguity, we’ll tell Lean explicitly which empty set we have in mind in this theorem. Since the goal is an equation between sets, our first step is to use the tactic apply Set.ext.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n\n\ncase h\nU : Type\nB : Set U\n⊢ ∀ (x : U),\n>>  x ∈ ∅ ∪ B ↔ x ∈ B\n\n\nBased on the form of the goal, our next two tactics should be fix x : U and apply Iff.intro. This leaves us with two goals, corresponding to the two directions of the biconditional, but we’ll focus first on just the left-to-right direction.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n\n\ncase h.mp\nU : Type\nB : Set U\nx : U\n⊢ x ∈ ∅ ∪ B → x ∈ B\n\n\nOf course, our next step is to assume x ∈ (∅ : Set U) ∪ B. To help us see how to move forward, we also write out the definition of this assumption.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  -- (→)\n  assume h1 : x ∈ (∅ : Set U) ∪ B\n  define at h1\n\n\ncase h.mp\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\n⊢ x ∈ B\n\n\nNow you should see a way to complete the proof: the statement x ∈ ∅ is false, so we should be able to apply the disjunctive syllogism rule to h1 to infer the goal x ∈ B. To carry out this plan, we’ll first have to prove x ∉ (∅ : Set U). We’ll use the have tactic, and since there’s no obvious term-mode proof to justify it, we’ll try a tactic-mode proof.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  -- (→)\n  assume h1 : x ∈ (∅ : Set U) ∪ B\n  define at h1\n  have h2 : x ∉ (∅ : Set U) := by\n\n    **done::\n\n\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\n⊢ ¬x ∈ ∅\n\n\nThe goal for our “proof within a proof” is a negative statement, so proof by contradiction seems like a good start.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  -- (→)\n  assume h1 : x ∈ (∅ : Set U) ∪ B\n  define at h1\n  have h2 : x ∉ (∅ : Set U) := by\n    by_contra h3\n    **done::\n\n\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\nh3 : x ∈ ∅\n⊢ False\n\n\nTo see how to use the new assumption h3, we use the tactic define at h3. The definition Lean gives for the statement x ∈ ∅ is False. In other words, Lean knows that, by the definition of ∅, the statement x ∈ ∅ is false. Since False is our goal, this completes the inner proof, and we can return to the main proof.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  -- (→)\n  assume h1 : x ∈ (∅ : Set U) ∪ B\n  define at h1\n  have h2 : x ∉ (∅ : Set U) := by\n    by_contra h3\n    define at h3  --Now h3 : False\n    show False from h3\n    done\n\n\ncase h.mp\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\nh2 : ¬x ∈ ∅\n⊢ x ∈ B\n\n\nNow that we have established the claim h2 : ¬x ∈ ∅, we can apply the disjunctive syllogism rule to h1 and h2 to reach the goal. This completes the left-to-right direction of the biconditional proof, so we move on to the right-to-left direction.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  -- (→)\n  assume h1 : x ∈ (∅ : Set U) ∪ B\n  define at h1\n  have h2 : x ∉ (∅ : Set U) := by\n    by_contra h3\n    define at h3  --Now h3 : False\n    show False from h3\n    done\n  disj_syll h1 h2  --Now h1 : x ∈ B\n  show x ∈ B from h1\n  -- (←)\n\n\ncase h.mpr\nU : Type\nB : Set U\nx : U\n⊢ x ∈ B → x ∈ ∅ ∪ B\n\n\nThis direction of the biconditional proof is easier: once we introduce the assumption h1 : x ∈ B, our goal will be x ∈ ∅ ∪ B, which means x ∈ ∅ ∨ x ∈ B, and we can prove it with the proof Or.inr h1.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    (∅ : Set U) ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  -- (→)\n  assume h1 : x ∈ (∅ : Set U) ∪ B\n  define at h1\n  have h2 : x ∉ (∅ : Set U) := by\n    by_contra h3\n    define at h3  --Now h3 : False\n    show False from h3\n    done\n  disj_syll h1 h2  --Now h1 : x ∈ B\n  show x ∈ B from h1\n  -- (←)\n  assume h1 : x ∈ B\n  show x ∈ (∅ : Set U) ∪ B from Or.inr h1\n\n\n>>Goals accomplished 🎉\n\n\nThe second fact that was called “clear” in the proof from Example 3.6.2 was the equation C ∪ D = D ∪ C. This looks like an instance of the commutativity of the union operator. Let’s prove that union is commutative.\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n\n\nU : Type\nX Y : Set U\n⊢ X ∪ Y = Y ∪ X\n\n\nOnce again, we begin with apply Set.ext, which converts the goal to ∀ (x : U), x ∈ X ∪ Y ↔︎ x ∈ Y ∪ X, and then fix x : U.\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n\n\ncase h\nU : Type\nX Y : Set U\nx : U\n⊢ x ∈ X ∪ Y ↔ x ∈ Y ∪ X\n\n\nTo understand the goal better, we’ll write out the definitions of the two sides of the biconditional. We use an extension of the define tactic that allows us to write out the definition of just a part of a given or the goal. The tactic define : x ∈ X ∪ Y will replace x ∈ X ∪ Y with its definition wherever it appears in the goal, and then define : x ∈ Y ∪ X will replace x ∈ Y ∪ X with its definition. (As usual, you can add at to do the replacements in a given rather than the goal.)\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n\n\ncase h\nU : Type\nX Y : Set U\nx : U\n⊢ x ∈ X ∨ x ∈ Y ↔\n>>  x ∈ Y ∨ x ∈ X\n\n\nBy the way, there are similar extensions of all of the tactics contrapos, demorgan, conditional, double_neg, bicond_neg, and quant_neg that allow you to use a logical equivalence to rewrite just a part of a formula. For example, if your goal is P ∧ (¬Q → R), then the tactic contrapos : ¬Q → R will change the goal to P ∧ (¬R → Q). If you have a given h : P → ¬∀ (x : U), Q x, then the tactic quant_neg : ¬∀ (x : U), Q x at h will change h to h : P → ∃ (x : U), ¬Q x.\nReturning to our proof of union_comm: the goal is now x ∈ X ∨ x ∈ Y ↔︎ x ∈ Y ∨ x ∈ X. You could prove this by a somewhat tedious application of the rules for biconditionals and disjunctions that were discussed in the last two sections, and we invite you to try it. But there is another possibility. The goal now has the form P ∨ Q ↔︎ Q ∨ P, which is the commutative law for “or” (see Section 1.2 of HTPI). We saw in a previous example that Lean has, in its library, the associative law for “and”; it is called and_assoc. Does Lean also know the commutative law for “or”?\nTry typing #check @or_ in VS Code. As soon as you type this, a pop-up window appears with possible completions of this command. You will see or_assoc on the list, as well as or_comm. Select or_comm, and you’ll get this response: @or_comm : ∀ {a b : Prop}, a ∨ b ↔︎ b ∨ a. Since a and b are implicit arguments in this theorem, you can use or_comm to prove any statement of the form a ∨ b ↔︎ b ∨ a, where Lean will figure out for itself what a and b stand for. In particular, or_comm will prove our current goal.\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n  show x ∈ X ∨ x ∈ Y ↔ x ∈ Y ∨ x ∈ X from or_comm\n\n\n>>Goals accomplished 🎉\n\n\nWe have now proven the two statements that were said to be “clearly” true in the proof in Example 3.6.2 of HTPI, and we have given them names. We can now use these theorems, in the file containing these proofs, to prove other theorems. As with any theorem in Lean’s library, you can use the #check command to confirm what these theorems say. If you type #check @empty_union and #check @union_comm, you will get these results:\n\n@empty_union : ∀ {U : Type} (B : Set U), ∅ ∪ B = B @union_comm : ∀ {U : Type} (X Y : Set U), X ∪ Y = Y ∪ X\n\nNotice that Lean has applied a universal quantifier to the variables that were declared at the beginnings of the two theorems. When you declare variables in the statement of a theorem, it is understood that they can stand for anything of the appropriate type, so Lean treats them as implicitly universally quantified (see Section 3.1 of HTPI). Also, we used curly braces when we introduced the type U, so it is an implicit argument in both theorems and will not need to be specified when we apply the theorems.\nWe are finally ready to prove the theorem from Example 3.6.2. Here is the theorem:\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n\n\nU : Type\n⊢ ∃! A, ∀ (B : Set U),\n>>  A ∪ B = B\n\n\nThe goal starts with ∃!, so we use our new tactic, exists_unique.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n\n\ncase Existence\nU : Type\n⊢ ∃ A, ∀ (B : Set U),\n>>  A ∪ B = B\ncase Uniqueness\nU : Type\n⊢ ∀ (A_1 A_2 : Set U),\n>>  (∀ (B : Set U),\n>>      A_1 ∪ B = B) →\n>>  (∀ (B : Set U),\n>>      A_2 ∪ B = B) →\n>>  A_1 = A_2\n\n\nWe have two goals, labeled Existence and Uniqueness. Imitating the proof from HTPI, we prove existence by using the value ∅ for A. It turns out that in this context, Lean is able to figure out which empty set we mean by ∅, so we don’t need to specify the type.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  -- Existence\n  apply Exists.intro ∅\n\n\ncase Existence\nU : Type\n⊢ ∀ (B : Set U),\n>>  ∅ ∪ B = B\n\n\nThe goal is now precisely the statement of the theorem empty_union, so we can prove it by simply citing that theorem.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  -- Existence\n  apply Exists.intro ∅\n  show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n  -- Uniqueness\n\n\ncase Uniqueness\nU : Type\n⊢ ∀ (A_1 A_2 : Set U),\n>>  (∀ (B : Set U),\n>>      A_1 ∪ B = B) →\n>>  (∀ (B : Set U),\n>>      A_2 ∪ B = B) →\n>>  A_1 = A_2\n\n\nFor the uniqueness proof, we begin by introducing arbitrary sets C and D and assuming ∀ (B : Set U), C ∪ B = B and ∀ (B : Set U), D ∪ B = B, exactly as in the HTPI proof.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  -- Existence\n  apply Exists.intro ∅\n  show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n  -- Uniqueness\n  fix C : Set U; fix D : Set U\n  assume h1 : ∀ (B : Set U), C ∪ B = B\n  assume h2 : ∀ (B : Set U), D ∪ B = B\n\n\ncase Uniqueness\nU : Type\nC D : Set U\nh1 : ∀ (B : Set U),\n>>  C ∪ B = B\nh2 : ∀ (B : Set U),\n>>  D ∪ B = B\n⊢ C = D\n\n\nThe next step in HTPI was to apply h1 to D, and h2 to C. We do the same thing in Lean.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  -- Existence\n  apply Exists.intro ∅\n  show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n  -- Uniqueness\n  fix C : Set U; fix D : Set U\n  assume h1 : ∀ (B : Set U), C ∪ B = B\n  assume h2 : ∀ (B : Set U), D ∪ B = B\n  have h3 : C ∪ D = D := h1 D\n  have h4 : D ∪ C = C := h2 C\n\n\ncase Uniqueness\nU : Type\nC D : Set U\nh1 : ∀ (B : Set U),\n>>  C ∪ B = B\nh2 : ∀ (B : Set U),\n>>  D ∪ B = B\nh3 : C ∪ D = D\nh4 : D ∪ C = C\n⊢ C = D\n\n\nThe goal can now be achieved by stringing together a sequence of equations: C = D ∪ C = C ∪ D = D. The first of these equations is h4.symm—that is, h4 read backwards; the second follows from the commutative law for union; and the third is h3. We saw in Section 3.4 that you can prove a biconditional statement in Lean by stringing together a sequence of biconditionals in a calculational proof. Exactly the same method applies to equations. Here is the complete proof of the theorem:\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  -- Existence\n  apply Exists.intro ∅\n  show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n  -- Uniqueness\n  fix C : Set U; fix D : Set U\n  assume h1 : ∀ (B : Set U), C ∪ B = B\n  assume h2 : ∀ (B : Set U), D ∪ B = B\n  have h3 : C ∪ D = D := h1 D\n  have h4 : D ∪ C = C := h2 C\n  show C = D from\n    calc\n      C = D ∪ C := h4.symm\n      _ = C ∪ D := union_comm D C\n      _ = D     := h3\nSince the statement ∃! (x : U), P x asserts both the existence and the uniqueness of an object satisfying the predicate P, we have the following strategy for using a given of this form:\n\n\nTo use a given of the form ∃! (x : U), P x:\n\nIntroduce a new variable, say a, into the proof to stand for an object of type U for which P a is true. You may also assert that ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x2.\n\nIf you have a given h : ∃! (x : U), P x, then the tactic obtain (a : U) (h1 : P a) (h2 : ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x_2) from h will introduce into the tactic state a new variable a of type U and new givens (h1 : P a) and (h2 : ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x_2). To illustrate the use of this tactic, let’s prove the theorem in Example 3.6.4 of HTPI.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n\n\nU : Type\nA B C : Set U\nh1 : ∃ x, x ∈ A ∩ B\nh2 : ∃ x, x ∈ A ∩ C\nh3 : ∃! x, x ∈ A\n⊢ ∃ x, x ∈ B ∩ C\n\n\nWe begin by applying the obtain tactic to h1, h2, and h3. In the case of h3, we get an extra given asserting the uniqueness of the element of A. We also write out the definitions of two of the new givens we obtain.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n\n\nU : Type\nA B C : Set U\nh1 : ∃ x, x ∈ A ∩ B\nh2 : ∃ x, x ∈ A ∩ C\nh3 : ∃! x, x ∈ A\nb : U\nh4 : b ∈ A ∧ b ∈ B\nc : U\nh5 : c ∈ A ∧ c ∈ C\na : U\nh6 : a ∈ A\nh7 : ∀ (y z : U),\n>>  y ∈ A → z ∈ A → y = z\n⊢ ∃ x, x ∈ B ∩ C\n\n\nThe key to the rest of the proof is the observation that, by the uniqueness of the element of A, b must be equal to c. To justify this conclusion, note that by two applications of universal instantiation, h7 b c is a proof of b ∈ A → c ∈ A → b = c, and therefore by two applications of modus ponens, h7 b c h4.left h5.left is a proof of b = c.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n  have h8 : b = c := h7 b c h4.left h5.left\n\n\nU : Type\nA B C : Set U\nh1 : ∃ x, x ∈ A ∩ B\nh2 : ∃ x, x ∈ A ∩ C\nh3 : ∃! x, x ∈ A\nb : U\nh4 : b ∈ A ∧ b ∈ B\nc : U\nh5 : c ∈ A ∧ c ∈ C\na : U\nh6 : a ∈ A\nh7 : ∀ (y z : U),\n>>  y ∈ A → z ∈ A → y = z\nh8 : b = c\n⊢ ∃ x, x ∈ B ∩ C\n\n\nFor our next step, we will need a new tactic. Since we have h8 : b = c, we should be able to replace b with c anywhere it appears. The tactic that allows us to do this called rewrite. If h is a proof of any equation s = t, then rewrite [h] will replace all occurrences of s in the goal with t. Notice that it is the left side of the equation that is replaced with the right side; if you want the replacement to go in the other direction, so that t is replaced with s, you can use rewrite [←h]. (Alternatively, since h.symm is a proof of t = s, you can use rewrite [h.symm].) You can also apply the rewrite tactic to biconditional statements. If you have h : P ↔︎ Q, then rewrite [h] will cause all occurrences of P in the goal to be replaced with Q (and rewrite [←h] will replace Q with P).\nAs with many other tactics, you can add at h' to specify that the replacement should be done in the given h' rather than the goal. In our case, rewrite [h8] at h4 will change both occurrences of b in h4 to c.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n  have h8 : b = c := h7 b c h4.left h5.left\n  rewrite [h8] at h4\n\n\nU : Type\nA B C : Set U\nh1 : ∃ x, x ∈ A ∩ B\nh2 : ∃ x, x ∈ A ∩ C\nh3 : ∃! x, x ∈ A\nb c : U\nh4 : c ∈ A ∧ c ∈ B\nh5 : c ∈ A ∧ c ∈ C\na : U\nh6 : a ∈ A\nh7 : ∀ (y z : U),\n>>  y ∈ A → z ∈ A → y = z\nh8 : b = c\n⊢ ∃ x, x ∈ B ∩ C\n\n\nNow the right sides of h4 and h5 tell us that we can prove the goal by plugging in c for x. Here is the complete proof:\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n  have h8 : b = c := h7 b c h4.left h5.left\n  rewrite [h8] at h4\n  show ∃ (x : U), x ∈ B ∩ C from\n    Exists.intro c (And.intro h4.right h5.right)\nYou might want to compare the Lean proof above to the proof of this theorem as it appears in HTPI:\n\nSuppose \\(A\\), \\(B\\), and \\(C\\) are sets, \\(A\\) and \\(B\\) are not disjoint, \\(A\\) and \\(C\\) are not disjoint, and \\(A\\) has exactly one element. Then \\(B\\) and \\(C\\) are not disjoint\n\n\nProof. Since \\(A\\) and \\(B\\) are not disjoint, we can let \\(b\\) be something such that \\(b \\in A\\) and \\(b \\in B\\). Similarly, since \\(A\\) and \\(C\\) are not disjoint, there is some object \\(c\\) such that \\(c \\in A\\) and \\(c \\in C\\). Since \\(A\\) has only one element, we must have \\(b = c\\). Thus \\(b = c \\in B \\cap C\\) and therefore \\(B\\) and \\(C\\) are not disjoint.  □\n\nBefore ending this section, we return to the question of how you can tell if a theorem you want to use is in Lean’s library. In an earlier example, we guessed that the commutative law for “or” might be in Lean’s library, and we were then able to use the #check command to confirm it. But there is another technique that we could have used: the tactic library_search. Let’s return to our proof of the theorem union_comm, which started like this:\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n\n\ncase h\nU : Type\nX Y : Set U\nx : U\n⊢ x ∈ X ∨ x ∈ Y ↔\n>>  x ∈ Y ∨ x ∈ X\n\n\nNow let’s give the library_search tactic a try.\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n  ##library_search::\nThe blue squiggle indicates that the library_search tactic has produced an answer, which you will find in the Infoview pane: exact or_comm' (x ∈ X) (x ∈ Y). The word exact is the name of a tactic that we have not discussed; it is a shorthand for show _ from, where the blank gets filled in with the goal. Thus, you can think of library_search’s answer as a shortened form of the tactic\n\nshow x ∈ X ∨ x ∈ Y ↔︎ x ∈ Y ∨ x ∈ X from or_comm' (x ∈ X) (x ∈ Y).\n\nUsually your proof will be more readable if you use the show tactic to state explicitly the goal that is being proven. This also gives Lean a chance to correct you if you have become confused about what goal you are proving. But sometimes—for example, if the goal is very long—it is convenient to use the exact tactic instead. You might think of exact as meaning “the following is a term-mode proof that is exactly what is needed to prove the goal.”\nThe library_search tactic has not only come up with a suggested tactic, it has applied that tactic, and the proof is now complete. You can confirm that the tactic completes the proof by replacing the line library_search in the proof with either library_search’s suggested exact tactic or the show tactic displayed above.\nBut what is or_comm'? The command #check or_comm' gives the result\n\nor_comm' : ∀ (a b : Prop), a ∨ b ↔︎ b ∨ a.\n\nSo or_comm' is the same as or_comm, except that the arguments a and b are not implicit; that’s why library_search had to specify values for those arguments in its answer. The #check command also produces a suggestion that we should use or_comm instead.\nThe library_search tactic is somewhat unpredictable; sometimes it is able to find the right theorem in the library, and sometimes it isn’t. But it is always worth a try. Other tactics that may be helpful are suggest and hint.\n\n\nExercises\n\ntheorem Exercise_3_4_15 (U : Type) (B : Set U) (F : Set (Set U)) :\n    ⋃₀{X : Set U | ∃ (A : Set U), A ∈ F ∧ X = A \\ B}\n      ⊆ ⋃₀(F \\ 𝒫 B) := sorry\n\n\ntheorem Exercise_3_5_9 (U : Type) (A B : Set U)\n    (h1 : 𝒫 (A ∪ B) = 𝒫 A ∪ 𝒫 B) : A ⊆ B ∨ B ⊆ A := by\n  --Hint:  Start like this:\n  have h2 : A ∪ B ∈ 𝒫 (A ∪ B) := sorry\n  \n  **done::\n\n\ntheorem Exercise_3_6_6b (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U), A ∪ B = A := sorry\n\n\ntheorem Exercise_3_6_7b (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U), A ∩ B = A := sorry\n\n\ntheorem Exercise_3_6_8a (U : Type) : ∀ (A : Set U),\n    ∃! (B : Set U), ∀ (C : Set U), C \\ A = C ∩ B := sorry\n\n\ntheorem Exercise_3_6_10 (U : Type) (A : Set U)\n    (h1 : ∀ (F : Set (Set U)), ⋃₀F = A → A ∈ F) :\n    ∃! (x : U), x ∈ A := by\n  --Hint:  Start like this:\n  let F0 := {X : Set U | X ⊆ A ∧ ∃! (x : U), x ∈ X}\n  --Now F0 is in the tactic state, with the definition above\n  have h2 : ⋃₀F0 = A := sorry\n  \n  **done::"
  },
  {
    "objectID": "Chap3.html#more-examples-of-proofs",
    "href": "Chap3.html#more-examples-of-proofs",
    "title": "3  Proofs",
    "section": "3.7. More Examples of Proofs",
    "text": "3.7. More Examples of Proofs\nIt is finally time to discuss proofs involving algebraic reasoning. Lean has types for several different kinds of numbers. Nat is the type of natural numbers—that is, the numbers 0, 1, 2, …. Int is the type of integers, Rat is the type of rational numbers, Real is the type of real numbers, and Complex is the type of complex numbers. Lean also uses the notation ℕ, ℤ, ℚ, ℝ, and ℂ for these types. To write formulas involving arithmetic operations, you should use the symbols + for addition, - for subtraction, * for multiplication, / for division, and ^ for exponentiation. To see what’s involved in proving theorems about numbers in Lean, we’ll turn to a few examples from earlier in Chapter 3 of HTPI.\nWe begin with Theorem 3.3.7, which concerns divisibility of integers. As in HTPI, for integers x and y, we will write x ∣ y to mean that x divides y, or y is divisible by x. The formal definition is that x ∣ y means that there is an integer k such that y = x * k. For example, 3 ∣ 12, since 12 = 3 * 4. Lean knows this notation, but there is an important warning: to type the vertical line that means “divides,” you must type \\|, not simply |. (There are two slightly different vertical line symbols, and you have to look closely to see that they are different: | and ∣. It is the second one that means “divides” in Lean, and to enter it you must type \\|.) Here is Theorem 3.3.7, written using our usual rephrasing of a statement of the form A ∧ B → C as A → B → C.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n\n\n⊢ ∀ (a b c : ℤ),\n>>  a ∣ b → b ∣ c → a ∣ c\n\n\nOf course, we begin the proof by introducing arbitrary integers a, b, and c, and assuming a ∣ b and b ∣ c. We also write out the definitions of our assumptions and the goal.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n\n\na b c : ℤ\nh1 : ∃ c, b = a * c\nh2 : ∃ c_1, c = b * c_1\n⊢ ∃ c_1, c = a * c_1\n\n\nWe always use existential givens right away, so we use h1 and h2 to introduce two new variables, m and n.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n\n\na b c : ℤ\nh1 : ∃ c, b = a * c\nh2 : ∃ c_1, c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = b * n\n⊢ ∃ c_1, c = a * c_1\n\n\nIf we substitute the value for b given in h3 into h4, we will see how to reach the goal. Of course, the rewrite tactic is what we need for this.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4: c = a * m * n\n\n\na b c : ℤ\nh1 : ∃ c, b = a * c\nh2 : ∃ c_1, c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = a * m * n\n⊢ ∃ c_1, c = a * c_1\n\n\nLooking at h4, we see that the value we should use for c_1 in the goal is m * n.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4: c = a * m * n\n  apply Exists.intro (m * n)\n\n\na b c : ℤ\nh1 : ∃ c, b = a * c\nh2 : ∃ c_1, c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = a * m * n\n⊢ c = a * (m * n)\n\n\nComparing h4 to the goal, you might think that we can finish the proof with show c = a * (m * n) from h4. But if you try it, you will get an error message. What’s the problem? The difference in the parentheses is the clue. Lean groups the arithmetic operations +, -, *, and / to the left, so h4 means h4 : c = (a * m) * n, which is not quite the same as the goal. To prove the goal, we will need to apply the associative law for multiplication.\nWe have already seen that and_assoc is Lean’s name for the associative law for “and”. Perhaps you can guess that the name for the associative law for multiplication is mul_assoc. If you type #check mul_assoc, Lean’s response will be something like:\n\nmul_assoc : ∀ (a b c : ?m.36649), a * b * c = a * (b * c).\n\nThe appearance of ?m... here indicates that mul_assoc has implicit arguments. The implicit arguments in this case are a little complicated—of course, you can see them by typing #check @mul_assoc—but what they mean is that mul_assoc can be used to prove any statement of the form ∀ (a b c : U), a * b * c = a * (b * c), as long as U is a type that has an associative multiplication operation. In particular, mul_assoc can be used as a proof of ∀ (a b c : Int), a * b * c = a * (b * c). (There are also versions of this theorem for particular number types. You can use the #check command to verify the theorems Nat.mul_assoc : ∀ (a b c : ℕ), a * b * c = a * (b * c), Int.mul_assoc : ∀ (a b c : ℤ), a * b * c = a * (b * c), and so on.)\nReturning to our proof of Theorem 3.3.7, by three applications of universal instantiation, mul_assoc a m n is a proof of a * m * n = a * (m * n), and that is exactly what we need to finish the proof. The tactic rewrite [mul_assoc a m n] at h4 will replace a * m * n in h4 with a * (m * n).\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4: c = a * m * n\n  apply Exists.intro (m * n)\n  rewrite [mul_assoc a m n] at h4\n\n\na b c : ℤ\nh1 : ∃ c, b = a * c\nh2 : ∃ c_1, c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = a * (m * n)\n⊢ c = a * (m * n)\n\n\nBy the way, this is a case in which Lean could have figured out some details on its own. If we had used rewrite [mul_assoc _ _ _] at h4, then Lean would have figured out that the blanks had to be filled in with a, m, and n. And as with the apply tactic, blanks at the end of rewrite rules can be left out, so even rewrite [mul_assoc] at h4 would have worked.\nOf course, now h4 really does match the goal exactly, so we can use it to complete the proof.\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4: c = a * m * n\n  apply Exists.intro (m * n)\n  rewrite [mul_assoc a m n] at h4\n  show c = a * (m * n) from h4\nAs usual, you might find it instructive to compare the Lean proof above to the proof of this theorem in HTPI.\nFor our next example, we’ll do a somewhat more complex proof concerning divisibility. Here is the proof from HTPI.\n\nFor every integer \\(n\\), \\(6 \\mid n\\) iff \\(2 \\mid n\\) and \\(3 \\mid n\\).\n\n\nProof. Let \\(n\\) be an arbitrary integer.\n(\\(\\to\\)) Suppose \\(6 \\mid n\\). Then we can choose an integer \\(k\\) such that \\(6k=n\\). Therefore \\(n = 6k = 2(3k)\\), so \\(2 \\mid n\\), and similarly \\(n = 6k = 3(2k)\\), so \\(3 \\mid n\\).\n(\\(\\leftarrow\\)) Suppose \\(2 \\mid n\\) and \\(3 \\mid n\\). Then we can choose integers \\(j\\) and \\(k\\) such that \\(n = 2j\\) and \\(n = 3k\\). Therefore \\(6(j-k) = 6j - 6k = 3(2j) - 2(3k) = 3n - 2n = n\\), so \\(6 \\mid n\\).  □\n\nLet’s try writing the proof in Lean. We use exactly the same strategy as in the HTPI proof: we begin by fixing an arbitrary integer n, and then we prove the two directions of the biconditional separately.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n\n\ncase mp\nn : ℤ\n⊢ 6 ∣ n → 2 ∣ n ∧ 3 ∣ n\ncase mpr\nn : ℤ\n⊢ 2 ∣ n ∧ 3 ∣ n → 6 ∣ n\n\n\nFor the left-to-right direction, we assume 6 ∣ n, and since the definition of this assumption is an existential statement, we immediately apply existential instantiation.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  assume h1 : 6 ∣ n; define at h1\n  obtain (k : Int) (h2 : n = 6 * k) from h1\n\n\ncase mp\nn : ℤ\nh1 : ∃ c, n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ 2 ∣ n ∧ 3 ∣ n\n\n\nOur goal is now a conjunction, so we prove the two conjuncts separately. Focusing just on the first one, 2 ∣ n, we write out the definition to decide how to proceed.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  assume h1 : 6 ∣ n; define at h1\n  obtain (k : Int) (h2 : n = 6 * k) from h1\n  apply And.intro\n  -- Proof that 2 ∣ n\n  define\n\n\ncase mp.left\nn : ℤ\nh1 : ∃ c, n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ ∃ c, n = 2 * c\n\n\nSince we have n = 6 * k = 2 * 3 * k, it looks like 3 * k is the value we should use for c.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  assume h1 : 6 ∣ n; define at h1\n  obtain (k : Int) (h2 : n = 6 * k) from h1\n  apply And.intro\n  -- Proof that 2 ∣ n\n  define\n  apply Exists.intro (3 * k)\n\n\ncase mp.left\nn : ℤ\nh1 : ∃ c, n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ n = 2 * (3 * k)\n\n\nOnce again, if you think carefully about it, you will see that in order to deduce the goal from h2, we will need to use the associativity of multiplication to rewrite the goal as n = 2 * 3 * k. As we have already seen, mul_assoc 2 3 k is a proof of 2 * 3 * k = 2 * (3 * k). Since we want to replace the right side of this equation with the left in the goal, we’ll use the tactic rewrite [←mul_assoc 2 3 k].\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  assume h1 : 6 ∣ n; define at h1\n  obtain (k : Int) (h2 : n = 6 * k) from h1\n  apply And.intro\n  -- Proof that 2 ∣ n\n  define\n  apply Exists.intro (3 * k)\n  rewrite [←mul_assoc 2 3 k]\n\n\ncase mp.left\nn : ℤ\nh1 : ∃ c, n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ n = 2 * 3 * k\n\n\nDo we have to convince Lean that 2 * 3 = 6? No, remember that Lean works out definitions on its own. Lean knows the definition of multiplication, and it knows that, according to that definition, 2 * 3 is equal to 6. So it regards n = 6 * k and n = 2 * 3 * k as definitionally equal, and therefore it will recognize h2 as a proof of the goal.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  assume h1 : 6 ∣ n; define at h1\n  obtain (k : Int) (h2 : n = 6 * k) from h1\n  apply And.intro\n  -- Proof that 2 ∣ n\n  define\n  apply Exists.intro (3 * k)\n  rewrite [←mul_assoc 2 3 k]\n  show n = 2 * 3 * k from h2\n\n\ncase mp.right\nn : ℤ\nh1 : ∃ c, n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ 3 ∣ n\n\n\nThe proof of the next goal, 3 ∣ n, is similar, and it completes the left-to-right direction of the biconditional.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  assume h1 : 6 ∣ n; define at h1\n  obtain (k : Int) (h2 : n = 6 * k) from h1\n  apply And.intro\n  -- Proof that 2 ∣ n\n  define\n  apply Exists.intro (3 * k)\n  rewrite [←mul_assoc 2 3 k]\n  show n = 2 * 3 * k from h2\n  -- Proof that 3 ∣ n\n  define\n  apply Exists.intro (2 * k)\n  rewrite [←mul_assoc 3 2 k]\n  show n = 3 * 2 * k from h2\n\n\ncase mpr\nn : ℤ\n⊢ 2 ∣ n ∧ 3 ∣ n → 6 ∣ n\n\n\nFor the right-to-left direction, we begin by assuming 2 ∣ n ∧ 3 ∣ n. We write out the definitions of 2 ∣ n and 3 ∣ n, and since this gives us two existential givens, we apply existential instantiation twice. To save space, we won’t repeat the proof of the first half of the proof in the displays below.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  ...\n  -- (←)\n  assume h1\n  have h2 := h1.left\n  have h3 := h1.right\n  define at h2; define at h3; define\n  obtain (j : Int) (h4 : n = 2 * j) from h2\n  obtain (k : Int) (h5 : n = 3 * k) from h3\n\n\ncase mpr\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ c, n = 2 * c\nh3 : ∃ c, n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ ∃ c, n = 6 * c\n\n\nThe next step in the HTPI proof is a string of equations that proves \\(6(j - k) = n\\), which establishes that \\(6 \\mid n\\). Let’s try to do the same thing in Lean, using a calculational proof:\n\n\ntheorem ??Theorem_3_4_7:: :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  -- (→)\n  ...\n  -- (←)\n  assume h1\n  have h2 := h1.left\n  have h3 := h1.right\n  define at h2; define at h3; define\n  obtain (j : Int) (h4 : n = 2 * j) from h2\n  obtain (k : Int) (h5 : n = 3 * k) from h3\n  have h6 : 6 * (j - k) = n :=\n    calc\n      6 * (j - k) = 6 * j - 6 * k := sorry\n      _ = 3 * (2 * j) - 2 * (3 * k) := sorry\n      _ = 3 * n - 2 * n := sorry\n      _ = (3 - 2) * n := sorry\n      _ = n := sorry\n  show ∃ (c : Int), n = 6 * c\n    from Exists.intro (j - k) h6.symm\n\n\n>>Goals accomplished 🎉\n\n\nSometimes the easiest way to write a calculational proof is to justify each line with sorry and then go back and fill in real justifications. Lean has accepted the proof above, so we know that we’ll have a complete proof if we can replace each sorry with a justification.\nTo justify the first line of the calculational proof, try replacing sorry with by library_search. Lean comes up with a justification: exact Int.mul_sub 6 j k. The theorem Int.mul_sub is the integer version of the general theorem mul_sub, which says\n\nmul_sub : ∀ (a b c : ?m.106124), a * (b - c) = a * b - a * c.\n\nThus we can fill in mul_sub 6 j k as a proof of the first equation.\nIt looks like we’ll have to use the associativity of multiplication again to prove the second equation, but it will take more than one step. Let’s try writing a tactic-mode proof. In the display below, we’ll just focus on the calculational proof.\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc\n    6 * (j - k) = 6 * j - 6 * k := mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n\n          **done::\n    _ = 3 * n - 2 * n := sorry\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ c, n = 2 * c\nh3 : ∃ c, n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 6 * j - 6 * k =\n>>  3 * (2 * j) -\n>>    2 * (3 * k)\n\n\nTo justify the second equation, we’ll have to use associativity to rewrite both 3 * (2 * j) as 3 * 2 * j and also 2 * (3 * k) as 2 * 3 * k. So we apply the rewrite tactic to both of the proofs mul_assoc 3 2 j : 3 * 2 * j = 3 * (2 * j) and mul_assoc 2 3 k : 2 * 3 * k = 2 * (3 * k):\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc\n    6 * (j - k) = 6 * j - 6 * k := mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n          rewrite [←mul_assoc 3 2 j]\n          rewrite [←mul_assoc 2 3 k]\n          **done::\n    _ = 3 * n - 2 * n := sorry\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ c, n = 2 * c\nh3 : ∃ c, n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 6 * j - 6 * k =\n>>  3 * 2 * j - \n>>    2 * 3 * k\n\n\nTo finish off the justification of the second equation, we’ll use the theorem Eq.refl. The command #check Eq.refl gives the result Eq.refl : ∀ (a : ?m.80342), a = a. This should remind you of the theorem Iff.refl : ∀ (a : Prop), a ↔︎ a. Recall that we were able to use Iff.refl _ to prove not only any statement of the form a ↔︎ a, but also statements of the form a ↔︎ a', where a and a' are definitionally equal. Similarly, Eq.refl _ will prove any equation of the form a = a', where a and a' are definitionally equal. Since Lean knows that, by definition, 3 * 2 = 6 and 2 * 3 = 6, the goal has this form. Thus we can complete the proof with the tactic show 6 * j - 6 * k = 3 * 2 * j - 2 * 3 * k from Eq.refl _. As we saw earlier, a shorter version of this would be exact Eq.refl _. But this situation comes up often enough that there is an even shorter version: the tactic rfl can be used as a shorthand for either exact Eq.refl _ or exact Iff.refl _. In other words, in a tactic-mode proof, if the goal has one of the forms a = a' or a ↔︎ a', where a and a' are definitionally equal, then the tactic rfl will prove the goal. So rfl will finish off the justification of the second equation, and we can move on to the third.\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc\n    6 * (j - k) = 6 * j - 6 * k := mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n          rewrite [←mul_assoc 3 2 j]\n          rewrite [←mul_assoc 2 3 k]\n          rfl\n          done\n    _ = 3 * n - 2 * n := by\n\n          **done::\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ c, n = 2 * c\nh3 : ∃ c, n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 3 * (2 * j) -\n>>  2 * (3 * k) =\n>>    3 * n - 2 * n\n\n\nTo justify the third equation we have to substitute n for both 2 * j and 3 * k. We can use h4 and h5 in the rewrite tactic to do this. In fact, we can do it in one step: you can put a list of proofs of equations or biconditionals inside the brackets, and the rewrite tactic will perform all of the replacements, one after another. In our case, the tactic rewrite [h4, h5] will first replace 2 * j in the goal with n, and then it will replace 3 * k with n.\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc\n    6 * (j - k) = 6 * j - 6 * k := mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n          rewrite [←mul_assoc 3 2 j]\n          rewrite [←mul_assoc 2 3 k]\n          rfl\n          done\n    _ = 3 * n - 2 * n := by\n          rewrite [←h4, ←h5]\n          **done::\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ c, n = 2 * c\nh3 : ∃ c, n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 3 * n - 2 * n =\n>>  3 * n - 2 * n\n\n\nOf course, the rfl tactic will now finish off the justification of the third equation.\nThe fourth equation is 3 * n - 2 * n = (3 - 2) * n. It looks like the algebraic law we need to justify this is a lot like the one that was used in the first equation, but with the subtraction to the left of the multiplication rather than to the right. It shouldn’t be surprising, therefore, that the name of the theorem we need is sub_mul. The command #check sub_mul gives the response\n\nsub_mul : ∀ (a b c : ?m.73144), (a - b) * c = a * c - b * c,\n\nso sub_mul 3 2 n is a proof of (3 - 2) * n = 3 * n - 2 * n. But the fourth equation has the sides of this equation reversed, so to justify it we need (sub_mul 3 2 n).symm.\nFinally, the fifth equation is (3 - 2) * n = n. Why is this true? Because it is definitionally equal to 1 * n = n. Is there a theorem to justify this last equation? One way to find the answer is to type in this example:\nexample (n : Int) : 1 * n = n := by ##library_search::\nLean responds with exact one_mul n, and #check one_mul yields one_mul : ∀ (a : ?m.73050), 1 * a = a. So one_mul n should justify the last equation. Here’s the complete calculational proof, where we have shortened the second step a bit by doing both rewrites in one step:\n  have h6 : 6 * (j - k) = n :=\n    calc\n      6 * (j - k) = 6 * j - 6 * k := mul_sub 6 j k\n      _ = 3 * (2 * j) - 2 * (3 * k) := by\n            rewrite [←mul_assoc 3 2 j, ←mul_assoc 2 3 k]; rfl\n      _ = 3 * n - 2 * n := by rewrite [←h4, ←h5]; rfl\n      _ = (3 - 2) * n := (sub_mul 3 2 n).symm\n      _ = n := one_mul n\nWhew! This example illustrates why algebraic reasoning in Lean can be difficult. But one reason why this proof was challenging is that we justified all of our steps from basic algebraic principles. Fortunately, there are more powerful tactics that can automate some algebraic reasoning. For example, the tactic ring can combine algebraic laws involving addition, subtraction, multiplication, and exponentiation with natural number exponents to prove many equations in one step. Also, the tactic rw is a variant of rewrite that automatically applies rfl after the rewriting if it can be used to finish the proof. Here’s a shortened version of our calculational proof that uses these tactics.\n  have h6 : 6 * (j - k) = n :=\n    calc\n      6 * (j - k) = 3 * (2 * j) - 2 * (3 * k) := by ring\n      _ = 3 * n - 2 * n := by rw [←h4, ←h5]\n      _ = n := by ring\n\nExercises\n\ntheorem Exercise_3_3_18a (a b c : Int)\n    (h1 : a ∣ b) (h2 : a ∣ c) : a ∣ (b + c) := sorry\n\n2. Complete the following proof by justifying the steps in the calculational proof. Remember that you can use the tactic demorgan : ... to apply one of De Morgan’s laws to just a part of the goal. You may also find the theorem and_or_left useful. (Use #check to see what the theorem says.)\ntheorem Exercise_3_4_6 (U : Type) (A B C : Set U) :\n    A \\ (B ∩ C) = (A \\ B) ∪ (A \\ C) := by\n  apply Set.ext\n  fix x : U\n  show x ∈ A \\ (B ∩ C) ↔ x ∈ A \\ B ∪ A \\ C from\n    calc x ∈ A \\ (B ∩ C)\n        ↔ x ∈ A ∧ ¬(x ∈ B ∧ x ∈ C) := sorry\n      _ ↔ x ∈ A ∧ (¬x ∈ B ∨ ¬x ∈ C) := sorry  \n      _ ↔ (x ∈ A ∧ ¬x ∈ B) ∨ (x ∈ A ∧ ¬x ∈ C) := sorry\n      _ ↔ x ∈ (A \\ B) ∪ (A \\ C) := sorry\n\n\n\nFor the next exercises you will need the following definitions:\ndef even (n : Int) : Prop := ∃ (k : Int), n = 2 * k\ndef odd (n : Int) : Prop := ∃ (k : Int), n = 2 * k + 1\n\ntheorem Exercise_3_4_10 (x y : Int)\n    (h1 : odd x) (h2 : odd y) : even (x - y) := sorry\n\n\ntheorem Exercise_3_4_27a :\n    ∀ (n : Int), 15 ∣ n ↔ 3 ∣ n ∧ 5 ∣ n := sorry"
  },
  {
    "objectID": "Chap4.html",
    "href": "Chap4.html",
    "title": "4  Relations",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\bigtriangleup}\n$$"
  },
  {
    "objectID": "Chap4.html#ordered-pairs-and-cartesian-products",
    "href": "Chap4.html#ordered-pairs-and-cartesian-products",
    "title": "4  Relations",
    "section": "4.1. Ordered Pairs and Cartesian Products",
    "text": "4.1. Ordered Pairs and Cartesian Products\nSection 4.1 of How To Prove It defines the Cartesian product \\(A \\times B\\) of two sets \\(A\\) and \\(B\\) to be the set of all ordered pairs \\((a, b)\\), where \\(a \\in A\\) and \\(b \\in B\\). However, in Lean, Cartesian product is an operation on types, not sets. If A and B are types, then A × B is the type of ordered pairs (a, b), where a has type A and b has type B. In other words, if you have a : A and b : B, then (a, b) is an object of type A × B. There is also notation for the first and second coordinates of an ordered pair. If p has type A × B, then p.1 is the first coordinate of p, and p.2 is the second coordinate. You can also use the notation p.fst for the first coordinate of p and p.snd for the second coordinate. This means that p = (p.1, p.2) = (p.fst, p.snd)."
  },
  {
    "objectID": "Chap4.html#relations",
    "href": "Chap4.html#relations",
    "title": "4  Relations",
    "section": "4.2. Relations",
    "text": "4.2. Relations\nSection 4.2 of HTPI defines a relation from \\(A\\) to \\(B\\) to be a subset of \\(A \\times B\\). In other words, if \\(R\\) is a relation from \\(A\\) to \\(B\\), then \\(R\\) is a set whose element are ordered pairs \\((a, b)\\), where \\(a \\in A\\) and \\(b \\in B\\). We will see in the next section that in Lean, it is convenient to use a somewhat different definition of relations. Nevertheless, we will take some time in this section to study sets of ordered pairs. If A and B are types, and R has type Set (A × B), then R is a set whose elements are ordered pairs (a, b), where a has type A and b has type B.\nSection 4.2 of HTPI discusses several concepts concerning relations. Here is how these concepts are defined in HTPI:\n\nSuppose \\(R\\) is a relation from \\(A\\) to \\(B\\). Then the domain of \\(R\\) is the set\n\n\\(\\text{Dom}(R) = \\{a \\in A \\mid \\exists b \\in B((a, b) \\in R)\\}\\).\n\nThe range of \\(R\\) is the set\n\n\\(\\text{Ran}(R) = \\{b \\in B \\mid \\exists a \\in A((a, b) \\in R)\\}\\).\n\nThe inverse of \\(R\\) is the relation \\(R^{-1}\\) from \\(B\\) to \\(A\\) define as follows:\n\n\\(R^{-1} = \\{(b, a) \\in B \\times A \\mid (a, b) \\in R\\}\\).\n\nFinally, suppose \\(R\\) is a relation from \\(A\\) to \\(B\\) and \\(S\\) is a relation from \\(B\\) to \\(C\\). Then the composition of \\(S\\) and \\(R\\) is the relation \\(S \\circ R\\) from \\(A\\) to \\(C\\) defined as follows:\n\n\\(S \\circ R = \\{(a, c) \\in A \\times C \\mid \\exists b \\in B((a, b) \\in R \\text{ and } (b, c) \\in S)\\}\\).\n\n\nThere are several examples in HTPI that illustrate these definitions. We will focus here on seeing how to work with these concepts in Lean.\nWe can write corresponding definitions in Lean as follows:\ndef Dom {A B : Type} (R : Set (A × B)) : Set A :=\n    { a : A | ∃ (b : B), (a, b) ∈ R }\ndef Ran {A B : Type} (R : Set (A × B)) : Set B :=\n    { b : B | ∃ (a : A), (a, b) ∈ R }\ndef inv {A B : Type} (R : Set (A × B)) : Set (B × A) :=\n    { (b, a) : B × A | (a, b) ∈ R }\ndef comp {A B C : Type} (S : Set (B × C)) (R : Set (A × B)) :\n    Set (A × C) := { (a, c) : A × C | ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S }\nDefinitions in Lean are introduced with the keyword def. In the definition of Dom, we have declared that A and B are implicit arguments and R is an explicit argument. That means that, in a Lean file containing these definitions, if we have R : Set (A × B), then we can just write Dom R for the domain of R, and Lean will figure out for itself what A and B are. After the list of arguments there is a colon and then the type of Dom R, which is Set A. This is followed by := and then the definition of Dom R. The definition says that Dom R is the set of all objects a of type A such that there is some b of type B with (a, b) ∈ R. This is a direct translation, into Lean’s type-theory language, of the first part of Definition 4.2.3. The other three definitions are similar; they define Ran R to be the range of R, inv R to be the inverse of R, and comp S R to be the composition of S and R.\nHere is the main theorem about these concepts, as stated in HTPI:\n\nSuppose \\(R\\) is a relation from \\(A\\) to \\(B\\), \\(S\\) is a relation from \\(B\\) to \\(C\\), and \\(T\\) is a relation from \\(C\\) to \\(D\\). Then:\n\n\\((R^{-1})^{-1} = R\\).\n\\(\\mathrm{Dom}(R^{-1}) = \\mathrm{Ran}(R)\\).\n\\(\\mathrm{Ran}(R^{-1}) = \\mathrm{Dom}(R)\\).\n\\(T \\circ (S \\circ R) = (T \\circ S) \\circ R\\).\n\\((S \\circ R)^{-1} = R^{-1} \\circ S^{-1}\\).\n\n\nAll five parts of this theorem follow directly from the definitions of the relevant concepts. In fact, in the first three parts, Lean recognizes the two sides of the equation as being definitionally equal, and therefore the tactic rfl proves those parts:\ntheorem Theorem_4_2_5_1 {A B : Type}\n    (R : Set (A × B)) : inv (inv R) = R := by rfl\n\ntheorem Theorem_4_2_5_2 {A B : Type}\n    (R : Set (A × B)) : Dom (inv R) = Ran R := by rfl\n\ntheorem Theorem_4_2_5_3 {A B : Type}\n    (R : Set (A × B)) : Ran (inv R) = Dom R := by rfl\nThe fourth part will take a little more work to prove. We start the proof like this:\ntheorem Theorem_4_2_5_4 {A B C D : Type}\n    (R : Set (A × B)) (S : Set (B × C)) (T : Set (C × D)) :\n    comp T (comp S R) = comp (comp T S) R := by\n  apply Set.ext\n  fix (a, d) : A × D\nAfter the apply Set.ext tactic, the goal is\n\n∀ (x : A × D), x ∈ comp T (comp S R) ↔︎ x ∈ comp (comp T S) R\n\nThe next step should be to introduce an arbitrary object of type A × D. We could just call this object x, but Lean let’s us use a shortcut here. An object of type A × D must have the form of an ordered pair, where the first coordinate has type A and the second has type D. So Lean let’s us write it as an ordered pair right away. That’s what we’ve done in the second step, fix (a, d) : A × D. This tactic introduces two new variables into the proof, a : A and d : D. (The proof in HTPI uses a similar shortcut. And we used a similar shortcut in the definitions of inv R and comp R, where the elements of these sets were written as ordered pairs.)\nHere is the complete proof.\ntheorem Theorem_4_2_5_4 {A B C D : Type}\n    (R : Set (A × B)) (S : Set (B × C)) (T : Set (C × D)) :\n    comp T (comp S R) = comp (comp T S) R := by\n  apply Set.ext\n  fix (a, d) : A × D\n  apply Iff.intro\n  -- (→)\n  assume h1 : (a, d) ∈ comp T (comp S R)\n                     --Goal: (a, d) ∈ comp (comp T S) R\n  define             --Goal: ∃ (x : B), (a, x) ∈ R ∧ (x, d) ∈ comp T S\n  define at h1       --h1: ∃ (x : C), (a, x) ∈ comp S R ∧ (x, d) ∈ T\n  obtain (c : C) (h2 : (a, c) ∈ comp S R ∧ (c, d) ∈ T) from h1\n  have h3 : (a, c) ∈ comp S R := h2.left\n  define at h3       --h3: ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S\n  obtain (b : B) (h4 : (a, b) ∈ R ∧ (b, c) ∈ S) from h3\n  apply Exists.intro b    --Goal: (a, b) ∈ R ∧ (b, d) ∈ comp T S\n  apply And.intro h4.left --Goal: (b, d) ∈ comp T S\n  define                  --Goal: ∃ (x : C), (b, x) ∈ S ∧ (x, d) ∈ T\n  show ∃ (x : C), (b, x) ∈ S ∧ (x, d) ∈ T from\n    Exists.intro c (And.intro h4.right h2.right)\n  -- (←)\n  assume h1 : (a, d) ∈ comp (comp T S) R\n  define; define at h1\n  obtain (b : B) (h2 : (a, b) ∈ R ∧ (b, d) ∈ comp T S) from h1\n  have h3 : (b, d) ∈ comp T S := h2.right\n  define at h3\n  obtain (c : C) (h4 : (b, c) ∈ S ∧ (c, d) ∈ T) from h3\n  apply Exists.intro c\n  apply And.intro _ h4.right\n  define\n  show ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S from\n    Exists.intro b (And.intro h2.left h4.left)\nOf course, if you have trouble reading this proof, you can enter it into Lean and see how the tactic state changes over the course of the proof.\nHere is a natural way to start the proof of part 5:\ntheorem Theorem_4_2_5_5 {A B C : Type}\n    (R : Set (A × B)) (S : Set (B × C)) :\n    inv (comp S R) = comp (inv R) (inv S) := by\n  apply Set.ext\n  fix (c, a) : C × A\n  apply Iff.intro\n  -- (→)\n  assume h1 : (c, a) ∈ inv (comp S R) --Goal: (c, a) ∈ comp (inv R) (inv S)\n  define at h1              --h1: ∃ x, (a, x) ∈ R ∧ (x, c) ∈ S\n  define                    --Goal: ∃ x, (c, x) ∈ inv S ∧ (x, a) ∈ inv R\n  obtain (b : B) (h2 : (a, b) ∈ R ∧ (b, c) ∈ S) from h1\n  apply Exists.intro b      --Goal: (c, b) ∈ inv S ∧ (b, a) ∈ inv R\n  define : (b, a) ∈ inv R\nAfter the tactics apply Set.ext and fix (c, a) : C × A, the goal is (c, a) ∈ inv (comp S R) ↔︎ (c, a) ∈ comp (inv R) (inv S). For the proof of the left-to-right direction, we assume h1 : (c, a) ∈ inv (comp S R), and we must prove (c, a) ∈ comp (inv R) (inv S). The definition of h1 is an existential statement, so we apply existential instantiation to obtain b : B and h2 : (a, b) ∈ R ∧ (b, c) ∈ S. The definition of the goal is also an existential statement, and after the tactic apply Exists.intro b, the goal is (c, b) ∈ inv S ∧ (b, a) ∈ inv R. It looks like this goal will follow easily from h2, using the definitions of the inverses of S and R.\nThe tactic define : (b, a) ∈ inv R should rewrite the second half of the goal, using the definition of inv R. You might expect the definition to be (a, b) ∈ R, but what the tactic produces is R (a, b). Usually, the define tactic does a good job of writing out definitions, but in this case it has let us down a bit. The definition isn’t wrong (explaining why would take us too far afield), but it isn’t what we wanted.\nOf course, we don’t really need to use the define tactic—Lean will recognize that (b, a) ∈ inv R and (a, b) ∈ R are definitionally equally on its own. But it would be nice if we could teach Lean to do a better job of writing out the definitions of inverses. We can do that by proving a preliminary theorem before proving part 5 of Theorem 4.2.5:\ntheorem simp_inv\n    {A B : Type} (R : Set (A × B)) (a : A) (b : B) :\n    (b, a) ∈ inv R ↔ (a, b) ∈ R := by rfl\nNow, any time we have a relation R : Set (A × B) and objects a : A and b : B, the expression simp_inv R a b will be a proof of the statement (b, a) ∈ inv R ↔︎ (a, b) ∈ R. (Note that A and B are implicit arguments and don’t need to be specified.) And that means that the tactic rewrite [simp_inv R a b] will change (b, a) ∈ inv R to (a, b) ∈ R, exactly as we wanted. In fact, as we’ve seen before, you can just write rewrite [simp_inv], and Lean will figure out how to apply the theorem simp_inv to rewrite some part of the goal.\nReturning to our proof of part 5 of Theorem 4.2.5, recall that after the step apply Exists.intro b, the goal is (c, b) ∈ inv S ∧ (b, a) ∈ inv R. Rather than using the define tactic to write out the definitions of the inverses, we’ll use the tactic rewrite [simp_inv, simp_inv]. Why do we list simp_inv twice in the rewrite tactic? When we ask Lean to use the theorem simp_inv as a rewriting rule, it figures out that simp_inv S b c is a proof of the statement (c, b) ∈ inv S ↔︎ (b, c) ∈ S, which can be used to rewrite the left half of the goal. To rewrite the right half, we need a different application of the simp_inv theorem, simp_inv R a b. So we have to ask Lean to apply the theorem a second time. After the rewrite tactic, the goal is (b, c) ∈ S ∧ (a, b) ∈ R, which will follow easily from h2.\nThe rest of the proof of straightforward. Here is the complete proof.\ntheorem Theorem_4_2_5_5 {A B C : Type}\n    (R : Set (A × B)) (S : Set (B × C)) :\n    inv (comp S R) = comp (inv R) (inv S) := by\n  apply Set.ext\n  fix (c, a) : C × A\n  apply Iff.intro\n  -- (→)\n  assume h1 : (c, a) ∈ inv (comp S R) --Goal: (c, a) ∈ comp (inv R) (inv S)\n  define at h1                 --h1: ∃ x, (a, x) ∈ R ∧ (x, c) ∈ S\n  define                       --Goal: ∃ x, (c, x) ∈ inv S ∧ (x, a) ∈ inv R\n  obtain (b : B) (h2 : (a, b) ∈ R ∧ (b, c) ∈ S) from h1\n  apply Exists.intro b         --Goal: (c, b) ∈ inv S ∧ (b, a) ∈ inv R\n  rewrite [simp_inv, simp_inv] --Goal: (b, c) ∈ S ∧ (a, b) ∈ R\n  show (b, c) ∈ S ∧ (a, b) ∈ R from And.intro h2.right h2.left\n  -- (←)\n  assume h1 : (c, a) ∈ comp (inv R) (inv S)\n  define at h1\n  define\n  obtain (b : B) (h2 : (c, b) ∈ inv S ∧ (b, a) ∈ inv R) from h1\n  apply Exists.intro b\n  rewrite [simp_inv, simp_inv] at h2\n  show (a, b) ∈ R ∧ (b, c) ∈ S from And.intro h2.right h2.left\nBy the way, an alternative way to complete both directions of this proof would have been to apply the commutativity of “and”. See if you can guess the name of that theorem (you can use #check to confirm your guess) and apply it as a third rewriting rule in the rewrite steps.\n\nExercises\n\ntheorem Exercise_4_2_9a {A B C : Type} (R : Set (A × B))\n    (S : Set (B × C)) : Dom (comp S R) ⊆ Dom R := sorry\n\n\ntheorem Exercise_4_2_9b {A B C : Type} (R : Set (A × B))\n    (S : Set (B × C)) : Ran R ⊆ Dom S → Dom (comp S R) = Dom R := sorry\n\n\n--Fill in the blank to get a correct theorem and then prove the theorem\ntheorem Exercise_4_2_9c {A B C : Type} (R : Set (A × B))\n    (S : Set (B × C)) : ___ → Ran (comp S R) = Ran S := sorry\n\n\ntheorem Exercise_4_2_12a {A B C : Type}\n    (R : Set (A × B)) (S T : Set (B × C)) :\n    (comp S R) \\ (comp T R) ⊆ comp (S \\ T) R := sorry\n\n5. Here is an incorrect theorem with an incorrect proof.\n\nSuppose \\(R\\) is a relation from \\(A\\) to \\(B\\) and \\(S\\) and \\(T\\) are relations from \\(B\\) to \\(C\\). Then \\((S \\setmin T) \\circ R \\subseteq (S \\circ R) \\setmin (T \\circ R)\\).\n\n\nSuppose \\((a, c) \\in (S \\setmin T) \\circ R\\). Then we can choose some \\(b \\in B\\) such that \\((a, b) \\in R\\) and \\((b, c) \\in S \\setmin T\\), so \\((b, c) \\in S\\) and \\((b, c) \\notin T\\). Since \\((a, b) \\in R\\) and \\((b, c) \\in S\\), \\((a, c) \\in S \\circ R\\). Similarly, since \\((a, b) \\in R\\) and \\((b, c) \\notin T\\), \\((a, c) \\notin T \\circ R\\). Therefore \\((a, c) \\in (S \\circ R) \\setmin (T \\circ R)\\). Since \\((a, c)\\) was arbitrary, this shows that \\((S \\setmin T) \\circ R \\subseteq (S \\circ R) \\setmin (T \\circ R)\\).  □\n\nFind the mistake in the proof by attempting to write the proof in Lean:\n--You won't be able to complete this proof\ntheorem Exercise_4_2_12b {A B C : Type}\n    (R : Set (A × B)) (S T : Set (B × C)) :\n    comp (S \\ T) R ⊆ (comp S R) \\ (comp T R) := sorry\n6. Is the following theorem correct? Try to prove it in Lean. If you can’t prove it, see if you can find a counterexample.\n--You might not be able to complete this proof\ntheorem Exercise_4_2_14c {A B C : Type}\n    (R : Set (A × B)) (S T : Set (B × C)) :\n    comp (S ∩ T) R = (comp S R) ∩ (comp T R) := sorry"
  },
  {
    "objectID": "Chap4.html#more-about-relations",
    "href": "Chap4.html#more-about-relations",
    "title": "4  Relations",
    "section": "4.3. More About Relations",
    "text": "4.3. More About Relations\nSection 4.3 of HTPI introduces new notation for working with relations. If \\(R \\subseteq A \\times B\\), \\(a \\in A\\), and \\(b \\in B\\), then HTPI introduces the notation \\(aRb\\) as an alternative way of saying \\((a, b) \\in R\\).\nThe notation we will use in Lean is slightly different. Corresponding to the notation \\(aRb\\) in HTPI, in Lean we will use the notation R a b. And we cannot use this notation when R has type Set (A × B). Rather, we will need to introduce a new type for the variable R in the notation R a b. The name we will use for this new type is Rel A B. Thus, if R has type Rel A B, a has type A, and b has type B, then R a b is a proposition. This should remind you of the way predicates work in Lean. If we have P : Pred A, then we think of P as representing a property that an object of type A might have, and if we also have a : A, then P a is the proposition asserting that a has the property represented by P. Similarly, if we have R : Rel A B, then we can think of R as representing a relationship that might hold between an object of type A and an object of type B, and if we also have a : A and b : B, then R a b is the proposition asserting that the relationship represented by R holds between a and b.\nNotice that in HTPI, the same variable \\(R\\) is used in both the notation \\(aRb\\) and \\((a, b) \\in R\\). But in Lean, the notation R a b is used when R has type Rel A B, and the notation (a, b) ∈ R is used when R has type Set (A × B). The types Rel A B and Set (A × B) are different, so we cannot use the same variable R in the two notations. However, there is a correspondence between the two types. Suppose R has type Rel A B. If we let R' denote the set of all ordered pairs (a, b) : A × B such that the proposition R a b is true, then R' has type Set (A × B). And there is then a simple relationship between R and R': for any objects a : A and b : B, the propositions R a b and (a, b) ∈ R' are equivalent. For our work in Lean, we will say that R is a relation from A to B, and R' is the extension of R.\nWe can define the extension of a relation, and state the correspondence between a relation and its extension, in Lean as follows:\ndef extension {A B : Type} (R : Rel A B) : Set (A × B) :=\n    { (a, b) : A × B | R a b }\n\ntheorem simp_ext {A B : Type} (R : Rel A B) (a : A) (b : B) :\n    (a, b) ∈ extension R ↔ R a b := by rfl\nThe rest of Chapter 4 of HTPI focuses on relations from a set to itself; in Lean, the corresponding idea is a relation from a type to itself. If A is any type and R has type Rel A A, then we will say that R is a binary relation on A. The notation BinRel A denotes the type of binary relations on A. In other words, BinRel A is just an abbreviation for Rel A A. If R is a binary relation on A, then we say that R is reflexive if for every x of type A, R x x holds. It is symmetric if for all x and y of type A, if R x y then R y x. And it is transitive if for all x, y, and z of type A, if R x y and R y z then R x z. Of course, we can tell Lean about these definitions, which correspond to Definition 4.3.2 in HTPI:\ndef reflexive {A : Type} (R : BinRel A) : Prop :=\n    ∀ (x : A), R x x\ndef symmetric {A : Type} (R : BinRel A) : Prop :=\n    ∀ (x y : A), R x y → R y x\ndef transitive {A : Type} (R : BinRel A) : Prop :=\n    ∀ (x y z : A), R x y → R y z → R x z\nOnce again, we refer you to HTPI to see examples of these concepts, and we focus here on proving theorems about these concepts in Lean. The main theorem about these concepts in Section 4.3 of HTPI is Theorem 4.3.4. Here is what it says:\n\nSuppose \\(R\\) is a relation on a set \\(A\\).\n\n\\(R\\) is reflexive iff \\(\\{(x, y) \\in A \\times A \\mid x = y\\} \\subseteq R\\).\n\\(R\\) is symmetric iff \\(R = R^{-1}\\).\n\\(R\\) is transitive iff \\(R \\circ R \\subseteq R\\).\n\n\nWe can prove corresponding statements in Lean, but we’ll have to be careful to distinguish between the types BinRel A and Set (A × A). In HTPI, each of the three statements in the theorem uses the same letter \\(R\\) on both sides of the “iff”, but we can’t write the statements that way in Lean. In each statement, the part before “iff” uses a concept that was defined for objects of type BinRel A, whereas the part after “iff” uses concepts that only make sense for objects of type Set (A × A). So we’ll have to rephrase the statements by using the correspondence between a relation of type BinRel A and its extension, which has type Set (A × A). Here’s the Lean theorem corresponding to statement 2 of Theorem 4.3.4:\ntheorem Theorem_4_3_4_2 {A : Type} (R : BinRel A) :\n    symmetric R ↔ extension R = inv (extension R) := by\n  apply Iff.intro\n  -- (→)\n  assume h1 : symmetric R\n  define at h1             --h1: ∀ (x y : A), R x y → R y x\n  apply Set.ext\n  fix (a, b) : A × A\n  show (a, b) ∈ extension R ↔ (a, b) ∈ inv (extension R) from\n    calc\n      (a, b) ∈ extension R ↔ R a b   := by rfl\n      _ ↔ R b a                      := Iff.intro (h1 a b) (h1 b a)\n      _ ↔ (a, b) ∈ inv (extension R) := by rfl\n  -- (←)\n  assume h1 : extension R = inv (extension R)\n  define                   --Goal:  ∀ (x y : A), R x y → R y x\n  fix a : A; fix b : A\n  assume h2 : R a b        --Goal:  R b a\n  rewrite [←simp_ext R, h1, simp_inv, simp_ext] at h2\n  show R b a from h2\nNote that near the end of the proof, we assume h2 : R a b, and our goal is R b a. We convert R a b to R b a by a sequence of rewrites. Applying the right-to-left direction of the theorem simp_ext R a b converts R a b to (a, b) ∈ extension R. Then rewriting with h1 converts this to (a, b) ∈ inv (extension R), using simp_inv (extension R) b a converts this to (b, a) ∈ extension R, and finally simp_ext R b a produces R b a. Usually we can leave out the arguments when we use a theorem as a rewriting rule, and Lean will figure them out for itself. But in this case, if you try using ←simp_ext as the first rewriting rule, you will see that Lean is unable to figure out that it should use the right-to-left direction of simp_ext R a b. Supplying the first argument turns out to be enough of a hint for Lean to figure out the rest. That’s why our first rewriting rule is ←simp_ext R.\nWe’ll leave the proofs of the other two statements in Theorem 4.3.4 as exercises for you.\nFor any types A and B, if we want to define a particular relation R from A to B, we can do it by specifying, for any a : A and b : B, what proposition is represented by R a b. For example, for any type A, we can define a relation elementhood A from A to Set A as follows:\ndef elementhood (A : Type) (a : A) (X : Set A) : Prop := a ∈ X\nThis definition says that if A is a type, a has type A, and X has type Set A, then elementhood A a X is the proposition a ∈ X. Thus, if elementhood A is followed by objects of type A and Set A, the result is a proposition, so elementhood A is functioning as a relation from A to Set A. For example, elementhood Int is a relation from integers to sets of integers, and elementhood Int 6 { n : Int | ∃ (k : Int), n = 2 * k } is the (true) statement that 6 is an element of the set of even integers. (You are asked to prove it in the exercises.)\n\nExercises\n\nexample :\n    elementhood Int 6 { n : Int | ∃ (k : Int), n = 2 * k } := sorry\n\n\ntheorem Theorem_4_3_4_1 {A : Type} (R : BinRel A) :\n    reflexive R ↔ { (x, y) : A × A | x = y } ⊆ extension R := sorry\n\n\ntheorem Theorem_4_3_4_3 {A : Type} (R : BinRel A) :\n    transitive R ↔\n        comp (extension R) (extension R) ⊆ extension R := sorry\n\n\n\n\nTo state our remaining exercises, we’ll need a definition. If R has type Set (A × B), then we define RelFromExt R to be the relation whose extension is R. A few simple theorems, which follow directly from the definition, clarify the meaning of RelFromExt R.\ndef RelFromExt {A B : Type}\n    (R : Set (A × B)) (a : A) (b : B) : Prop := (a, b) ∈ R\n\ntheorem simp_RelFromExt {A B : Type}\n    (R : Set (A × B)) (a : A) (b : B) :\n    RelFromExt R a b ↔ (a, b) ∈ R := by rfl\n\nexample {A B : Type} (R : Rel A B) :\n    RelFromExt (extension R) = R := by rfl\n\nexample {A B : Type} (R : Set (A × B)) :\n    extension (RelFromExt R) = R := by rfl\n\ntheorem Exercise_4_3_12a {A : Type} (R : BinRel A) (h1 : reflexive R) :\n    reflexive (RelFromExt (inv (extension R))) := sorry\n\n\ntheorem Exercise_4_3_12c {A : Type} (R : BinRel A) (h1 : transitive R) :\n    transitive (RelFromExt (inv (extension R))) := sorry\n\n\ntheorem Exercise_4_3_18 {A : Type}\n    (R S : BinRel A) (h1 : transitive R) (h2 : transitive S)\n    (h3 : comp (extension S) (extension R) ⊆\n        comp (extension R) (extension S)) :\n    transitive (RelFromExt (comp (extension R) (extension S))) := sorry\n\n\n\n\nIn the next two exercises, determine whether or not the theorem is correct.\n\n--You might not be able to complete this proof\ntheorem Exercise_4_3_13b {A : Type}\n    (R1 R2 : BinRel A) (h1 : symmetric R1) (h2 : symmetric R2) :\n    symmetric (RelFromExt ((extension R1) ∪ (extension R2))) := sorry\n\n\n--You might not be able to complete this proof\ntheorem Exercise_4_3_13c {A : Type}\n    (R1 R2 : BinRel A) (h1 : transitive R1) (h2 : transitive R2) :\n    transitive (RelFromExt ((extension R1) ∪ (extension R2))) := sorry"
  },
  {
    "objectID": "Chap4.html#ordering-relations",
    "href": "Chap4.html#ordering-relations",
    "title": "4  Relations",
    "section": "4.4. Ordering Relations",
    "text": "4.4. Ordering Relations\nSection 4.4 of HTPI begins by defining several new concepts about binary relations. Here are the definitions, written in Lean:\ndef antisymmetric {A : Type} (R : BinRel A) : Prop :=\n    ∀ (x y : A), R x y → R y x → x = y\n\ndef partial_order {A : Type} (R : BinRel A) : Prop :=\n    reflexive R ∧ transitive R ∧ antisymmetric R\n\ndef total_order {A : Type} (R : BinRel A) : Prop :=\n    partial_order R ∧ ∀ (x y : A), R x y ∨ R y x\nThese definitions say that if R is a binary relation on A, then R is antisymmetric if R x y and R y x cannot both be true unless x = y. R is a partial order on A—or just a partial order, if A is clear from context—if it is reflexive, transitive, and antisymmetric. And R is a total order on A if it is a partial order and also, for any x and y of type A, either R x y or R y x. Note that, since Lean groups the connective ∧ to the right, partial_order R means reflexive R ∧ (transitive R ∧ antisymmetric R), and therefore if h is a proof of partial_order R, then h.left is a proof of reflexive R, h.right.left is a proof of transitive R, and h.right.right is a proof of antisymmetric R.\nExample 4.4.3 in HTPI gives several examples of partial orders and total orders. We’ll give one of those examples here. For any type A, we define sub A to be the subset relation on sets of objects of type A:\ndef sub (A : Type) (X Y : Set A) : Prop := X ⊆ Y\nAccording to this definition, sub A is a binary relation on Set A, and for any two sets X and Y of type Set A, sub A X Y is the proposition X ⊆ Y. We will leave it as an exercise for you to prove that sub A is a partial order on the type Set A.\nNotice that X ⊆ Y could be thought of as expressing a sense in which Y is “at least as large as” X. Often, if R is a partial order on A and a and b have type A, then R a b can be thought of as meaning that b is in some sense “at least as large as” a. Many of the concepts we study for partial and total orders are motivated by this interpretation of R.\nFor example, if R is a partial order on A, B has type Set A, and b has type A, then we say that b is an R-smallest element of B if it is an element of B, and every element of B is at least as large as b, according to this interpretation of the ordering R. We say that b is an R-minimal element of B if it is an element of B, and there is no other element of B that is smaller than b, according to the ordering R. We can state these precisely as definitions in Lean:\ndef smallestElt {A : Type} (R : BinRel A) (b : A) (B : Set A) : Prop :=\n    b ∈ B ∧ ∀ x ∈ B, R b x\n\ndef minimalElt {A : Type} (R : BinRel A) (b : A) (B : Set A) : Prop :=\n    b ∈ B ∧ ¬∃ x ∈ B, R x b ∧ x ≠ b\nNotice that, as in HTPI, in Lean we can write ∀ x ∈ B, P x as an abbreviation for ∀ (x : A), x ∈ B → P x, and ∃ x ∈ B, P x as an abbreviation for ∃ (x : A), x ∈ B ∧ P x. According to these definitions, smallestElt R b B is the proposition that b is an R-smallest element of B, and minimalElt R b B means that b is an R-minimal element of B.\nTheorem 4.4.6 in HTPI asserts three statements about these concepts. We’ll prove the second and third, and leave the first as an exercise for you. The first statement in Theorem 4.4.6 says that if B has an R-smallest element, then that R-smallest element is unique. Thus, we can talk about the R-smallest element of B rather than an R-smallest element. The second says that if b is the R-smallest element of B, then it is also an R-minimal element, and it is the only R-minimal element. Here is how you might start the proof:\ntheorem Theorem_4_4_6_2 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R) (h2 : smallestElt R b B) :\n    minimalElt R b B ∧ ∀ (c : A), minimalElt R c B → b = c := by\n  define at h1  --h1: reflexive R ∧ transitive R ∧ antisymmetric R\n  define at h2  --h2: b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro\n  -- Proof that b is minimal\n  define        --Goal: b ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b\n  apply And.intro h2.left\n  quant_neg     --Goal: ∀ (x : A), ¬(x ∈ B ∧ R x b ∧ x ≠ b)\n  **demorgan : ¬(x ∈ B ∧ R x b ∧ x ≠ b)::\nWhen the goal is ∀ (x : A), ¬(x ∈ B ∧ R x b ∧ x ≠ b), it is tempting to apply the demorgan tactic to ¬(x ∈ B ∧ R x b ∧ x ≠ b), but unfortunately this generates an error in Lean: unknown identifier 'x'. The problem is that x is not defined in the tactic state, so without the quantifier ∀ (x : A) in front of it, ¬(x ∈ B ∧ R x b ∧ x ≠ b) doesn’t mean anything to Lean. The solution to the problem is to deal with the universal quantifier first by introducing an arbitrary x of type A. Once x has been introduced, we can apply the demorgan tactic.\ntheorem Theorem_4_4_6_2 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R) (h2 : smallestElt R b B) :\n    minimalElt R b B ∧ ∀ (c : A), minimalElt R c B → b = c := by\n  define at h1     --h1: reflexive R ∧ transitive R ∧ antisymmetric R\n  define at h2     --h2: b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro\n  -- Proof that b is minimal\n  define           --Goal: b ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b\n  apply And.intro h2.left\n  quant_neg        --Goal: ∀ (x : A), ¬(x ∈ B ∧ R x b ∧ x ≠ b)\n  fix x : A\n  demorgan         --Goal: ¬x ∈ B ∨ ¬(R x b ∧ x ≠ b)\n  or_right with h3 --h3: x ∈ B; Goal: ¬(R x b ∧ x ≠ b)\n  demorgan         --Goal: ¬R x b ∨ x = b\n  or_right with h4 --h4: R x b; Goal: x = b\n  have h5 : R b x := h2.right x h3\n  have h6 : antisymmetric R := h1.right.right\n  define at h6     --h6: ∀ (x y : A), R x y → R y x → x = y\n  show x = b from h6 x b h4 h5\n  -- Proof that b is only minimal element\n  fix c : A\n  assume h3 : minimalElt R c B\n  define at h3    --h3: c ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x c ∧ x ≠ c\n  contradict h3.right with h4\n                  --h4: ¬b = c; Goal: ∃ (x : A), x ∈ B ∧ R x c ∧ x ≠ c\n  have h5 : R b c := h2.right c h3.left\n  show ∃ (x : A), x ∈ B ∧ R x c ∧ x ≠ c from\n    Exists.intro b (And.intro h2.left (And.intro h5 h4))\nFinally, the third statement in Theorem 4.4.6 says that if R is a total order, then any R-minimal element of a set B must be the R-smallest element of B. The beginning of the proof is straightforward:\ntheorem Theorem_4_4_6_3 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : total_order R) (h2 : minimalElt R b B) : smallestElt R b B := by\n  define at h1         --h1: partial_order R ∧ ∀ (x y : A), R x y ∨ R y x\n  define at h2         --h2: b ∈ B ∧ ¬∃ x, x ∈ B ∧ R x b ∧ x ≠ b\n  define               --Goal: b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro h2.left  --Goal: ∀ (x : A), x ∈ B → R b x\n  fix x : A\n  assume h3 : x ∈ B        --Goal: R b x\nSurprisingly, at this point it is difficult to find a way to reach the goal R b x. See HTPI for an explanation of why it turns out to be helpful to split the proof into two cases, depending on whether or not x = b. Of course, we use the by_cases tactic for this.\ntheorem Theorem_4_4_6_3 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : total_order R) (h2 : minimalElt R b B) : smallestElt R b B := by\n  define at h1         --h1: partial_order R ∧ ∀ (x y : A), R x y ∨ R y x\n  define at h2         --h2: b ∈ B ∧ ¬∃ x, x ∈ B ∧ R x b ∧ x ≠ b\n  define               --Goal: b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro h2.left  --Goal: ∀ (x : A), x ∈ B → R b x\n  fix x : A\n  assume h3 : x ∈ B        --Goal: R b x\n  by_cases h4 : x = b\n  -- Case 1 : x = b\n  rewrite [h4]             --Goal: R b b\n  have h5 : partial_order R := h1.left\n  define at h5\n  have h6 : reflexive R := h5.left\n  define at h6\n  show R b b from h6 b\n  -- Case 2 : x ≠ b\n  have h5 : ∀ (x y : A), R x y ∨ R y x := h1.right\n  have h6 : R x b ∨ R b x := h5 x b\n  have h7 : ¬R x b := by\n    contradict h2.right with h8\n    show ∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b from\n      Exists.intro x (And.intro h3 (And.intro h8 h4))\n  disj_syll h6 h7\n  show R b x from h6\nImitating the definitions above, you should be able to formulate definitions of R-largest and R-maximal elements. Section 4.4 of HTPI defines four more terms: upper bound, lower bound, least upper bound, and greatest lower bound. We will discuss upper bounds and least upper bounds, and leave lower bounds and greatest lower bounds for you to figure out on your own.\nIf R is a partial order on A, B has type Set A, and a has type A, then a is called an upper bound for B if it is at least as large as every element of B. If it is the smallest element of the set of upper bounds, then it is called the least upper bound of B. The phrase “least upper bound” is often abbreviated “lub”. Here are these definitions, written in Lean:\ndef upperBd {A : Type} (R : BinRel A) (a : A) (B : Set A) : Prop :=\n    ∀ x ∈ B, R x a\n\ndef lub {A : Type} (R : BinRel A) (a : A) (B : Set A) : Prop :=\n    smallestElt R a { c : A | upperBd R c B }\nAs usual, we will let you consult HTPI for examples of these concepts. But we will mention one example: If A is a type and F has type Set (Set A)—that is, F is a set whose elements are sets of objects of type A—then the least upper bound of F, with respect to the partial order sub A, is ⋃₀F. We leave the proof of this fact as an exercise.\n\nExercises\n\ntheorem Example_4_4_3_1 {A : Type} : partial_order (sub A) := sorry\n\n\ntheorem Theorem_4_4_6_1 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R) (h2 : smallestElt R b B) :\n    ∀ (c : A), smallestElt R c B → b = c := sorry\n\n\n--If F is a set of sets, then ⋃₀F is the lub of F in the subset ordering\ntheorem Theorem_4_4_11 {A : Type} (F : Set (Set A)) :\n    lub (sub A) (⋃₀F) F := sorry\n\n\ntheorem Exercise_4_4_8 {A B : Type} (R : BinRel A) (S : BinRel B)\n    (T : BinRel (A × B)) (h1 : partial_order R) (h2 : partial_order S)\n    (h3 : ∀ (a a' : A) (b b' : B),\n        T (a, b) (a', b') ↔ R a a' ∧ S b b') :\n    partial_order T := sorry\n\n\ntheorem Exercise_4_4_9_part {A B : Type} (R : BinRel A) (S : BinRel B)\n    (L : BinRel (A × B)) (h1 : total_order R) (h2 : total_order S)\n    (h3 : ∀ (a a' : A) (b b' : B),\n        L (a, b) (a', b') ↔ R a a' ∧ (a = a' → S b b')) :\n    ∀ (a a' : A) (b b' : B),\n        L (a, b) (a', b') ∨ L (a', b') (a, b) := sorry\n\n\ntheorem Exercise_4_4_15a {A : Type}\n    (R1 R2 : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R1) (h2 : partial_order R2)\n    (h3 : extension R1 ⊆ extension R2) :\n    smallestElt R1 b B → smallestElt R2 b B := sorry\n\n\ntheorem Exercise_4_4_15b {A : Type}\n    (R1 R2 : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R1) (h2 : partial_order R2)\n    (h3 : extension R1 ⊆ extension R2) :\n    minimalElt R2 b B → minimalElt R1 b B := sorry\n\n\ntheorem Exercise_4_4_18a {A : Type}\n    (R : BinRel A) (B1 B2 : Set A) (h1 : partial_order R)\n    (h2 : ∀ x ∈ B1, ∃ y ∈ B2, R x y) (h3 : ∀ x ∈ B2, ∃ y ∈ B1, R x y) :\n    ∀ (x : A), upperBd R x B1 ↔ upperBd R x B2 := sorry\n\n\ntheorem Exercise_4_4_22 {A : Type}\n    (R : BinRel A) (B1 B2 : Set A) (x1 x2 : A)\n    (h1 : lub R x1 B1) (h2 : lub R x2 B2) :\n    B1 ⊆ B2 → R x1 x2 := sorry\n\n10. See the exercises in the previous section for the definition of the operation RelFromExt that is used in this exercise.\ntheorem Exercise_4_4_24 (A : Type) (R : Set (A × A)) :\n    smallestElt (sub (A × A)) (R ∪ (inv R))\n    { T : Set (A × A) | R ⊆ T ∧ symmetric (RelFromExt T) } := sorry"
  },
  {
    "objectID": "Chap4.html#equivalence-relations",
    "href": "Chap4.html#equivalence-relations",
    "title": "4  Relations",
    "section": "4.5 Equivalence Relations",
    "text": "4.5 Equivalence Relations\nChapter 4 of HTPI concludes with the study of one more combination of properties that a relation might have. A binary relation \\(R\\) on a set \\(A\\) is called an equivalence relation if it is reflexive, symmetric, and transitive. If \\(x \\in A\\), then the equivalence class of \\(x\\) with respect to \\(R\\) is the set of all \\(y \\in A\\) such that \\(yRx\\). In HTPI, this equivalence class is denoted \\([x]_R\\), so we have \\[\n[x]_R = \\{y \\in A \\mid yRx\\}.\n\\] The set whose elements are all of these equivalence classes is called \\(A\\) mod \\(R\\). It is written \\(A/R\\), so \\[\nA/R = \\{[x]_R \\mid x \\in A\\}.\n\\] Note that \\(A/R\\) is a set whose elements are sets: for each \\(x \\in A\\), \\([x]_R\\) is a subset of \\(A\\), and \\([x]_R \\in A/R\\).\nTo define these concepts in Lean, we write:\ndef equiv_rel {A : Type} (R : BinRel A) : Prop :=\n  reflexive R ∧ symmetric R ∧ transitive R\n\ndef equivClass {A : Type} (R : BinRel A) (x : A) : Set A :=\n  { y : A | R y x }\n\ndef mod (A : Type) (R : BinRel A) : Set (Set A) :=\n  { equivClass R x | x : A }\nThus, equiv_rel R is the proposition that R is an equivalence relation, equivClass R x is the equivalence class of x with respect to R, and mod A R is A mod R. Note that equivClass R x has type Set A, while mod A R has type Set (Set A). The definition of mod A R is shorthand for { X : Set A | ∃ (x : A), equivClass R x = X }.\nHTPI gives several examples of equivalence relations, and these examples illustrate that equivalence classes always have certain properties. The most important of these are that each equivalence class is a nonempty set, the equivalence classes do not overlap, and their union is all of A. We say that the equivalence classes form a partition of A. To state and prove these properties in Lean we will need some definitions. We start with these:\ndef is_empty {A : Type} (X : Set A) : Prop := ¬∃ (x : A), x ∈ X \n\ndef pairwise_disjoint {A : Type} (F : Set (Set A)) : Prop :=\n  ∀ X ∈ F, ∀ Y ∈ F, X ≠ Y → is_empty (X ∩ Y)\nTo say that a set X is empty, we could write X = ∅, but it is more convenient to have a statement that says more explicitly what it means for a set to be empty. Thus, we have defined is_empty X to be the proposition saying that X has no elements. If F has type Set (Set A), then pairwise_disjoint F is the proposition that no two distinct elements of F have any element in common—in other words, the elements of F do not overlap. We can now give the precise definition of a partition:\ndef partition {A : Type} (F : Set (Set A)) : Prop :=\n  (∀ (x : A), x ∈ ⋃₀F) ∧ pairwise_disjoint F ∧ ∀ X ∈ F, ¬is_empty X\nThe main theorem about equivalence relations in HTPI is Theorem 4.5.4, which says that mod A R is a partition of A. The proof of this theorem is hard enough that HTPI proves two facts about equivalence classes first. A fact that is proven just for the purpose of using it to prove something else is often called a lemma. We can use this term in Lean as well. Here is the first part of Lemma 4.5.5 from HTPI\nlemma Lemma_4_5_5_1 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ (x : A), x ∈ equivClass R x := by\n  fix x : A\n  define           --Goal: R x x\n  define at h      --h: reflexive R ∧ symmetric R ∧ transitive R\n  have href : reflexive R := h.left\n  show R x x from href x\nThe command #check @Lemma_4_5_5_1 produces the result\n\n@Lemma_4_5_5_1 : ∀ {A : Type} (R : BinRel A),\n    equiv_rel R → ∀ (x : A), x ∈ equivClass R x\n\nThus, if we have R : BinRel A, h : equiv_rel R, and x : A, then Lemma_4_5_5_1 R h x is a proof of x ∈ equivClass R x. We will use this at the end of the proof of our next lemma:\nlemma Lemma_4_5_5_2 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ (x y : A), y ∈ equivClass R x ↔\n      equivClass R y = equivClass R x := by\n  have hsymm : symmetric R := h.right.left\n  have htrans : transitive R := h.right.right\n  fix x : A; fix y : A\n  apply Iff.intro\n  -- (→)\n  assume h2 : y ∈ equivClass R x --Goal:  equivClass R y = equivClass R x\n  define at h2                   --h2: R y x\n  apply Set.ext\n  fix z : A\n  apply Iff.intro\n  -- Proof that z ∈ equivClass R y → z ∈ equivClass R x\n  assume h3 : z ∈ equivClass R y\n  define                         --Goal: R z x\n  define at h3                   --h3: R z y\n  show R z x from htrans z y x h3 h2\n  -- Proof that z ∈ equivClass R x → z ∈ equivClass R y\n  assume h3 : z ∈ equivClass R x\n  define                         --Goal: R z y\n  define at h3                   --h3: R z x\n  have h4 : R x y := hsymm y x h2\n  show R z y from htrans z x y h3 h4\n  -- (←)\n  assume h2 : equivClass R y = equivClass R x  --Goal: y ∈ equivClass R x\n  rewrite [←h2]                  --Goal: y ∈ equivClass R y\n  show y ∈ equivClass R y from Lemma_4_5_5_1 R h y\nThe definition of “partition” has three parts, so to prove Theorem 4.5.4 we will have to prove three statements. It will make the proof easier to read if we prove the three statements separately.\nlemma Theorem_4_5_4_part_1 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ (x : A), x ∈ ⋃₀(mod A R) := by\n  fix x : A\n  define        --Goal: ∃ (a : Set A), a ∈ mod A R ∧ x ∈ a\n  apply Exists.intro (equivClass R x)\n  apply And.intro _ (Lemma_4_5_5_1 R h x)\n                --Goal: equivClass R x ∈ mod A R\n  define        --Goal: ∃ (x_1 : A), equivClass R x_1 = equivClass R x\n  apply Exists.intro x\n  rfl\n\nlemma Theorem_4_5_4_part_2 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    pairwise_disjoint (mod A R) := by\n  define\n  fix X : Set A\n  assume h2 : X ∈ mod A R\n  fix Y : Set A\n  assume h3 : Y ∈ mod A R           --Goal: X ≠ Y → is_empty (X ∩ Y)\n  define at h2; define at h3\n  obtain (x : A) (h4 : equivClass R x = X) from h2\n  obtain (y : A) (h5 : equivClass R y = Y) from h3\n  contrapos\n  assume h6 : ∃ (x : A), x ∈ X ∩ Y  --Goal: X = Y\n  obtain (z : A) (h7 : z ∈ X ∩ Y) from h6\n  define at h7\n  rewrite [←h4, ←h5] at h7 --h7: z ∈ equivClass R x ∧ z ∈ equivClass R y\n  have h8 : equivClass R z = equivClass R x :=\n    (Lemma_4_5_5_2 R h x z).ltr h7.left\n  have h9 : equivClass R z = equivClass R y :=\n    (Lemma_4_5_5_2 R h y z).ltr h7.right\n  show X = Y from\n    calc\n      X = equivClass R x := h4.symm\n      _ = equivClass R z := h8.symm\n      _ = equivClass R y := h9\n      _ = Y              := h5\n\nlemma Theorem_4_5_4_part_3 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ X ∈ mod A R, ¬is_empty X := by\n  fix X : Set A\n  assume h2 : X ∈ mod A R  --Goal: ¬is_empty X\n  define; double_neg       --Goal: ∃ (x : A), x ∈ X\n  define at h2             --h2: ∃ (x : A), equivClass R x = X\n  obtain (x : A) (h3 : equivClass R x = X) from h2\n  rewrite [←h3]\n  show ∃ (x_1 : A), x_1 ∈ equivClass R x from\n    Exists.intro x (Lemma_4_5_5_1 R h x)\nIts easy now to put everything together to prove Theorem 4.5.4.\ntheorem Theorem_4_5_4 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    partition (mod A R) := And.intro (Theorem_4_5_4_part_1 R h)\n      (And.intro (Theorem_4_5_4_part_2 R h) (Theorem_4_5_4_part_3 R h))\nTheorem 4.5.4 shows that an equivalence relation on A determines a partition of A, namely mod A R. Our next project will be to prove Theorem 4.5.6 in HTPI, which says that every partition of A arises in this way; that is, every partition is mod A R for some equivalence relation R. To prove this, we must show how to use a partition F to define an equivalence relation R for which mod A R = F. The proof in HTPI defines the required equivalence relation R as a set of ordered pairs, but in Lean we will need to define it instead as a binary relation on A. Translating HTPI’s set-theoretic definition into Lean’s notation for binary relations leads to the following definition:\ndef EqRelFromPart {A : Type} (F : Set (Set A)) (x y : A) : Prop :=\n    ∃ X ∈ F, x ∈ X ∧ y ∈ X\nIn other words, EqRelFromPart F is the binary relation on A that is true of any two objects x and y of type A if and only if x and y belong to the same set in F. Our plan now is to show that if F is a partition of A, then EqRelFromPart F is an equivalence relation on A, and mod A (EqRelFromPart F) = F.\nOnce again, HTPI breaks the proof up by proving some lemmas first, and we will find it convenient to break the proof into even smaller pieces. We will leave the proofs of most of these lemmas as exercises for you.\nlemma overlap_implies_equal {A : Type}\n    (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ Y ∈ F, ∀ (x : A), x ∈ X → x ∈ Y → X = Y := sorry\n\nlemma Lemma_4_5_7_ref {A : Type} (F : Set (Set A)) (h : partition F):\n    reflexive (EqRelFromPart F) := sorry\n  \nlemma Lemma_4_5_7_symm {A : Type} (F : Set (Set A)) (h : partition F):\n    symmetric (EqRelFromPart F) := sorry\n\nlemma Lemma_4_5_7_trans {A : Type} (F : Set (Set A)) (h : partition F):\n    transitive (EqRelFromPart F) := sorry\nIt is now easy to put these pieces together to prove Lemma 4.5.7 in HTPI:\nlemma Lemma_4_5_7 {A : Type} (F : Set (Set A)) (h : partition F) :\n    equiv_rel (EqRelFromPart F) := And.intro (Lemma_4_5_7_ref F h)\n      (And.intro (Lemma_4_5_7_symm F h) (Lemma_4_5_7_trans F h))\nWe need one more lemma before we can prove Theorem 4.5.6:\nlemma Lemma_4_5_8 {A : Type} (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ x ∈ X, equivClass (EqRelFromPart F) x = X := sorry\nWe are finally now ready to address Theorem 4.5.6. Here is the statement of the theorem:\ntheorem Theorem_4_5_6 {A : Type} (F : Set (Set A)) (h: partition F) :\n    ∃ (R : BinRel A), equiv_rel R ∧ mod A R = F\nOf course, the relation R that we will use to prove the theorem is EqRelFromPart F, so we could start the proof with the tactic apply Exists.intro (EqRelFromPart F). But this means that the rest of the proof will involve many statements about the relation EqRelFromPart F. When a complicated object appears multiple times in a proof, it can make the proof a little easier to read if we give that object a name. We can do that by using a new tactic. The tactic let R : BinRel A := EqRelFromPart F introduces the new variable R into the tactic state. The variable R has type BinRel A, and it is definitionally equal to EqRelFromPart F. That means that, when necessary, Lean will fill in this definition of R. For example, one of our first steps will be to apply Lemma_4_5_7 to F and h. The conclusion of that lemma is equiv_rel (EqRelFromPart F), but Lean will recognize this as meaning the same thing as equiv_rel R. Here is the proof of the theorem:\ntheorem Theorem_4_5_6 {A : Type} (F : Set (Set A)) (h: partition F) :\n    ∃ (R : BinRel A), equiv_rel R ∧ mod A R = F := by\n  let R : BinRel A := EqRelFromPart F\n  apply Exists.intro R              --Goal: equiv_rel R ∧ mod A R = F\n  apply And.intro (Lemma_4_5_7 F h) --Goal: mod A R = F\n  apply Set.ext\n  fix X : Set A                     --Goal:  X ∈ mod A R ↔ X ∈ F\n  apply Iff.intro\n  -- (→)\n  assume h2 : X ∈ mod A R           --Goal: X ∈ F\n  define at h2                      --h2: ∃ (x : A), equivClass R x = X\n  obtain (x : A) (h3 : equivClass R x = X) from h2\n  have h4 : x ∈ ⋃₀F := h.left x\n  define at h4\n  obtain (Y : Set A) (h5 : Y ∈ F ∧ x ∈ Y) from h4\n  have h6 : equivClass R x = Y :=\n    Lemma_4_5_8 F h Y h5.left x h5.right\n  rewrite [←h3, h6]\n  show Y ∈ F from h5.left\n  -- (←)\n  assume h2 : X ∈ F                 --Goal: X ∈ mod A R\n  have h3 : ¬is_empty X := h.right.right X h2\n  define at h3; double_neg at h3    --h3: ∃ (x : A), x ∈ X\n  obtain (x : A) (h4 : x ∈ X) from h3\n  define                       --Goal: ∃ (x : A), equivClass R x = X\n  show ∃ (x : A), equivClass R x = X from\n    Exists.intro x (Lemma_4_5_8 F h X h2 x h4)\n\nExercises\n\nlemma overlap_implies_equal {A : Type}\n    (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ Y ∈ F, ∀ (x : A), x ∈ X → x ∈ Y → X = Y := sorry\n\n\nlemma Lemma_4_5_7_ref {A : Type} (F : Set (Set A)) (h : partition F):\n    reflexive (EqRelFromPart F) := sorry\n\n\nlemma Lemma_4_5_7_symm {A : Type} (F : Set (Set A)) (h : partition F):\n    symmetric (EqRelFromPart F) := sorry\n\n\nlemma Lemma_4_5_7_trans {A : Type} (F : Set (Set A)) (h : partition F):\n    transitive (EqRelFromPart F) := sorry\n\n\nlemma Lemma_4_5_8 {A : Type} (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ x ∈ X, equivClass (EqRelFromPart F) x = X := sorry\n\n\nlemma elt_mod_equiv_class_of_elt\n    {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ X ∈ mod A R, ∀ x ∈ X, equivClass R x = X := sorry\n\n\n\n\nThe next exercises use the following definitions:\ndef dot {A : Type} (F G : Set (Set A)) : Set (Set A) :=\n    { Z : Set A | ¬is_empty Z ∧ ∃ X ∈ F, ∃ Y ∈ G, Z = X ∩ Y }\n\ndef conj {A : Type} (R S : BinRel A) (x y : A) : Prop :=\n    R x y ∧ S x y\n\ntheorem Exercise_4_5_20a {A : Type} (R S : BinRel A)\n    (h1 : equiv_rel R) (h2 : equiv_rel S) :\n    equiv_rel (conj R S) := sorry\n\n\ntheorem Exercise_4_5_20b {A : Type} (R S : BinRel A)\n    (h1 : equiv_rel R) (h2 : equiv_rel S) :\n    ∀ (x : A), equivClass (conj R S) x =\n      equivClass R x ∩ equivClass S x := sorry\n\n\ntheorem Exercise_4_5_20c {A : Type} (R S : BinRel A)\n    (h1 : equiv_rel R) (h2 : equiv_rel S) :\n    mod A (conj R S) = dot (mod A R) (mod A S) := sorry"
  }
]